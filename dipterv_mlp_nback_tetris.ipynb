{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import os\n",
    "import numpy as np\n",
    "from numpy import newaxis\n",
    "from sklearn import preprocessing\n",
    "np.random.seed(1337)\n",
    "\n",
    "import keras\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.layers import Dense, Input, Flatten\n",
    "from keras.layers import Dropout\n",
    "from keras.models import Model,Sequential\n",
    "import sys\n",
    "\n",
    "import pandas as pd\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data read started...\n",
      "Data read finished.\n",
      "(1125, 11)\n"
     ]
    }
   ],
   "source": [
    "print(\"Data read started...\")\n",
    "data = pd.read_csv(\"result_nback_tetris.csv\")\n",
    "data = data.as_matrix()\n",
    "print (\"Data read finished.\")\n",
    "\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eliminate EEG data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1125, 7)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range (1,5):\n",
    "    data = np.delete(data, i, 1) \n",
    "\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 126910.0, 150630.0, 1839.9, 4.746279069767441, 371.67, 522.54], dtype=object)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Z normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/amoro/anaconda2/lib/python2.7/site-packages/sklearn/utils/validation.py:420: DataConversionWarning: Data with input dtype object was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/home/amoro/anaconda2/lib/python2.7/site-packages/sklearn/utils/validation.py:420: DataConversionWarning: Data with input dtype object was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/home/amoro/anaconda2/lib/python2.7/site-packages/sklearn/utils/validation.py:420: DataConversionWarning: Data with input dtype object was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/home/amoro/anaconda2/lib/python2.7/site-packages/sklearn/utils/validation.py:420: DataConversionWarning: Data with input dtype object was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/home/amoro/anaconda2/lib/python2.7/site-packages/sklearn/utils/validation.py:420: DataConversionWarning: Data with input dtype object was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/home/amoro/anaconda2/lib/python2.7/site-packages/sklearn/utils/validation.py:420: DataConversionWarning: Data with input dtype object was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "for i in range (1,7):\n",
    "    data[:, i] = preprocessing.scale(data[:, i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_data = data[:, 1:]\n",
    "y_data = data[:, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " ..., \n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "ohe = OneHotEncoder()\n",
    "y_one_hot = ohe.fit_transform(y_data.reshape(-1,1)).toarray()\n",
    "print(y_one_hot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shuffle data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "indices = np.arange(x_data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "x_data = x_data[indices]\n",
    "y_one_hot = y_one_hot[indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Divide into train, validation and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len_data = len(x_data)\n",
    "\n",
    "nb_test = int(len_data*0.15)\n",
    "nb_validation = int(len_data*0.15)\n",
    "nb_train = int(len_data*0.7)\n",
    "\n",
    "end_valid = nb_train+nb_validation\n",
    "\n",
    "x_train = x_data[0:nb_train]\n",
    "y_train = y_one_hot[0:nb_train]\n",
    "\n",
    "x_valid = x_data[nb_train:end_valid]\n",
    "y_valid = y_one_hot[nb_train:end_valid]\n",
    "\n",
    "x_test = x_data[end_valid:]\n",
    "y_test = y_one_hot[end_valid:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(787, 6) (787, 3) (168, 6) (168, 3) (170, 6) (170, 3)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape, y_train.shape, x_valid.shape, y_valid.shape, x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(200, input_shape=(6,)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(200, activation='relu'))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(200, activation='relu'))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(3, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 787 samples, validate on 168 samples\n",
      "Epoch 1/150\n",
      "787/787 [==============================] - 0s - loss: 1.0511 - acc: 0.4447 - val_loss: 1.0121 - val_acc: 0.5060\n",
      "Epoch 2/150\n",
      "787/787 [==============================] - 0s - loss: 0.9908 - acc: 0.5210 - val_loss: 0.9514 - val_acc: 0.5476\n",
      "Epoch 3/150\n",
      "787/787 [==============================] - 0s - loss: 0.9655 - acc: 0.5565 - val_loss: 0.9251 - val_acc: 0.6310\n",
      "Epoch 4/150\n",
      "787/787 [==============================] - 0s - loss: 0.9332 - acc: 0.5476 - val_loss: 0.8655 - val_acc: 0.5714\n",
      "Epoch 5/150\n",
      "787/787 [==============================] - 0s - loss: 0.8830 - acc: 0.5832 - val_loss: 0.8204 - val_acc: 0.6488\n",
      "Epoch 6/150\n",
      "787/787 [==============================] - 0s - loss: 0.8735 - acc: 0.5756 - val_loss: 0.7784 - val_acc: 0.6190\n",
      "Epoch 7/150\n",
      "787/787 [==============================] - 0s - loss: 0.8277 - acc: 0.6074 - val_loss: 0.7587 - val_acc: 0.6548\n",
      "Epoch 8/150\n",
      "787/787 [==============================] - 0s - loss: 0.8009 - acc: 0.6112 - val_loss: 0.7509 - val_acc: 0.6548\n",
      "Epoch 9/150\n",
      "787/787 [==============================] - 0s - loss: 0.8126 - acc: 0.5909 - val_loss: 0.7108 - val_acc: 0.6845\n",
      "Epoch 10/150\n",
      "787/787 [==============================] - 0s - loss: 0.7930 - acc: 0.5972 - val_loss: 0.7188 - val_acc: 0.6726\n",
      "Epoch 11/150\n",
      "787/787 [==============================] - 0s - loss: 0.7571 - acc: 0.6379 - val_loss: 0.6919 - val_acc: 0.6488\n",
      "Epoch 12/150\n",
      "787/787 [==============================] - 0s - loss: 0.7580 - acc: 0.6290 - val_loss: 0.7023 - val_acc: 0.6726\n",
      "Epoch 13/150\n",
      "787/787 [==============================] - 0s - loss: 0.7484 - acc: 0.6213 - val_loss: 0.6815 - val_acc: 0.6548\n",
      "Epoch 14/150\n",
      "787/787 [==============================] - 0s - loss: 0.7134 - acc: 0.6595 - val_loss: 0.6953 - val_acc: 0.6726\n",
      "Epoch 15/150\n",
      "787/787 [==============================] - 0s - loss: 0.7110 - acc: 0.6595 - val_loss: 0.6826 - val_acc: 0.6726\n",
      "Epoch 16/150\n",
      "787/787 [==============================] - 0s - loss: 0.7294 - acc: 0.6391 - val_loss: 0.6802 - val_acc: 0.6905\n",
      "Epoch 17/150\n",
      "787/787 [==============================] - 0s - loss: 0.7338 - acc: 0.6493 - val_loss: 0.6789 - val_acc: 0.6488\n",
      "Epoch 18/150\n",
      "787/787 [==============================] - 0s - loss: 0.7000 - acc: 0.6595 - val_loss: 0.6937 - val_acc: 0.6726\n",
      "Epoch 19/150\n",
      "787/787 [==============================] - 0s - loss: 0.6815 - acc: 0.6722 - val_loss: 0.6631 - val_acc: 0.6786\n",
      "Epoch 20/150\n",
      "787/787 [==============================] - 0s - loss: 0.7114 - acc: 0.6506 - val_loss: 0.6360 - val_acc: 0.6726\n",
      "Epoch 21/150\n",
      "787/787 [==============================] - 0s - loss: 0.6705 - acc: 0.6709 - val_loss: 0.6761 - val_acc: 0.6845\n",
      "Epoch 22/150\n",
      "787/787 [==============================] - 0s - loss: 0.6828 - acc: 0.6569 - val_loss: 0.6460 - val_acc: 0.6905\n",
      "Epoch 23/150\n",
      "787/787 [==============================] - 0s - loss: 0.6605 - acc: 0.6518 - val_loss: 0.6454 - val_acc: 0.7202\n",
      "Epoch 24/150\n",
      "787/787 [==============================] - 0s - loss: 0.6570 - acc: 0.6760 - val_loss: 0.6446 - val_acc: 0.6786\n",
      "Epoch 25/150\n",
      "787/787 [==============================] - 0s - loss: 0.6679 - acc: 0.6544 - val_loss: 0.6402 - val_acc: 0.7143\n",
      "Epoch 26/150\n",
      "787/787 [==============================] - 0s - loss: 0.6482 - acc: 0.6684 - val_loss: 0.6344 - val_acc: 0.7083\n",
      "Epoch 27/150\n",
      "787/787 [==============================] - 0s - loss: 0.6751 - acc: 0.6302 - val_loss: 0.6205 - val_acc: 0.7024\n",
      "Epoch 28/150\n",
      "787/787 [==============================] - 0s - loss: 0.6569 - acc: 0.6836 - val_loss: 0.6178 - val_acc: 0.6905\n",
      "Epoch 29/150\n",
      "787/787 [==============================] - 0s - loss: 0.6479 - acc: 0.6645 - val_loss: 0.6222 - val_acc: 0.7321\n",
      "Epoch 30/150\n",
      "787/787 [==============================] - 0s - loss: 0.6380 - acc: 0.6785 - val_loss: 0.6465 - val_acc: 0.6964\n",
      "Epoch 31/150\n",
      "787/787 [==============================] - 0s - loss: 0.6401 - acc: 0.6747 - val_loss: 0.6151 - val_acc: 0.6905\n",
      "Epoch 32/150\n",
      "787/787 [==============================] - 0s - loss: 0.6325 - acc: 0.6861 - val_loss: 0.6164 - val_acc: 0.7262\n",
      "Epoch 33/150\n",
      "787/787 [==============================] - 0s - loss: 0.6365 - acc: 0.6798 - val_loss: 0.6199 - val_acc: 0.7024\n",
      "Epoch 34/150\n",
      "787/787 [==============================] - 0s - loss: 0.6033 - acc: 0.7065 - val_loss: 0.6066 - val_acc: 0.7321\n",
      "Epoch 35/150\n",
      "787/787 [==============================] - 0s - loss: 0.6111 - acc: 0.6938 - val_loss: 0.6096 - val_acc: 0.7024\n",
      "Epoch 36/150\n",
      "787/787 [==============================] - 0s - loss: 0.6162 - acc: 0.6811 - val_loss: 0.6154 - val_acc: 0.7262\n",
      "Epoch 37/150\n",
      "787/787 [==============================] - 0s - loss: 0.6357 - acc: 0.6925 - val_loss: 0.5953 - val_acc: 0.7321\n",
      "Epoch 38/150\n",
      "787/787 [==============================] - 0s - loss: 0.6203 - acc: 0.6989 - val_loss: 0.5930 - val_acc: 0.7262\n",
      "Epoch 39/150\n",
      "787/787 [==============================] - 0s - loss: 0.6099 - acc: 0.6950 - val_loss: 0.5847 - val_acc: 0.7262\n",
      "Epoch 40/150\n",
      "787/787 [==============================] - 0s - loss: 0.5925 - acc: 0.7141 - val_loss: 0.5830 - val_acc: 0.7262\n",
      "Epoch 41/150\n",
      "787/787 [==============================] - 0s - loss: 0.5913 - acc: 0.7128 - val_loss: 0.6048 - val_acc: 0.7083\n",
      "Epoch 42/150\n",
      "787/787 [==============================] - 0s - loss: 0.5905 - acc: 0.7039 - val_loss: 0.5763 - val_acc: 0.7381\n",
      "Epoch 43/150\n",
      "787/787 [==============================] - 0s - loss: 0.6002 - acc: 0.6709 - val_loss: 0.5852 - val_acc: 0.7321\n",
      "Epoch 44/150\n",
      "787/787 [==============================] - 0s - loss: 0.6092 - acc: 0.7014 - val_loss: 0.5806 - val_acc: 0.7321\n",
      "Epoch 45/150\n",
      "787/787 [==============================] - 0s - loss: 0.5769 - acc: 0.7014 - val_loss: 0.6023 - val_acc: 0.7024\n",
      "Epoch 46/150\n",
      "787/787 [==============================] - 0s - loss: 0.6000 - acc: 0.6823 - val_loss: 0.5947 - val_acc: 0.7024\n",
      "Epoch 47/150\n",
      "787/787 [==============================] - 0s - loss: 0.5722 - acc: 0.7230 - val_loss: 0.5783 - val_acc: 0.7440\n",
      "Epoch 48/150\n",
      "787/787 [==============================] - 0s - loss: 0.5709 - acc: 0.7141 - val_loss: 0.5937 - val_acc: 0.7262\n",
      "Epoch 49/150\n",
      "787/787 [==============================] - 0s - loss: 0.5642 - acc: 0.7294 - val_loss: 0.5713 - val_acc: 0.7321\n",
      "Epoch 50/150\n",
      "787/787 [==============================] - 0s - loss: 0.5665 - acc: 0.7090 - val_loss: 0.5501 - val_acc: 0.7560\n",
      "Epoch 51/150\n",
      "787/787 [==============================] - 0s - loss: 0.5752 - acc: 0.7039 - val_loss: 0.5731 - val_acc: 0.7500\n",
      "Epoch 52/150\n",
      "787/787 [==============================] - 0s - loss: 0.5767 - acc: 0.7065 - val_loss: 0.5590 - val_acc: 0.7262\n",
      "Epoch 53/150\n",
      "787/787 [==============================] - 0s - loss: 0.5846 - acc: 0.7090 - val_loss: 0.5786 - val_acc: 0.7619\n",
      "Epoch 54/150\n",
      "787/787 [==============================] - 0s - loss: 0.5626 - acc: 0.7255 - val_loss: 0.5659 - val_acc: 0.7560\n",
      "Epoch 55/150\n",
      "787/787 [==============================] - 0s - loss: 0.5578 - acc: 0.7230 - val_loss: 0.5529 - val_acc: 0.7619\n",
      "Epoch 56/150\n",
      "787/787 [==============================] - 0s - loss: 0.5472 - acc: 0.7446 - val_loss: 0.5743 - val_acc: 0.7440\n",
      "Epoch 57/150\n",
      "787/787 [==============================] - 0s - loss: 0.5490 - acc: 0.7052 - val_loss: 0.5609 - val_acc: 0.7560\n",
      "Epoch 58/150\n",
      "787/787 [==============================] - 0s - loss: 0.5553 - acc: 0.7319 - val_loss: 0.5612 - val_acc: 0.7500\n",
      "Epoch 59/150\n",
      "787/787 [==============================] - 0s - loss: 0.5493 - acc: 0.7217 - val_loss: 0.5491 - val_acc: 0.7619\n",
      "Epoch 60/150\n",
      "787/787 [==============================] - 0s - loss: 0.5509 - acc: 0.7319 - val_loss: 0.5440 - val_acc: 0.7798\n",
      "Epoch 61/150\n",
      "787/787 [==============================] - 0s - loss: 0.5607 - acc: 0.7281 - val_loss: 0.5531 - val_acc: 0.7738\n",
      "Epoch 62/150\n",
      "787/787 [==============================] - 0s - loss: 0.5491 - acc: 0.7306 - val_loss: 0.5514 - val_acc: 0.7500\n",
      "Epoch 63/150\n",
      "787/787 [==============================] - 0s - loss: 0.5427 - acc: 0.7230 - val_loss: 0.5541 - val_acc: 0.7560\n",
      "Epoch 64/150\n",
      "787/787 [==============================] - 0s - loss: 0.5418 - acc: 0.7433 - val_loss: 0.5410 - val_acc: 0.7440\n",
      "Epoch 65/150\n",
      "787/787 [==============================] - 0s - loss: 0.5351 - acc: 0.7408 - val_loss: 0.5479 - val_acc: 0.7679\n",
      "Epoch 66/150\n",
      "787/787 [==============================] - 0s - loss: 0.5306 - acc: 0.7319 - val_loss: 0.5433 - val_acc: 0.7679\n",
      "Epoch 67/150\n",
      "787/787 [==============================] - 0s - loss: 0.5430 - acc: 0.7319 - val_loss: 0.5335 - val_acc: 0.7619\n",
      "Epoch 68/150\n",
      "787/787 [==============================] - 0s - loss: 0.5312 - acc: 0.7421 - val_loss: 0.5304 - val_acc: 0.7560\n",
      "Epoch 69/150\n",
      "787/787 [==============================] - 0s - loss: 0.5317 - acc: 0.7357 - val_loss: 0.5338 - val_acc: 0.7679\n",
      "Epoch 70/150\n",
      "787/787 [==============================] - 0s - loss: 0.5331 - acc: 0.7344 - val_loss: 0.5414 - val_acc: 0.7679\n",
      "Epoch 71/150\n",
      "787/787 [==============================] - 0s - loss: 0.5346 - acc: 0.7294 - val_loss: 0.5478 - val_acc: 0.7619\n",
      "Epoch 72/150\n",
      "787/787 [==============================] - 0s - loss: 0.5263 - acc: 0.7370 - val_loss: 0.5369 - val_acc: 0.7619\n",
      "Epoch 73/150\n",
      "787/787 [==============================] - 0s - loss: 0.5299 - acc: 0.7433 - val_loss: 0.5415 - val_acc: 0.7798\n",
      "Epoch 74/150\n",
      "787/787 [==============================] - 0s - loss: 0.5263 - acc: 0.7484 - val_loss: 0.5495 - val_acc: 0.7738\n",
      "Epoch 75/150\n",
      "787/787 [==============================] - 0s - loss: 0.5354 - acc: 0.7294 - val_loss: 0.5234 - val_acc: 0.7679\n",
      "Epoch 76/150\n",
      "787/787 [==============================] - 0s - loss: 0.5220 - acc: 0.7446 - val_loss: 0.5202 - val_acc: 0.7917\n",
      "Epoch 77/150\n",
      "787/787 [==============================] - 0s - loss: 0.5109 - acc: 0.7560 - val_loss: 0.5267 - val_acc: 0.7857\n",
      "Epoch 78/150\n",
      "787/787 [==============================] - 0s - loss: 0.5288 - acc: 0.7281 - val_loss: 0.5177 - val_acc: 0.7857\n",
      "Epoch 79/150\n",
      "787/787 [==============================] - 0s - loss: 0.5202 - acc: 0.7535 - val_loss: 0.5285 - val_acc: 0.7917\n",
      "Epoch 80/150\n",
      "787/787 [==============================] - 0s - loss: 0.5063 - acc: 0.7510 - val_loss: 0.5479 - val_acc: 0.7738\n",
      "Epoch 81/150\n",
      "787/787 [==============================] - 0s - loss: 0.5130 - acc: 0.7357 - val_loss: 0.5210 - val_acc: 0.7619\n",
      "Epoch 82/150\n",
      "787/787 [==============================] - 0s - loss: 0.5107 - acc: 0.7446 - val_loss: 0.5370 - val_acc: 0.7738\n",
      "Epoch 83/150\n",
      "787/787 [==============================] - 0s - loss: 0.5223 - acc: 0.7332 - val_loss: 0.5253 - val_acc: 0.7679\n",
      "Epoch 84/150\n",
      "787/787 [==============================] - 0s - loss: 0.4949 - acc: 0.7586 - val_loss: 0.5169 - val_acc: 0.7917\n",
      "Epoch 85/150\n",
      "787/787 [==============================] - 0s - loss: 0.5012 - acc: 0.7395 - val_loss: 0.5271 - val_acc: 0.7857\n",
      "Epoch 86/150\n",
      "787/787 [==============================] - 0s - loss: 0.5134 - acc: 0.7484 - val_loss: 0.5390 - val_acc: 0.7976\n",
      "Epoch 87/150\n",
      "787/787 [==============================] - 0s - loss: 0.5206 - acc: 0.7294 - val_loss: 0.5145 - val_acc: 0.7440\n",
      "Epoch 88/150\n",
      "787/787 [==============================] - 0s - loss: 0.5210 - acc: 0.7459 - val_loss: 0.5270 - val_acc: 0.7798\n",
      "Epoch 89/150\n",
      "787/787 [==============================] - 0s - loss: 0.4998 - acc: 0.7382 - val_loss: 0.5498 - val_acc: 0.7738\n",
      "Epoch 90/150\n",
      "787/787 [==============================] - 0s - loss: 0.4869 - acc: 0.7446 - val_loss: 0.5316 - val_acc: 0.7738\n",
      "Epoch 91/150\n",
      "787/787 [==============================] - 0s - loss: 0.5084 - acc: 0.7395 - val_loss: 0.5108 - val_acc: 0.7560\n",
      "Epoch 92/150\n",
      "787/787 [==============================] - 0s - loss: 0.5068 - acc: 0.7433 - val_loss: 0.5296 - val_acc: 0.7798\n",
      "Epoch 93/150\n",
      "787/787 [==============================] - 0s - loss: 0.4952 - acc: 0.7497 - val_loss: 0.5150 - val_acc: 0.7500\n",
      "Epoch 94/150\n",
      "787/787 [==============================] - 0s - loss: 0.4924 - acc: 0.7675 - val_loss: 0.5517 - val_acc: 0.7321\n",
      "Epoch 95/150\n",
      "787/787 [==============================] - 0s - loss: 0.4885 - acc: 0.7433 - val_loss: 0.5302 - val_acc: 0.7679\n",
      "Epoch 96/150\n",
      "787/787 [==============================] - 0s - loss: 0.4899 - acc: 0.7357 - val_loss: 0.5272 - val_acc: 0.7619\n",
      "Epoch 97/150\n",
      "787/787 [==============================] - 0s - loss: 0.5036 - acc: 0.7497 - val_loss: 0.5247 - val_acc: 0.7440\n",
      "Epoch 98/150\n",
      "787/787 [==============================] - 0s - loss: 0.4867 - acc: 0.7446 - val_loss: 0.5268 - val_acc: 0.7619\n",
      "Epoch 99/150\n",
      "787/787 [==============================] - 0s - loss: 0.4850 - acc: 0.7382 - val_loss: 0.5056 - val_acc: 0.7976\n",
      "Epoch 100/150\n",
      "787/787 [==============================] - 0s - loss: 0.4764 - acc: 0.7738 - val_loss: 0.5509 - val_acc: 0.7679\n",
      "Epoch 101/150\n",
      "787/787 [==============================] - 0s - loss: 0.4753 - acc: 0.7459 - val_loss: 0.5060 - val_acc: 0.7738\n",
      "Epoch 102/150\n",
      "787/787 [==============================] - 0s - loss: 0.4904 - acc: 0.7243 - val_loss: 0.5350 - val_acc: 0.7440\n",
      "Epoch 103/150\n",
      "787/787 [==============================] - 0s - loss: 0.4996 - acc: 0.7459 - val_loss: 0.5037 - val_acc: 0.7976\n",
      "Epoch 104/150\n",
      "787/787 [==============================] - 0s - loss: 0.4840 - acc: 0.7548 - val_loss: 0.5065 - val_acc: 0.7560\n",
      "Epoch 105/150\n",
      "787/787 [==============================] - 0s - loss: 0.4611 - acc: 0.7637 - val_loss: 0.5079 - val_acc: 0.8095\n",
      "Epoch 106/150\n",
      "787/787 [==============================] - 0s - loss: 0.4780 - acc: 0.7573 - val_loss: 0.5201 - val_acc: 0.7798\n",
      "Epoch 107/150\n",
      "787/787 [==============================] - 0s - loss: 0.4876 - acc: 0.7395 - val_loss: 0.5168 - val_acc: 0.7738\n",
      "Epoch 108/150\n",
      "787/787 [==============================] - 0s - loss: 0.4930 - acc: 0.7687 - val_loss: 0.5158 - val_acc: 0.7798\n",
      "Epoch 109/150\n",
      "787/787 [==============================] - 0s - loss: 0.4626 - acc: 0.7560 - val_loss: 0.5049 - val_acc: 0.7560\n",
      "Epoch 110/150\n",
      "787/787 [==============================] - 0s - loss: 0.4739 - acc: 0.7649 - val_loss: 0.5074 - val_acc: 0.7857\n",
      "Epoch 111/150\n",
      "787/787 [==============================] - 0s - loss: 0.4829 - acc: 0.7382 - val_loss: 0.5210 - val_acc: 0.7679\n",
      "Epoch 112/150\n",
      "787/787 [==============================] - 0s - loss: 0.4822 - acc: 0.7510 - val_loss: 0.5271 - val_acc: 0.7798\n",
      "Epoch 113/150\n",
      "787/787 [==============================] - 0s - loss: 0.4978 - acc: 0.7382 - val_loss: 0.4871 - val_acc: 0.7738\n",
      "Epoch 114/150\n",
      "787/787 [==============================] - 0s - loss: 0.4704 - acc: 0.7586 - val_loss: 0.4962 - val_acc: 0.7917\n",
      "Epoch 115/150\n",
      "787/787 [==============================] - 0s - loss: 0.4604 - acc: 0.7713 - val_loss: 0.5065 - val_acc: 0.8036\n",
      "Epoch 116/150\n",
      "787/787 [==============================] - 0s - loss: 0.4645 - acc: 0.7649 - val_loss: 0.5077 - val_acc: 0.7798\n",
      "Epoch 117/150\n",
      "787/787 [==============================] - 0s - loss: 0.4726 - acc: 0.7637 - val_loss: 0.5120 - val_acc: 0.7857\n",
      "Epoch 118/150\n",
      "787/787 [==============================] - 0s - loss: 0.4767 - acc: 0.7510 - val_loss: 0.5394 - val_acc: 0.7976\n",
      "Epoch 119/150\n",
      "787/787 [==============================] - 0s - loss: 0.4581 - acc: 0.7700 - val_loss: 0.5125 - val_acc: 0.7917\n",
      "Epoch 120/150\n",
      "787/787 [==============================] - 0s - loss: 0.4658 - acc: 0.7624 - val_loss: 0.5002 - val_acc: 0.7798\n",
      "Epoch 121/150\n",
      "787/787 [==============================] - 0s - loss: 0.4855 - acc: 0.7421 - val_loss: 0.5064 - val_acc: 0.7857\n",
      "Epoch 122/150\n",
      "787/787 [==============================] - 0s - loss: 0.4752 - acc: 0.7586 - val_loss: 0.4947 - val_acc: 0.7857\n",
      "Epoch 123/150\n",
      "787/787 [==============================] - 0s - loss: 0.4531 - acc: 0.7738 - val_loss: 0.5035 - val_acc: 0.7917\n",
      "Epoch 124/150\n",
      "787/787 [==============================] - 0s - loss: 0.4561 - acc: 0.7624 - val_loss: 0.5119 - val_acc: 0.7738\n",
      "Epoch 125/150\n",
      "787/787 [==============================] - 0s - loss: 0.4801 - acc: 0.7649 - val_loss: 0.5050 - val_acc: 0.8036\n",
      "Epoch 126/150\n",
      "787/787 [==============================] - 0s - loss: 0.4557 - acc: 0.7649 - val_loss: 0.5203 - val_acc: 0.8095\n",
      "Epoch 127/150\n",
      "787/787 [==============================] - 0s - loss: 0.4761 - acc: 0.7573 - val_loss: 0.5074 - val_acc: 0.8095\n",
      "Epoch 128/150\n",
      "787/787 [==============================] - 0s - loss: 0.4559 - acc: 0.7827 - val_loss: 0.4923 - val_acc: 0.8036\n",
      "Epoch 129/150\n",
      "787/787 [==============================] - 0s - loss: 0.4641 - acc: 0.7471 - val_loss: 0.5037 - val_acc: 0.7917\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f74d0f30fd0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 50\n",
    "epochs = 150\n",
    "earlyStopping=keras.callbacks.EarlyStopping(monitor='val_loss', patience=15, verbose=0, mode='auto')\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['acc'])\n",
    "model.fit(x_train, y_train, nb_epoch=epochs,batch_size=batch_size, \n",
    "          callbacks=[earlyStopping], shuffle=True, validation_data = (x_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "170/170 [==============================] - 0s     \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.51653211257036991, 0.78235294187770166]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_test, y_test, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160/170 [===========================>..] - ETA: 0s[2 0 2 2 0 2 0 0 2 1 0 1 1 0 2 2 0 2 0 2 0 0 2 0 0 2 2 0 2 0 1 0 2 0 1 2 0\n",
      " 1 0 2 2 1 2 2 0 1 1 0 0 2 2 1 1 2 0 2 2 1 2 0 0 1 2 2 2 1 0 0 0 1 0 2 0 1\n",
      " 2 2 2 2 1 2 2 0 2 1 1 0 0 2 2 2 1 1 2 0 1 2 0 0 0 2 0 0 1 1 2 0 2 0 0 0 2\n",
      " 0 0 1 2 1 2 0 1 2 0 2 2 1 1 0 2 2 1 1 0 1 0 2 2 2 2 2 1 1 2 1 0 0 1 1 2 0\n",
      " 2 0 2 0 0 2 1 2 1 1 0 2 2 1 2 0 2 0 0 0 1 0]\n",
      "160/170 [===========================>..] - ETA: 0s                 precision    recall  f1-score   support\n",
      "\n",
      "class 0(level1)       0.85      0.95      0.90        55\n",
      "class 1(level2)       0.71      0.60      0.65        50\n",
      "class 2(level3)       0.76      0.78      0.77        65\n",
      "\n",
      "    avg / total       0.78      0.78      0.78       170\n",
      "\n",
      "[[52  2  1]\n",
      " [ 5 30 15]\n",
      " [ 4 10 51]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "\n",
    "y_pred = model.predict_classes(x_test)\n",
    "print(y_pred)\n",
    "\n",
    "p=model.predict_proba(x_test)\n",
    "\n",
    "target_names = ['class 0(level1)', 'class 1(level2)', 'class 2(level3)']\n",
    "print(classification_report(np.argmax(y_test,axis=1), y_pred,target_names=target_names))\n",
    "print(confusion_matrix(np.argmax(y_test,axis=1), y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
