{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import os\n",
    "import numpy as np\n",
    "from numpy import newaxis\n",
    "from sklearn import preprocessing\n",
    "np.random.seed(1337)\n",
    "\n",
    "import keras\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.layers import Dense, Input, Flatten\n",
    "from keras.layers import Dropout\n",
    "from keras.models import Model,Sequential\n",
    "import sys\n",
    "\n",
    "import pandas as pd\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data read started...\n",
      "Data read finished.\n",
      "(1125, 11)\n"
     ]
    }
   ],
   "source": [
    "print(\"Data read started...\")\n",
    "data = pd.read_csv(\"result_nback_tetris.csv\")\n",
    "data = data.as_matrix()\n",
    "print (\"Data read finished.\")\n",
    "\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eliminate EEG data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1125, 7)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range (1,5):\n",
    "    data = np.delete(data, 1, 1) \n",
    "\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 1318300.0, 1839.9, 673.37, 4.746279069767441, 371.67, 522.54], dtype=object)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Z normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/amoro/anaconda2/lib/python2.7/site-packages/sklearn/utils/validation.py:420: DataConversionWarning: Data with input dtype object was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/home/amoro/anaconda2/lib/python2.7/site-packages/sklearn/utils/validation.py:420: DataConversionWarning: Data with input dtype object was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/home/amoro/anaconda2/lib/python2.7/site-packages/sklearn/utils/validation.py:420: DataConversionWarning: Data with input dtype object was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/home/amoro/anaconda2/lib/python2.7/site-packages/sklearn/utils/validation.py:420: DataConversionWarning: Data with input dtype object was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/home/amoro/anaconda2/lib/python2.7/site-packages/sklearn/utils/validation.py:420: DataConversionWarning: Data with input dtype object was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/home/amoro/anaconda2/lib/python2.7/site-packages/sklearn/utils/validation.py:420: DataConversionWarning: Data with input dtype object was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "for i in range (1,7):\n",
    "    data[:, i] = preprocessing.scale(data[:, i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_data = data[:, 1:]\n",
    "y_data = data[:, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " ..., \n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "ohe = OneHotEncoder()\n",
    "y_one_hot = ohe.fit_transform(y_data.reshape(-1,1)).toarray()\n",
    "print(y_one_hot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shuffle data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "indices = np.arange(x_data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "x_data = x_data[indices]\n",
    "y_one_hot = y_one_hot[indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Divide into train, validation and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len_data = len(x_data)\n",
    "\n",
    "nb_test = int(len_data*0.15)\n",
    "nb_validation = int(len_data*0.15)\n",
    "nb_train = int(len_data*0.7)\n",
    "\n",
    "end_valid = nb_train+nb_validation\n",
    "\n",
    "x_train = x_data[0:nb_train]\n",
    "y_train = y_one_hot[0:nb_train]\n",
    "\n",
    "x_valid = x_data[nb_train:end_valid]\n",
    "y_valid = y_one_hot[nb_train:end_valid]\n",
    "\n",
    "x_test = x_data[end_valid:]\n",
    "y_test = y_one_hot[end_valid:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(787, 6) (787, 3) (168, 6) (168, 3) (170, 6) (170, 3)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape, y_train.shape, x_valid.shape, y_valid.shape, x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(200, input_shape=(6,)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(200, activation='relu'))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(200, activation='relu'))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(3, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 787 samples, validate on 168 samples\n",
      "Epoch 1/150\n",
      "787/787 [==============================] - 0s - loss: 1.0371 - acc: 0.4562 - val_loss: 0.9754 - val_acc: 0.5357\n",
      "Epoch 2/150\n",
      "787/787 [==============================] - 0s - loss: 0.9718 - acc: 0.5451 - val_loss: 0.9082 - val_acc: 0.6250\n",
      "Epoch 3/150\n",
      "787/787 [==============================] - 0s - loss: 0.9550 - acc: 0.5604 - val_loss: 0.9013 - val_acc: 0.6131\n",
      "Epoch 4/150\n",
      "787/787 [==============================] - 0s - loss: 0.9218 - acc: 0.5693 - val_loss: 0.8192 - val_acc: 0.6012\n",
      "Epoch 5/150\n",
      "787/787 [==============================] - 0s - loss: 0.8654 - acc: 0.5832 - val_loss: 0.7863 - val_acc: 0.6429\n",
      "Epoch 6/150\n",
      "787/787 [==============================] - 0s - loss: 0.8468 - acc: 0.5883 - val_loss: 0.7370 - val_acc: 0.6369\n",
      "Epoch 7/150\n",
      "787/787 [==============================] - 0s - loss: 0.8317 - acc: 0.5896 - val_loss: 0.7107 - val_acc: 0.6845\n",
      "Epoch 8/150\n",
      "787/787 [==============================] - 0s - loss: 0.7893 - acc: 0.6264 - val_loss: 0.7290 - val_acc: 0.6488\n",
      "Epoch 9/150\n",
      "787/787 [==============================] - 0s - loss: 0.7974 - acc: 0.5997 - val_loss: 0.6764 - val_acc: 0.7083\n",
      "Epoch 10/150\n",
      "787/787 [==============================] - 0s - loss: 0.7631 - acc: 0.6252 - val_loss: 0.6911 - val_acc: 0.6429\n",
      "Epoch 11/150\n",
      "787/787 [==============================] - 0s - loss: 0.7504 - acc: 0.6353 - val_loss: 0.6379 - val_acc: 0.7024\n",
      "Epoch 12/150\n",
      "787/787 [==============================] - 0s - loss: 0.7474 - acc: 0.6290 - val_loss: 0.6541 - val_acc: 0.6905\n",
      "Epoch 13/150\n",
      "787/787 [==============================] - 0s - loss: 0.7034 - acc: 0.6861 - val_loss: 0.6174 - val_acc: 0.7024\n",
      "Epoch 14/150\n",
      "787/787 [==============================] - 0s - loss: 0.6925 - acc: 0.6950 - val_loss: 0.6002 - val_acc: 0.7321\n",
      "Epoch 15/150\n",
      "787/787 [==============================] - 0s - loss: 0.6732 - acc: 0.6747 - val_loss: 0.5974 - val_acc: 0.7321\n",
      "Epoch 16/150\n",
      "787/787 [==============================] - 0s - loss: 0.6904 - acc: 0.6633 - val_loss: 0.5904 - val_acc: 0.7381\n",
      "Epoch 17/150\n",
      "787/787 [==============================] - 0s - loss: 0.6825 - acc: 0.6938 - val_loss: 0.5988 - val_acc: 0.7381\n",
      "Epoch 18/150\n",
      "787/787 [==============================] - 0s - loss: 0.6638 - acc: 0.6900 - val_loss: 0.5940 - val_acc: 0.7202\n",
      "Epoch 19/150\n",
      "787/787 [==============================] - 0s - loss: 0.6443 - acc: 0.6938 - val_loss: 0.5625 - val_acc: 0.7202\n",
      "Epoch 20/150\n",
      "787/787 [==============================] - 0s - loss: 0.6479 - acc: 0.6861 - val_loss: 0.5598 - val_acc: 0.7321\n",
      "Epoch 21/150\n",
      "787/787 [==============================] - 0s - loss: 0.6063 - acc: 0.7128 - val_loss: 0.5653 - val_acc: 0.7262\n",
      "Epoch 22/150\n",
      "787/787 [==============================] - 0s - loss: 0.6351 - acc: 0.6861 - val_loss: 0.5391 - val_acc: 0.7619\n",
      "Epoch 23/150\n",
      "787/787 [==============================] - 0s - loss: 0.6032 - acc: 0.7128 - val_loss: 0.5237 - val_acc: 0.7381\n",
      "Epoch 24/150\n",
      "787/787 [==============================] - 0s - loss: 0.6142 - acc: 0.7166 - val_loss: 0.5500 - val_acc: 0.7440\n",
      "Epoch 25/150\n",
      "787/787 [==============================] - 0s - loss: 0.5998 - acc: 0.7078 - val_loss: 0.5372 - val_acc: 0.7381\n",
      "Epoch 26/150\n",
      "787/787 [==============================] - 0s - loss: 0.5976 - acc: 0.7192 - val_loss: 0.5246 - val_acc: 0.7321\n",
      "Epoch 27/150\n",
      "787/787 [==============================] - 0s - loss: 0.5901 - acc: 0.7255 - val_loss: 0.5154 - val_acc: 0.7381\n",
      "Epoch 28/150\n",
      "787/787 [==============================] - 0s - loss: 0.5764 - acc: 0.7141 - val_loss: 0.5129 - val_acc: 0.7560\n",
      "Epoch 29/150\n",
      "787/787 [==============================] - 0s - loss: 0.5565 - acc: 0.7306 - val_loss: 0.5135 - val_acc: 0.7619\n",
      "Epoch 30/150\n",
      "787/787 [==============================] - 0s - loss: 0.5486 - acc: 0.7319 - val_loss: 0.5095 - val_acc: 0.7381\n",
      "Epoch 31/150\n",
      "787/787 [==============================] - 0s - loss: 0.5774 - acc: 0.7319 - val_loss: 0.5189 - val_acc: 0.7262\n",
      "Epoch 32/150\n",
      "787/787 [==============================] - 0s - loss: 0.5693 - acc: 0.7319 - val_loss: 0.4897 - val_acc: 0.7619\n",
      "Epoch 33/150\n",
      "787/787 [==============================] - 0s - loss: 0.5506 - acc: 0.7446 - val_loss: 0.4728 - val_acc: 0.7798\n",
      "Epoch 34/150\n",
      "787/787 [==============================] - 0s - loss: 0.5372 - acc: 0.7281 - val_loss: 0.4893 - val_acc: 0.7738\n",
      "Epoch 35/150\n",
      "787/787 [==============================] - 0s - loss: 0.5483 - acc: 0.7484 - val_loss: 0.4873 - val_acc: 0.7738\n",
      "Epoch 36/150\n",
      "787/787 [==============================] - 0s - loss: 0.5194 - acc: 0.7484 - val_loss: 0.4823 - val_acc: 0.7560\n",
      "Epoch 37/150\n",
      "787/787 [==============================] - 0s - loss: 0.5438 - acc: 0.7421 - val_loss: 0.4544 - val_acc: 0.7857\n",
      "Epoch 38/150\n",
      "787/787 [==============================] - 0s - loss: 0.5437 - acc: 0.7382 - val_loss: 0.4961 - val_acc: 0.7798\n",
      "Epoch 39/150\n",
      "787/787 [==============================] - 0s - loss: 0.5159 - acc: 0.7294 - val_loss: 0.5021 - val_acc: 0.7619\n",
      "Epoch 40/150\n",
      "787/787 [==============================] - 0s - loss: 0.5273 - acc: 0.7459 - val_loss: 0.4534 - val_acc: 0.7798\n",
      "Epoch 41/150\n",
      "787/787 [==============================] - 0s - loss: 0.5014 - acc: 0.7598 - val_loss: 0.4907 - val_acc: 0.7738\n",
      "Epoch 42/150\n",
      "787/787 [==============================] - 0s - loss: 0.5274 - acc: 0.7446 - val_loss: 0.4823 - val_acc: 0.7202\n",
      "Epoch 43/150\n",
      "787/787 [==============================] - 0s - loss: 0.5197 - acc: 0.7611 - val_loss: 0.4514 - val_acc: 0.7738\n",
      "Epoch 44/150\n",
      "787/787 [==============================] - 0s - loss: 0.5007 - acc: 0.7586 - val_loss: 0.4448 - val_acc: 0.7917\n",
      "Epoch 45/150\n",
      "787/787 [==============================] - 0s - loss: 0.4751 - acc: 0.7700 - val_loss: 0.4941 - val_acc: 0.7262\n",
      "Epoch 46/150\n",
      "787/787 [==============================] - 0s - loss: 0.5153 - acc: 0.7471 - val_loss: 0.4560 - val_acc: 0.7857\n",
      "Epoch 47/150\n",
      "787/787 [==============================] - 0s - loss: 0.5082 - acc: 0.7459 - val_loss: 0.4389 - val_acc: 0.8214\n",
      "Epoch 48/150\n",
      "787/787 [==============================] - 0s - loss: 0.4789 - acc: 0.7700 - val_loss: 0.4821 - val_acc: 0.7857\n",
      "Epoch 49/150\n",
      "787/787 [==============================] - 0s - loss: 0.4629 - acc: 0.7967 - val_loss: 0.4476 - val_acc: 0.7917\n",
      "Epoch 50/150\n",
      "787/787 [==============================] - 0s - loss: 0.4709 - acc: 0.7700 - val_loss: 0.4307 - val_acc: 0.8155\n",
      "Epoch 51/150\n",
      "787/787 [==============================] - 0s - loss: 0.4912 - acc: 0.7700 - val_loss: 0.4286 - val_acc: 0.8036\n",
      "Epoch 52/150\n",
      "787/787 [==============================] - 0s - loss: 0.4842 - acc: 0.7611 - val_loss: 0.4246 - val_acc: 0.8214\n",
      "Epoch 53/150\n",
      "787/787 [==============================] - 0s - loss: 0.4807 - acc: 0.7713 - val_loss: 0.4233 - val_acc: 0.8214\n",
      "Epoch 54/150\n",
      "787/787 [==============================] - 0s - loss: 0.4715 - acc: 0.7802 - val_loss: 0.4365 - val_acc: 0.8393\n",
      "Epoch 55/150\n",
      "787/787 [==============================] - 0s - loss: 0.4610 - acc: 0.7827 - val_loss: 0.4112 - val_acc: 0.8214\n",
      "Epoch 56/150\n",
      "787/787 [==============================] - 0s - loss: 0.4701 - acc: 0.7751 - val_loss: 0.4287 - val_acc: 0.8274\n",
      "Epoch 57/150\n",
      "787/787 [==============================] - 0s - loss: 0.4387 - acc: 0.8069 - val_loss: 0.4063 - val_acc: 0.8155\n",
      "Epoch 58/150\n",
      "787/787 [==============================] - 0s - loss: 0.4396 - acc: 0.7942 - val_loss: 0.3876 - val_acc: 0.8274\n",
      "Epoch 59/150\n",
      "787/787 [==============================] - 0s - loss: 0.4549 - acc: 0.7865 - val_loss: 0.4030 - val_acc: 0.8333\n",
      "Epoch 60/150\n",
      "787/787 [==============================] - 0s - loss: 0.4493 - acc: 0.7840 - val_loss: 0.4181 - val_acc: 0.8274\n",
      "Epoch 61/150\n",
      "787/787 [==============================] - 0s - loss: 0.4676 - acc: 0.7865 - val_loss: 0.4122 - val_acc: 0.8333\n",
      "Epoch 62/150\n",
      "787/787 [==============================] - 0s - loss: 0.4380 - acc: 0.7954 - val_loss: 0.4021 - val_acc: 0.8333\n",
      "Epoch 63/150\n",
      "787/787 [==============================] - 0s - loss: 0.4662 - acc: 0.7726 - val_loss: 0.3859 - val_acc: 0.8274\n",
      "Epoch 64/150\n",
      "787/787 [==============================] - 0s - loss: 0.4427 - acc: 0.7967 - val_loss: 0.3905 - val_acc: 0.8452\n",
      "Epoch 65/150\n",
      "787/787 [==============================] - 0s - loss: 0.4593 - acc: 0.8005 - val_loss: 0.3916 - val_acc: 0.8214\n",
      "Epoch 66/150\n",
      "787/787 [==============================] - 0s - loss: 0.4190 - acc: 0.8119 - val_loss: 0.3696 - val_acc: 0.8512\n",
      "Epoch 67/150\n",
      "787/787 [==============================] - 0s - loss: 0.4243 - acc: 0.8030 - val_loss: 0.3848 - val_acc: 0.8631\n",
      "Epoch 68/150\n",
      "787/787 [==============================] - 0s - loss: 0.4202 - acc: 0.8158 - val_loss: 0.3619 - val_acc: 0.8452\n",
      "Epoch 69/150\n",
      "787/787 [==============================] - 0s - loss: 0.4052 - acc: 0.8234 - val_loss: 0.3557 - val_acc: 0.8452\n",
      "Epoch 70/150\n",
      "787/787 [==============================] - 0s - loss: 0.4265 - acc: 0.8119 - val_loss: 0.3718 - val_acc: 0.8512\n",
      "Epoch 71/150\n",
      "787/787 [==============================] - 0s - loss: 0.4204 - acc: 0.8107 - val_loss: 0.3705 - val_acc: 0.8690\n",
      "Epoch 72/150\n",
      "787/787 [==============================] - 0s - loss: 0.4136 - acc: 0.8094 - val_loss: 0.3843 - val_acc: 0.8512\n",
      "Epoch 73/150\n",
      "787/787 [==============================] - 0s - loss: 0.4176 - acc: 0.7980 - val_loss: 0.3958 - val_acc: 0.8214\n",
      "Epoch 74/150\n",
      "787/787 [==============================] - 0s - loss: 0.4047 - acc: 0.8158 - val_loss: 0.3542 - val_acc: 0.8690\n",
      "Epoch 75/150\n",
      "787/787 [==============================] - 0s - loss: 0.4123 - acc: 0.8030 - val_loss: 0.3531 - val_acc: 0.8631\n",
      "Epoch 76/150\n",
      "787/787 [==============================] - 0s - loss: 0.4097 - acc: 0.8043 - val_loss: 0.3497 - val_acc: 0.8571\n",
      "Epoch 77/150\n",
      "787/787 [==============================] - 0s - loss: 0.4081 - acc: 0.7878 - val_loss: 0.3494 - val_acc: 0.8452\n",
      "Epoch 78/150\n",
      "787/787 [==============================] - 0s - loss: 0.4004 - acc: 0.8145 - val_loss: 0.3353 - val_acc: 0.8512\n",
      "Epoch 79/150\n",
      "787/787 [==============================] - 0s - loss: 0.4050 - acc: 0.8107 - val_loss: 0.3467 - val_acc: 0.8810\n",
      "Epoch 80/150\n",
      "787/787 [==============================] - 0s - loss: 0.4041 - acc: 0.7929 - val_loss: 0.3404 - val_acc: 0.8571\n",
      "Epoch 81/150\n",
      "787/787 [==============================] - 0s - loss: 0.3903 - acc: 0.8234 - val_loss: 0.3362 - val_acc: 0.8631\n",
      "Epoch 82/150\n",
      "787/787 [==============================] - 0s - loss: 0.3929 - acc: 0.8158 - val_loss: 0.3677 - val_acc: 0.8512\n",
      "Epoch 83/150\n",
      "787/787 [==============================] - 0s - loss: 0.3839 - acc: 0.8234 - val_loss: 0.3241 - val_acc: 0.8750\n",
      "Epoch 84/150\n",
      "787/787 [==============================] - 0s - loss: 0.3932 - acc: 0.8208 - val_loss: 0.3314 - val_acc: 0.8571\n",
      "Epoch 85/150\n",
      "787/787 [==============================] - 0s - loss: 0.3872 - acc: 0.8043 - val_loss: 0.3530 - val_acc: 0.8333\n",
      "Epoch 86/150\n",
      "787/787 [==============================] - 0s - loss: 0.3553 - acc: 0.8361 - val_loss: 0.3498 - val_acc: 0.8810\n",
      "Epoch 87/150\n",
      "787/787 [==============================] - 0s - loss: 0.3501 - acc: 0.8247 - val_loss: 0.3381 - val_acc: 0.8512\n",
      "Epoch 88/150\n",
      "787/787 [==============================] - 0s - loss: 0.3626 - acc: 0.8335 - val_loss: 0.3418 - val_acc: 0.8452\n",
      "Epoch 89/150\n",
      "787/787 [==============================] - 0s - loss: 0.4038 - acc: 0.8107 - val_loss: 0.3528 - val_acc: 0.8452\n",
      "Epoch 90/150\n",
      "787/787 [==============================] - 0s - loss: 0.3633 - acc: 0.8247 - val_loss: 0.3176 - val_acc: 0.8750\n",
      "Epoch 91/150\n",
      "787/787 [==============================] - 0s - loss: 0.3495 - acc: 0.8475 - val_loss: 0.3350 - val_acc: 0.8393\n",
      "Epoch 92/150\n",
      "787/787 [==============================] - 0s - loss: 0.3855 - acc: 0.8208 - val_loss: 0.3178 - val_acc: 0.8750\n",
      "Epoch 93/150\n",
      "787/787 [==============================] - 0s - loss: 0.3782 - acc: 0.8335 - val_loss: 0.3078 - val_acc: 0.8690\n",
      "Epoch 94/150\n",
      "787/787 [==============================] - 0s - loss: 0.3876 - acc: 0.8285 - val_loss: 0.3160 - val_acc: 0.8631\n",
      "Epoch 95/150\n",
      "787/787 [==============================] - 0s - loss: 0.3685 - acc: 0.8348 - val_loss: 0.3240 - val_acc: 0.8869\n",
      "Epoch 96/150\n",
      "787/787 [==============================] - 0s - loss: 0.3601 - acc: 0.8234 - val_loss: 0.3122 - val_acc: 0.8929\n",
      "Epoch 97/150\n",
      "787/787 [==============================] - 0s - loss: 0.3465 - acc: 0.8501 - val_loss: 0.3238 - val_acc: 0.8690\n",
      "Epoch 98/150\n",
      "787/787 [==============================] - 0s - loss: 0.3575 - acc: 0.8424 - val_loss: 0.3107 - val_acc: 0.8750\n",
      "Epoch 99/150\n",
      "787/787 [==============================] - 0s - loss: 0.3498 - acc: 0.8272 - val_loss: 0.3043 - val_acc: 0.8690\n",
      "Epoch 100/150\n",
      "787/787 [==============================] - 0s - loss: 0.3582 - acc: 0.8437 - val_loss: 0.3215 - val_acc: 0.8810\n",
      "Epoch 101/150\n",
      "787/787 [==============================] - 0s - loss: 0.3607 - acc: 0.8348 - val_loss: 0.3094 - val_acc: 0.8988\n",
      "Epoch 102/150\n",
      "787/787 [==============================] - 0s - loss: 0.3694 - acc: 0.8297 - val_loss: 0.3390 - val_acc: 0.8631\n",
      "Epoch 103/150\n",
      "787/787 [==============================] - 0s - loss: 0.3805 - acc: 0.8424 - val_loss: 0.2988 - val_acc: 0.8750\n",
      "Epoch 104/150\n",
      "787/787 [==============================] - 0s - loss: 0.3549 - acc: 0.8475 - val_loss: 0.3289 - val_acc: 0.8869\n",
      "Epoch 105/150\n",
      "787/787 [==============================] - 0s - loss: 0.3366 - acc: 0.8551 - val_loss: 0.2977 - val_acc: 0.8750\n",
      "Epoch 106/150\n",
      "787/787 [==============================] - 0s - loss: 0.3405 - acc: 0.8564 - val_loss: 0.3037 - val_acc: 0.8750\n",
      "Epoch 107/150\n",
      "787/787 [==============================] - 0s - loss: 0.3581 - acc: 0.8488 - val_loss: 0.2986 - val_acc: 0.8929\n",
      "Epoch 108/150\n",
      "787/787 [==============================] - 0s - loss: 0.3172 - acc: 0.8602 - val_loss: 0.3128 - val_acc: 0.8631\n",
      "Epoch 109/150\n",
      "787/787 [==============================] - 0s - loss: 0.3416 - acc: 0.8475 - val_loss: 0.3330 - val_acc: 0.8750\n",
      "Epoch 110/150\n",
      "787/787 [==============================] - 0s - loss: 0.3475 - acc: 0.8463 - val_loss: 0.3379 - val_acc: 0.8750\n",
      "Epoch 111/150\n",
      "787/787 [==============================] - 0s - loss: 0.3508 - acc: 0.8412 - val_loss: 0.2769 - val_acc: 0.8750\n",
      "Epoch 112/150\n",
      "787/787 [==============================] - 0s - loss: 0.3424 - acc: 0.8539 - val_loss: 0.3106 - val_acc: 0.8869\n",
      "Epoch 113/150\n",
      "787/787 [==============================] - 0s - loss: 0.3240 - acc: 0.8615 - val_loss: 0.2962 - val_acc: 0.8810\n",
      "Epoch 114/150\n",
      "787/787 [==============================] - 0s - loss: 0.3329 - acc: 0.8437 - val_loss: 0.2907 - val_acc: 0.8571\n",
      "Epoch 115/150\n",
      "787/787 [==============================] - 0s - loss: 0.3280 - acc: 0.8602 - val_loss: 0.3212 - val_acc: 0.8869\n",
      "Epoch 116/150\n",
      "787/787 [==============================] - 0s - loss: 0.3333 - acc: 0.8374 - val_loss: 0.3035 - val_acc: 0.8869\n",
      "Epoch 117/150\n",
      "787/787 [==============================] - 0s - loss: 0.3324 - acc: 0.8539 - val_loss: 0.2920 - val_acc: 0.8929\n",
      "Epoch 118/150\n",
      "787/787 [==============================] - 0s - loss: 0.3242 - acc: 0.8577 - val_loss: 0.3026 - val_acc: 0.8810\n",
      "Epoch 119/150\n",
      "787/787 [==============================] - 0s - loss: 0.3093 - acc: 0.8463 - val_loss: 0.2838 - val_acc: 0.8750\n",
      "Epoch 120/150\n",
      "787/787 [==============================] - 0s - loss: 0.3324 - acc: 0.8424 - val_loss: 0.2919 - val_acc: 0.9048\n",
      "Epoch 121/150\n",
      "787/787 [==============================] - 0s - loss: 0.3288 - acc: 0.8551 - val_loss: 0.2983 - val_acc: 0.8810\n",
      "Epoch 122/150\n",
      "787/787 [==============================] - 0s - loss: 0.3120 - acc: 0.8640 - val_loss: 0.2716 - val_acc: 0.8750\n",
      "Epoch 123/150\n",
      "787/787 [==============================] - 0s - loss: 0.3456 - acc: 0.8501 - val_loss: 0.3404 - val_acc: 0.8631\n",
      "Epoch 124/150\n",
      "787/787 [==============================] - 0s - loss: 0.3533 - acc: 0.8374 - val_loss: 0.3111 - val_acc: 0.8571\n",
      "Epoch 125/150\n",
      "787/787 [==============================] - 0s - loss: 0.3547 - acc: 0.8450 - val_loss: 0.3215 - val_acc: 0.8571\n",
      "Epoch 126/150\n",
      "787/787 [==============================] - 0s - loss: 0.3050 - acc: 0.8602 - val_loss: 0.3013 - val_acc: 0.8690\n",
      "Epoch 127/150\n",
      "787/787 [==============================] - 0s - loss: 0.3237 - acc: 0.8539 - val_loss: 0.2796 - val_acc: 0.8690\n",
      "Epoch 128/150\n",
      "787/787 [==============================] - 0s - loss: 0.3172 - acc: 0.8577 - val_loss: 0.3028 - val_acc: 0.8750\n",
      "Epoch 129/150\n",
      "787/787 [==============================] - 0s - loss: 0.3120 - acc: 0.8602 - val_loss: 0.2716 - val_acc: 0.8988\n",
      "Epoch 130/150\n",
      "787/787 [==============================] - 0s - loss: 0.3154 - acc: 0.8577 - val_loss: 0.3023 - val_acc: 0.8869\n",
      "Epoch 131/150\n",
      "787/787 [==============================] - 0s - loss: 0.3309 - acc: 0.8488 - val_loss: 0.2724 - val_acc: 0.8810\n",
      "Epoch 132/150\n",
      "787/787 [==============================] - 0s - loss: 0.3185 - acc: 0.8628 - val_loss: 0.2827 - val_acc: 0.8988\n",
      "Epoch 133/150\n",
      "787/787 [==============================] - 0s - loss: 0.2993 - acc: 0.8615 - val_loss: 0.2759 - val_acc: 0.8750\n",
      "Epoch 134/150\n",
      "787/787 [==============================] - 0s - loss: 0.3091 - acc: 0.8577 - val_loss: 0.2758 - val_acc: 0.8810\n",
      "Epoch 135/150\n",
      "787/787 [==============================] - 0s - loss: 0.3166 - acc: 0.8513 - val_loss: 0.2994 - val_acc: 0.8869\n",
      "Epoch 136/150\n",
      "787/787 [==============================] - 0s - loss: 0.3264 - acc: 0.8729 - val_loss: 0.2769 - val_acc: 0.8929\n",
      "Epoch 137/150\n",
      "787/787 [==============================] - 0s - loss: 0.3267 - acc: 0.8602 - val_loss: 0.2670 - val_acc: 0.8929\n",
      "Epoch 138/150\n",
      "787/787 [==============================] - 0s - loss: 0.3255 - acc: 0.8564 - val_loss: 0.2762 - val_acc: 0.8988\n",
      "Epoch 139/150\n",
      "787/787 [==============================] - 0s - loss: 0.3040 - acc: 0.8615 - val_loss: 0.3134 - val_acc: 0.8512\n",
      "Epoch 140/150\n",
      "787/787 [==============================] - 0s - loss: 0.2911 - acc: 0.8793 - val_loss: 0.2506 - val_acc: 0.9048\n",
      "Epoch 141/150\n",
      "787/787 [==============================] - 0s - loss: 0.3060 - acc: 0.8615 - val_loss: 0.2979 - val_acc: 0.8869\n",
      "Epoch 142/150\n",
      "787/787 [==============================] - 0s - loss: 0.3259 - acc: 0.8590 - val_loss: 0.2861 - val_acc: 0.8750\n",
      "Epoch 143/150\n",
      "787/787 [==============================] - 0s - loss: 0.2995 - acc: 0.8679 - val_loss: 0.2890 - val_acc: 0.8988\n",
      "Epoch 144/150\n",
      "787/787 [==============================] - 0s - loss: 0.2856 - acc: 0.8755 - val_loss: 0.2718 - val_acc: 0.8750\n",
      "Epoch 145/150\n",
      "787/787 [==============================] - 0s - loss: 0.2826 - acc: 0.8818 - val_loss: 0.2664 - val_acc: 0.8929\n",
      "Epoch 146/150\n",
      "787/787 [==============================] - 0s - loss: 0.3047 - acc: 0.8640 - val_loss: 0.2914 - val_acc: 0.8810\n",
      "Epoch 147/150\n",
      "787/787 [==============================] - 0s - loss: 0.3013 - acc: 0.8818 - val_loss: 0.2654 - val_acc: 0.8869\n",
      "Epoch 148/150\n",
      "787/787 [==============================] - 0s - loss: 0.3045 - acc: 0.8640 - val_loss: 0.2564 - val_acc: 0.8810\n",
      "Epoch 149/150\n",
      "787/787 [==============================] - 0s - loss: 0.2771 - acc: 0.8920 - val_loss: 0.2713 - val_acc: 0.8690\n",
      "Epoch 150/150\n",
      "787/787 [==============================] - 0s - loss: 0.2885 - acc: 0.8742 - val_loss: 0.2548 - val_acc: 0.8869\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f4107538790>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 50\n",
    "epochs = 150\n",
    "earlyStopping=keras.callbacks.EarlyStopping(monitor='val_loss', patience=15, verbose=0, mode='auto')\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['acc'])\n",
    "model.fit(x_train, y_train, nb_epoch=epochs,batch_size=batch_size, \n",
    "          callbacks=[earlyStopping], shuffle=True, validation_data = (x_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150/170 [=========================>....] - ETA: 0s"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.27207818276741924, 0.90588235855102539]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_test, y_test, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "170/170 [==============================] - 0s     \n",
      "[2 0 2 1 1 1 0 0 2 1 0 1 1 0 2 2 2 2 0 2 0 0 1 0 0 2 2 1 2 0 1 0 2 0 1 2 0\n",
      " 1 0 1 2 1 2 2 0 1 2 0 0 2 2 1 1 2 0 2 1 1 2 0 0 0 2 2 2 1 0 1 0 2 0 2 0 1\n",
      " 1 2 2 2 1 2 2 0 2 2 1 0 0 2 2 2 2 1 1 0 2 2 0 0 0 0 0 0 2 2 2 0 2 0 0 0 2\n",
      " 0 0 1 1 2 2 1 2 2 0 2 2 1 2 0 2 2 1 1 0 0 0 2 2 2 2 1 2 1 1 1 0 0 1 1 2 0\n",
      " 2 0 2 0 0 2 1 2 2 1 0 2 2 1 1 0 2 0 0 0 1 1]\n",
      " 96/170 [===============>..............] - ETA: 0s                 precision    recall  f1-score   support\n",
      "\n",
      "class 0(level1)       0.93      0.98      0.96        55\n",
      "class 1(level2)       0.93      0.82      0.87        50\n",
      "class 2(level3)       0.87      0.91      0.89        65\n",
      "\n",
      "    avg / total       0.91      0.91      0.91       170\n",
      "\n",
      "[[54  1  0]\n",
      " [ 0 41  9]\n",
      " [ 4  2 59]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "\n",
    "y_pred = model.predict_classes(x_test)\n",
    "print(y_pred)\n",
    "\n",
    "p=model.predict_proba(x_test)\n",
    "\n",
    "target_names = ['class 0(level1)', 'class 1(level2)', 'class 2(level3)']\n",
    "print(classification_report(np.argmax(y_test,axis=1), y_pred,target_names=target_names))\n",
    "print(confusion_matrix(np.argmax(y_test,axis=1), y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
