{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import os\n",
    "import numpy as np\n",
    "from numpy import newaxis\n",
    "from sklearn import preprocessing\n",
    "np.random.seed(1337)\n",
    "\n",
    "import keras\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.layers import Dense, Input, Flatten\n",
    "from keras.layers import Dropout\n",
    "from keras.models import Model,Sequential\n",
    "import sys\n",
    "\n",
    "import pandas as pd\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data read started...\n",
      "Data read finished.\n",
      "Data read started...\n",
      "Data read finished.\n",
      "(522, 11) (603, 11)\n"
     ]
    }
   ],
   "source": [
    "print(\"Data read started...\")\n",
    "nback = pd.read_csv(\"result_nback_3.csv\")\n",
    "nback = nback.as_matrix()\n",
    "print (\"Data read finished.\")\n",
    "\n",
    "print(\"Data read started...\")\n",
    "tetris = pd.read_csv(\"result_tetris.csv\")\n",
    "tetris = tetris.as_matrix()\n",
    "print (\"Data read finished.\")\n",
    "\n",
    "print(nback.shape, tetris.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eliminate EEG data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(522, 7) (603, 7)\n"
     ]
    }
   ],
   "source": [
    "for i in range (1,5):\n",
    "    nback = np.delete(nback, i, 1) \n",
    "    tetris = np.delete(tetris, i, 1)\n",
    "\n",
    "print(nback.shape, tetris.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['3back', 126910.0, 150630.0, 1839.9, 4.746279069767441, 371.67,\n",
       "       522.54], dtype=object)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nback[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dictionary for the levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "level_nback=[\"1back\",\"2back\",\"3back\"]\n",
    "level_tetris=[\"tetris1\",\"tetris2\",\"tetris3\"]\n",
    "\n",
    "level_n2int = dict((p, i) for i, p in enumerate(level_nback))\n",
    "int2level_n = dict((i, p) for i, p in enumerate(level_nback))\n",
    "\n",
    "level_t2int = dict((p, i) for i, p in enumerate(level_tetris))\n",
    "int2level_t = dict((i, p) for i, p in enumerate(level_tetris))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Z normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/amoro/anaconda2/lib/python2.7/site-packages/sklearn/utils/validation.py:420: DataConversionWarning: Data with input dtype object was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/home/amoro/anaconda2/lib/python2.7/site-packages/sklearn/utils/validation.py:420: DataConversionWarning: Data with input dtype object was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/home/amoro/anaconda2/lib/python2.7/site-packages/sklearn/utils/validation.py:420: DataConversionWarning: Data with input dtype object was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/home/amoro/anaconda2/lib/python2.7/site-packages/sklearn/utils/validation.py:420: DataConversionWarning: Data with input dtype object was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/home/amoro/anaconda2/lib/python2.7/site-packages/sklearn/utils/validation.py:420: DataConversionWarning: Data with input dtype object was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/home/amoro/anaconda2/lib/python2.7/site-packages/sklearn/utils/validation.py:420: DataConversionWarning: Data with input dtype object was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/home/amoro/anaconda2/lib/python2.7/site-packages/sklearn/utils/validation.py:420: DataConversionWarning: Data with input dtype object was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/home/amoro/anaconda2/lib/python2.7/site-packages/sklearn/utils/validation.py:420: DataConversionWarning: Data with input dtype object was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/home/amoro/anaconda2/lib/python2.7/site-packages/sklearn/utils/validation.py:420: DataConversionWarning: Data with input dtype object was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/home/amoro/anaconda2/lib/python2.7/site-packages/sklearn/utils/validation.py:420: DataConversionWarning: Data with input dtype object was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/home/amoro/anaconda2/lib/python2.7/site-packages/sklearn/utils/validation.py:420: DataConversionWarning: Data with input dtype object was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/home/amoro/anaconda2/lib/python2.7/site-packages/sklearn/utils/validation.py:420: DataConversionWarning: Data with input dtype object was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "for i in range (1,7):\n",
    "    nback[:, i] = preprocessing.scale(nback[:, i])\n",
    "    tetris[:, i] = preprocessing.scale(tetris[:, i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(len(nback)):\n",
    "    nback[i, 0] = level_n2int[nback[i, 0]]\n",
    "    \n",
    "for i in range(len(tetris)):\n",
    "    tetris[i, 0] = level_t2int[tetris[i, 0]]\n",
    "\n",
    "x_nback = nback[:, 1:]\n",
    "y_nback = nback[:, 0]\n",
    "x_tetris = tetris[:, 1:]\n",
    "y_tetris = tetris[:, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " ..., \n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]] [[ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "ohe = OneHotEncoder()\n",
    "nback_one_hot = ohe.fit_transform(y_nback.reshape(-1,1)).toarray()\n",
    "tetris_one_hot = ohe.fit_transform(y_tetris.reshape(-1,1)).toarray()\n",
    "print(nback_one_hot, tetris_one_hot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shuffle data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "indices = np.arange(nback.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "x_nback = x_nback[indices]\n",
    "nback_one_hot = nback_one_hot[indices]\n",
    "\n",
    "indices = np.arange(tetris.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "x_tetris = x_tetris[indices]\n",
    "tetris_one_hot = tetris_one_hot[indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Divide into train, validation and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nb_train = int(len(x_nback)*0.85)\n",
    "\n",
    "x_train = x_nback[0:nb_train]\n",
    "y_train = nback_one_hot[0:nb_train]\n",
    "\n",
    "x_valid = x_nback[nb_train:]\n",
    "y_valid = nback_one_hot[nb_train:]\n",
    "\n",
    "x_test = x_tetris[0:100]\n",
    "y_test = tetris_one_hot[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(443, 6) (443, 3) (79, 6) (79, 3) (100, 6) (100, 3)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape, y_train.shape, x_valid.shape, y_valid.shape, x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(200, input_shape=(6,)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(200, activation='relu'))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(3, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 443 samples, validate on 79 samples\n",
      "Epoch 1/150\n",
      "443/443 [==============================] - 0s - loss: 0.9964 - acc: 0.5214 - val_loss: 0.9170 - val_acc: 0.6076\n",
      "Epoch 2/150\n",
      "443/443 [==============================] - 0s - loss: 0.8842 - acc: 0.5914 - val_loss: 0.8968 - val_acc: 0.6456\n",
      "Epoch 3/150\n",
      "443/443 [==============================] - 0s - loss: 0.8595 - acc: 0.6253 - val_loss: 0.8972 - val_acc: 0.6456\n",
      "Epoch 4/150\n",
      "443/443 [==============================] - 0s - loss: 0.8546 - acc: 0.6050 - val_loss: 0.8675 - val_acc: 0.6456\n",
      "Epoch 5/150\n",
      "443/443 [==============================] - 0s - loss: 0.8356 - acc: 0.6230 - val_loss: 0.8501 - val_acc: 0.6456\n",
      "Epoch 6/150\n",
      "443/443 [==============================] - 0s - loss: 0.8180 - acc: 0.6321 - val_loss: 0.8389 - val_acc: 0.6456\n",
      "Epoch 7/150\n",
      "443/443 [==============================] - 0s - loss: 0.8100 - acc: 0.6456 - val_loss: 0.8354 - val_acc: 0.6582\n",
      "Epoch 8/150\n",
      "443/443 [==============================] - 0s - loss: 0.7938 - acc: 0.6366 - val_loss: 0.8178 - val_acc: 0.6582\n",
      "Epoch 9/150\n",
      "443/443 [==============================] - 0s - loss: 0.7840 - acc: 0.6546 - val_loss: 0.8147 - val_acc: 0.6709\n",
      "Epoch 10/150\n",
      "443/443 [==============================] - 0s - loss: 0.7652 - acc: 0.6501 - val_loss: 0.8069 - val_acc: 0.6835\n",
      "Epoch 11/150\n",
      "443/443 [==============================] - 0s - loss: 0.7713 - acc: 0.6591 - val_loss: 0.7983 - val_acc: 0.6709\n",
      "Epoch 12/150\n",
      "443/443 [==============================] - 0s - loss: 0.7396 - acc: 0.6795 - val_loss: 0.7892 - val_acc: 0.6962\n",
      "Epoch 13/150\n",
      "443/443 [==============================] - 0s - loss: 0.7308 - acc: 0.6975 - val_loss: 0.7762 - val_acc: 0.6962\n",
      "Epoch 14/150\n",
      "443/443 [==============================] - 0s - loss: 0.7433 - acc: 0.6930 - val_loss: 0.7582 - val_acc: 0.7089\n",
      "Epoch 15/150\n",
      "443/443 [==============================] - 0s - loss: 0.7296 - acc: 0.6795 - val_loss: 0.7646 - val_acc: 0.7089\n",
      "Epoch 16/150\n",
      "443/443 [==============================] - 0s - loss: 0.7291 - acc: 0.6840 - val_loss: 0.7658 - val_acc: 0.6962\n",
      "Epoch 17/150\n",
      "443/443 [==============================] - 0s - loss: 0.7221 - acc: 0.6795 - val_loss: 0.7470 - val_acc: 0.7342\n",
      "Epoch 18/150\n",
      "443/443 [==============================] - 0s - loss: 0.7043 - acc: 0.6817 - val_loss: 0.7553 - val_acc: 0.6962\n",
      "Epoch 19/150\n",
      "443/443 [==============================] - 0s - loss: 0.7297 - acc: 0.6840 - val_loss: 0.7468 - val_acc: 0.6835\n",
      "Epoch 20/150\n",
      "443/443 [==============================] - 0s - loss: 0.7073 - acc: 0.6998 - val_loss: 0.7174 - val_acc: 0.6962\n",
      "Epoch 21/150\n",
      "443/443 [==============================] - 0s - loss: 0.6835 - acc: 0.6998 - val_loss: 0.7203 - val_acc: 0.6962\n",
      "Epoch 22/150\n",
      "443/443 [==============================] - 0s - loss: 0.6893 - acc: 0.6862 - val_loss: 0.7178 - val_acc: 0.6962\n",
      "Epoch 23/150\n",
      "443/443 [==============================] - 0s - loss: 0.6773 - acc: 0.7020 - val_loss: 0.7141 - val_acc: 0.7215\n",
      "Epoch 24/150\n",
      "443/443 [==============================] - 0s - loss: 0.6668 - acc: 0.7043 - val_loss: 0.7086 - val_acc: 0.6962\n",
      "Epoch 25/150\n",
      "443/443 [==============================] - 0s - loss: 0.6556 - acc: 0.7269 - val_loss: 0.7117 - val_acc: 0.7215\n",
      "Epoch 26/150\n",
      "443/443 [==============================] - 0s - loss: 0.6506 - acc: 0.7269 - val_loss: 0.6886 - val_acc: 0.7342\n",
      "Epoch 27/150\n",
      "443/443 [==============================] - 0s - loss: 0.6385 - acc: 0.7269 - val_loss: 0.6951 - val_acc: 0.7089\n",
      "Epoch 28/150\n",
      "443/443 [==============================] - 0s - loss: 0.6283 - acc: 0.7178 - val_loss: 0.6778 - val_acc: 0.7089\n",
      "Epoch 29/150\n",
      "443/443 [==============================] - 0s - loss: 0.6462 - acc: 0.7269 - val_loss: 0.6859 - val_acc: 0.7089\n",
      "Epoch 30/150\n",
      "443/443 [==============================] - 0s - loss: 0.6593 - acc: 0.7359 - val_loss: 0.6634 - val_acc: 0.7342\n",
      "Epoch 31/150\n",
      "443/443 [==============================] - 0s - loss: 0.6446 - acc: 0.7359 - val_loss: 0.6683 - val_acc: 0.6962\n",
      "Epoch 32/150\n",
      "443/443 [==============================] - 0s - loss: 0.6240 - acc: 0.7223 - val_loss: 0.6571 - val_acc: 0.7215\n",
      "Epoch 33/150\n",
      "443/443 [==============================] - 0s - loss: 0.6093 - acc: 0.7404 - val_loss: 0.6705 - val_acc: 0.6835\n",
      "Epoch 34/150\n",
      "443/443 [==============================] - 0s - loss: 0.5904 - acc: 0.7381 - val_loss: 0.6537 - val_acc: 0.7089\n",
      "Epoch 35/150\n",
      "443/443 [==============================] - 0s - loss: 0.5870 - acc: 0.7517 - val_loss: 0.6533 - val_acc: 0.7215\n",
      "Epoch 36/150\n",
      "443/443 [==============================] - 0s - loss: 0.5967 - acc: 0.7517 - val_loss: 0.6432 - val_acc: 0.7089\n",
      "Epoch 37/150\n",
      "443/443 [==============================] - 0s - loss: 0.5837 - acc: 0.7449 - val_loss: 0.6496 - val_acc: 0.7089\n",
      "Epoch 38/150\n",
      "443/443 [==============================] - 0s - loss: 0.6174 - acc: 0.7156 - val_loss: 0.6420 - val_acc: 0.7215\n",
      "Epoch 39/150\n",
      "443/443 [==============================] - 0s - loss: 0.5824 - acc: 0.7675 - val_loss: 0.6217 - val_acc: 0.7089\n",
      "Epoch 40/150\n",
      "443/443 [==============================] - 0s - loss: 0.5862 - acc: 0.7494 - val_loss: 0.6247 - val_acc: 0.7089\n",
      "Epoch 41/150\n",
      "443/443 [==============================] - 0s - loss: 0.5698 - acc: 0.7381 - val_loss: 0.6117 - val_acc: 0.7215\n",
      "Epoch 42/150\n",
      "443/443 [==============================] - 0s - loss: 0.5821 - acc: 0.7336 - val_loss: 0.6236 - val_acc: 0.7342\n",
      "Epoch 43/150\n",
      "443/443 [==============================] - 0s - loss: 0.5713 - acc: 0.7404 - val_loss: 0.6219 - val_acc: 0.7089\n",
      "Epoch 44/150\n",
      "443/443 [==============================] - 0s - loss: 0.5756 - acc: 0.7607 - val_loss: 0.6024 - val_acc: 0.7342\n",
      "Epoch 45/150\n",
      "443/443 [==============================] - 0s - loss: 0.5694 - acc: 0.7314 - val_loss: 0.6083 - val_acc: 0.6962\n",
      "Epoch 46/150\n",
      "443/443 [==============================] - 0s - loss: 0.5386 - acc: 0.7743 - val_loss: 0.6243 - val_acc: 0.7089\n",
      "Epoch 47/150\n",
      "443/443 [==============================] - 0s - loss: 0.5792 - acc: 0.7472 - val_loss: 0.5961 - val_acc: 0.7215\n",
      "Epoch 48/150\n",
      "443/443 [==============================] - 0s - loss: 0.5531 - acc: 0.7562 - val_loss: 0.5874 - val_acc: 0.7468\n",
      "Epoch 49/150\n",
      "443/443 [==============================] - 0s - loss: 0.5341 - acc: 0.7607 - val_loss: 0.5931 - val_acc: 0.7215\n",
      "Epoch 50/150\n",
      "443/443 [==============================] - 0s - loss: 0.5335 - acc: 0.7856 - val_loss: 0.5914 - val_acc: 0.7342\n",
      "Epoch 51/150\n",
      "443/443 [==============================] - 0s - loss: 0.5446 - acc: 0.7675 - val_loss: 0.5746 - val_acc: 0.7089\n",
      "Epoch 52/150\n",
      "443/443 [==============================] - 0s - loss: 0.5358 - acc: 0.7720 - val_loss: 0.5752 - val_acc: 0.7342\n",
      "Epoch 53/150\n",
      "443/443 [==============================] - 0s - loss: 0.5319 - acc: 0.7517 - val_loss: 0.5726 - val_acc: 0.7215\n",
      "Epoch 54/150\n",
      "443/443 [==============================] - 0s - loss: 0.5100 - acc: 0.7833 - val_loss: 0.5905 - val_acc: 0.7215\n",
      "Epoch 55/150\n",
      "443/443 [==============================] - 0s - loss: 0.5023 - acc: 0.7856 - val_loss: 0.5852 - val_acc: 0.7468\n",
      "Epoch 56/150\n",
      "443/443 [==============================] - 0s - loss: 0.5274 - acc: 0.7562 - val_loss: 0.5678 - val_acc: 0.7342\n",
      "Epoch 57/150\n",
      "443/443 [==============================] - 0s - loss: 0.5217 - acc: 0.7562 - val_loss: 0.5682 - val_acc: 0.7595\n",
      "Epoch 58/150\n",
      "443/443 [==============================] - 0s - loss: 0.5177 - acc: 0.7810 - val_loss: 0.5710 - val_acc: 0.7342\n",
      "Epoch 59/150\n",
      "443/443 [==============================] - 0s - loss: 0.4990 - acc: 0.7788 - val_loss: 0.5646 - val_acc: 0.7468\n",
      "Epoch 60/150\n",
      "443/443 [==============================] - 0s - loss: 0.5125 - acc: 0.7810 - val_loss: 0.5770 - val_acc: 0.7089\n",
      "Epoch 61/150\n",
      "443/443 [==============================] - 0s - loss: 0.4986 - acc: 0.7810 - val_loss: 0.5631 - val_acc: 0.7215\n",
      "Epoch 62/150\n",
      "443/443 [==============================] - 0s - loss: 0.4990 - acc: 0.7743 - val_loss: 0.5380 - val_acc: 0.7342\n",
      "Epoch 63/150\n",
      "443/443 [==============================] - 0s - loss: 0.4967 - acc: 0.7788 - val_loss: 0.5762 - val_acc: 0.7215\n",
      "Epoch 64/150\n",
      "443/443 [==============================] - 0s - loss: 0.5164 - acc: 0.8014 - val_loss: 0.5585 - val_acc: 0.7215\n",
      "Epoch 65/150\n",
      "443/443 [==============================] - 0s - loss: 0.5022 - acc: 0.7698 - val_loss: 0.5463 - val_acc: 0.7342\n",
      "Epoch 66/150\n",
      "443/443 [==============================] - 0s - loss: 0.4912 - acc: 0.7788 - val_loss: 0.5362 - val_acc: 0.7722\n",
      "Epoch 67/150\n",
      "443/443 [==============================] - 0s - loss: 0.5055 - acc: 0.7630 - val_loss: 0.5618 - val_acc: 0.7342\n",
      "Epoch 68/150\n",
      "443/443 [==============================] - 0s - loss: 0.4968 - acc: 0.7833 - val_loss: 0.5458 - val_acc: 0.7468\n",
      "Epoch 69/150\n",
      "443/443 [==============================] - 0s - loss: 0.4772 - acc: 0.7878 - val_loss: 0.5343 - val_acc: 0.7595\n",
      "Epoch 70/150\n",
      "443/443 [==============================] - 0s - loss: 0.4549 - acc: 0.8036 - val_loss: 0.5230 - val_acc: 0.7342\n",
      "Epoch 71/150\n",
      "443/443 [==============================] - 0s - loss: 0.4777 - acc: 0.7833 - val_loss: 0.5254 - val_acc: 0.7722\n",
      "Epoch 72/150\n",
      "443/443 [==============================] - 0s - loss: 0.5081 - acc: 0.7720 - val_loss: 0.5399 - val_acc: 0.7975\n",
      "Epoch 73/150\n",
      "443/443 [==============================] - 0s - loss: 0.4714 - acc: 0.7923 - val_loss: 0.5320 - val_acc: 0.7975\n",
      "Epoch 74/150\n",
      "443/443 [==============================] - 0s - loss: 0.4648 - acc: 0.7968 - val_loss: 0.5178 - val_acc: 0.7595\n",
      "Epoch 75/150\n",
      "443/443 [==============================] - 0s - loss: 0.4667 - acc: 0.7810 - val_loss: 0.5403 - val_acc: 0.7342\n",
      "Epoch 76/150\n",
      "443/443 [==============================] - 0s - loss: 0.4626 - acc: 0.8081 - val_loss: 0.5401 - val_acc: 0.7342\n",
      "Epoch 77/150\n",
      "443/443 [==============================] - 0s - loss: 0.4552 - acc: 0.7968 - val_loss: 0.5113 - val_acc: 0.7722\n",
      "Epoch 78/150\n",
      "443/443 [==============================] - 0s - loss: 0.4407 - acc: 0.8126 - val_loss: 0.5055 - val_acc: 0.7595\n",
      "Epoch 79/150\n",
      "443/443 [==============================] - 0s - loss: 0.4435 - acc: 0.8081 - val_loss: 0.5215 - val_acc: 0.7342\n",
      "Epoch 80/150\n",
      "443/443 [==============================] - 0s - loss: 0.4334 - acc: 0.8239 - val_loss: 0.5094 - val_acc: 0.7848\n",
      "Epoch 81/150\n",
      "443/443 [==============================] - 0s - loss: 0.4586 - acc: 0.7901 - val_loss: 0.5162 - val_acc: 0.7722\n",
      "Epoch 82/150\n",
      "443/443 [==============================] - 0s - loss: 0.4513 - acc: 0.8081 - val_loss: 0.5246 - val_acc: 0.7848\n",
      "Epoch 83/150\n",
      "443/443 [==============================] - 0s - loss: 0.4379 - acc: 0.8194 - val_loss: 0.4956 - val_acc: 0.7848\n",
      "Epoch 84/150\n",
      "443/443 [==============================] - 0s - loss: 0.4502 - acc: 0.8081 - val_loss: 0.4868 - val_acc: 0.7975\n",
      "Epoch 85/150\n",
      "443/443 [==============================] - 0s - loss: 0.4516 - acc: 0.8036 - val_loss: 0.4949 - val_acc: 0.7975\n",
      "Epoch 86/150\n",
      "443/443 [==============================] - 0s - loss: 0.4314 - acc: 0.8081 - val_loss: 0.5094 - val_acc: 0.7595\n",
      "Epoch 87/150\n",
      "443/443 [==============================] - 0s - loss: 0.4277 - acc: 0.8126 - val_loss: 0.5202 - val_acc: 0.7722\n",
      "Epoch 88/150\n",
      "443/443 [==============================] - 0s - loss: 0.4481 - acc: 0.7923 - val_loss: 0.4932 - val_acc: 0.7722\n",
      "Epoch 89/150\n",
      "443/443 [==============================] - 0s - loss: 0.4250 - acc: 0.8149 - val_loss: 0.4846 - val_acc: 0.7975\n",
      "Epoch 90/150\n",
      "443/443 [==============================] - 0s - loss: 0.4248 - acc: 0.8194 - val_loss: 0.4927 - val_acc: 0.7722\n",
      "Epoch 91/150\n",
      "443/443 [==============================] - 0s - loss: 0.4344 - acc: 0.8172 - val_loss: 0.5078 - val_acc: 0.7975\n",
      "Epoch 92/150\n",
      "443/443 [==============================] - 0s - loss: 0.4205 - acc: 0.8081 - val_loss: 0.4828 - val_acc: 0.7975\n",
      "Epoch 93/150\n",
      "443/443 [==============================] - 0s - loss: 0.4462 - acc: 0.8104 - val_loss: 0.4776 - val_acc: 0.7848\n",
      "Epoch 94/150\n",
      "443/443 [==============================] - 0s - loss: 0.4505 - acc: 0.8081 - val_loss: 0.4658 - val_acc: 0.7848\n",
      "Epoch 95/150\n",
      "443/443 [==============================] - 0s - loss: 0.4289 - acc: 0.8307 - val_loss: 0.4682 - val_acc: 0.7975\n",
      "Epoch 96/150\n",
      "443/443 [==============================] - 0s - loss: 0.4064 - acc: 0.8352 - val_loss: 0.4835 - val_acc: 0.7848\n",
      "Epoch 97/150\n",
      "443/443 [==============================] - 0s - loss: 0.4623 - acc: 0.8194 - val_loss: 0.4572 - val_acc: 0.7848\n",
      "Epoch 98/150\n",
      "443/443 [==============================] - 0s - loss: 0.4059 - acc: 0.8284 - val_loss: 0.4577 - val_acc: 0.7848\n",
      "Epoch 99/150\n",
      "443/443 [==============================] - 0s - loss: 0.4071 - acc: 0.8217 - val_loss: 0.4648 - val_acc: 0.7848\n",
      "Epoch 100/150\n",
      "443/443 [==============================] - 0s - loss: 0.4057 - acc: 0.8442 - val_loss: 0.4630 - val_acc: 0.7975\n",
      "Epoch 101/150\n",
      "443/443 [==============================] - 0s - loss: 0.4263 - acc: 0.8194 - val_loss: 0.4721 - val_acc: 0.7848\n",
      "Epoch 102/150\n",
      "443/443 [==============================] - 0s - loss: 0.4148 - acc: 0.8194 - val_loss: 0.4652 - val_acc: 0.7975\n",
      "Epoch 103/150\n",
      "443/443 [==============================] - 0s - loss: 0.4132 - acc: 0.8262 - val_loss: 0.4923 - val_acc: 0.7848\n",
      "Epoch 104/150\n",
      "443/443 [==============================] - 0s - loss: 0.4113 - acc: 0.8307 - val_loss: 0.4490 - val_acc: 0.7975\n",
      "Epoch 105/150\n",
      "443/443 [==============================] - 0s - loss: 0.4055 - acc: 0.8217 - val_loss: 0.4766 - val_acc: 0.7722\n",
      "Epoch 106/150\n",
      "443/443 [==============================] - 0s - loss: 0.3899 - acc: 0.8352 - val_loss: 0.4987 - val_acc: 0.7848\n",
      "Epoch 107/150\n",
      "443/443 [==============================] - 0s - loss: 0.3951 - acc: 0.8420 - val_loss: 0.4625 - val_acc: 0.8101\n",
      "Epoch 108/150\n",
      "443/443 [==============================] - 0s - loss: 0.3766 - acc: 0.8442 - val_loss: 0.4577 - val_acc: 0.7848\n",
      "Epoch 109/150\n",
      "443/443 [==============================] - 0s - loss: 0.3960 - acc: 0.8352 - val_loss: 0.4521 - val_acc: 0.7975\n",
      "Epoch 110/150\n",
      "443/443 [==============================] - 0s - loss: 0.4032 - acc: 0.8420 - val_loss: 0.4435 - val_acc: 0.7975\n",
      "Epoch 111/150\n",
      "443/443 [==============================] - 0s - loss: 0.3982 - acc: 0.8533 - val_loss: 0.4772 - val_acc: 0.7722\n",
      "Epoch 112/150\n",
      "443/443 [==============================] - 0s - loss: 0.3868 - acc: 0.8330 - val_loss: 0.4577 - val_acc: 0.7975\n",
      "Epoch 113/150\n",
      "443/443 [==============================] - 0s - loss: 0.4209 - acc: 0.8149 - val_loss: 0.4490 - val_acc: 0.8101\n",
      "Epoch 114/150\n",
      "443/443 [==============================] - 0s - loss: 0.3789 - acc: 0.8555 - val_loss: 0.4337 - val_acc: 0.7975\n",
      "Epoch 115/150\n",
      "443/443 [==============================] - 0s - loss: 0.3850 - acc: 0.8375 - val_loss: 0.4455 - val_acc: 0.8101\n",
      "Epoch 116/150\n",
      "443/443 [==============================] - 0s - loss: 0.4062 - acc: 0.8420 - val_loss: 0.4465 - val_acc: 0.7848\n",
      "Epoch 117/150\n",
      "443/443 [==============================] - 0s - loss: 0.3716 - acc: 0.8736 - val_loss: 0.4435 - val_acc: 0.8228\n",
      "Epoch 118/150\n",
      "443/443 [==============================] - 0s - loss: 0.3716 - acc: 0.8510 - val_loss: 0.4482 - val_acc: 0.7722\n",
      "Epoch 119/150\n",
      "443/443 [==============================] - 0s - loss: 0.3826 - acc: 0.8375 - val_loss: 0.4462 - val_acc: 0.8101\n",
      "Epoch 120/150\n",
      "443/443 [==============================] - 0s - loss: 0.4122 - acc: 0.8330 - val_loss: 0.4382 - val_acc: 0.8228\n",
      "Epoch 121/150\n",
      "443/443 [==============================] - 0s - loss: 0.3934 - acc: 0.8330 - val_loss: 0.4313 - val_acc: 0.8228\n",
      "Epoch 122/150\n",
      "443/443 [==============================] - 0s - loss: 0.3724 - acc: 0.8600 - val_loss: 0.4332 - val_acc: 0.8228\n",
      "Epoch 123/150\n",
      "443/443 [==============================] - 0s - loss: 0.3933 - acc: 0.8488 - val_loss: 0.4533 - val_acc: 0.8101\n",
      "Epoch 124/150\n",
      "443/443 [==============================] - 0s - loss: 0.3604 - acc: 0.8578 - val_loss: 0.4289 - val_acc: 0.7975\n",
      "Epoch 125/150\n",
      "443/443 [==============================] - 0s - loss: 0.3687 - acc: 0.8465 - val_loss: 0.4251 - val_acc: 0.8101\n",
      "Epoch 126/150\n",
      "443/443 [==============================] - 0s - loss: 0.3782 - acc: 0.8442 - val_loss: 0.4252 - val_acc: 0.8101\n",
      "Epoch 127/150\n",
      "443/443 [==============================] - 0s - loss: 0.3686 - acc: 0.8488 - val_loss: 0.4326 - val_acc: 0.8101\n",
      "Epoch 128/150\n",
      "443/443 [==============================] - 0s - loss: 0.3820 - acc: 0.8488 - val_loss: 0.4334 - val_acc: 0.8228\n",
      "Epoch 129/150\n",
      "443/443 [==============================] - 0s - loss: 0.3831 - acc: 0.8420 - val_loss: 0.4182 - val_acc: 0.8101\n",
      "Epoch 130/150\n",
      "443/443 [==============================] - 0s - loss: 0.3638 - acc: 0.8600 - val_loss: 0.4171 - val_acc: 0.7975\n",
      "Epoch 131/150\n",
      "443/443 [==============================] - 0s - loss: 0.3775 - acc: 0.8488 - val_loss: 0.4136 - val_acc: 0.8101\n",
      "Epoch 132/150\n",
      "443/443 [==============================] - 0s - loss: 0.3702 - acc: 0.8646 - val_loss: 0.4147 - val_acc: 0.8101\n",
      "Epoch 133/150\n",
      "443/443 [==============================] - 0s - loss: 0.3539 - acc: 0.8578 - val_loss: 0.4173 - val_acc: 0.8101\n",
      "Epoch 134/150\n",
      "443/443 [==============================] - 0s - loss: 0.3611 - acc: 0.8442 - val_loss: 0.4075 - val_acc: 0.8228\n",
      "Epoch 135/150\n",
      "443/443 [==============================] - 0s - loss: 0.3631 - acc: 0.8578 - val_loss: 0.4270 - val_acc: 0.7848\n",
      "Epoch 136/150\n",
      "443/443 [==============================] - 0s - loss: 0.3668 - acc: 0.8510 - val_loss: 0.3953 - val_acc: 0.8354\n",
      "Epoch 137/150\n",
      "443/443 [==============================] - 0s - loss: 0.3419 - acc: 0.8533 - val_loss: 0.4000 - val_acc: 0.8354\n",
      "Epoch 138/150\n",
      "443/443 [==============================] - 0s - loss: 0.3361 - acc: 0.8691 - val_loss: 0.4179 - val_acc: 0.8101\n",
      "Epoch 139/150\n",
      "443/443 [==============================] - 0s - loss: 0.3545 - acc: 0.8646 - val_loss: 0.4307 - val_acc: 0.7975\n",
      "Epoch 140/150\n",
      "443/443 [==============================] - 0s - loss: 0.3468 - acc: 0.8691 - val_loss: 0.4077 - val_acc: 0.8101\n",
      "Epoch 141/150\n",
      "443/443 [==============================] - 0s - loss: 0.3587 - acc: 0.8736 - val_loss: 0.4019 - val_acc: 0.8228\n",
      "Epoch 142/150\n",
      "443/443 [==============================] - 0s - loss: 0.3582 - acc: 0.8623 - val_loss: 0.4083 - val_acc: 0.8228\n",
      "Epoch 143/150\n",
      "443/443 [==============================] - 0s - loss: 0.3293 - acc: 0.8781 - val_loss: 0.4051 - val_acc: 0.8228\n",
      "Epoch 144/150\n",
      "443/443 [==============================] - 0s - loss: 0.3372 - acc: 0.8713 - val_loss: 0.4042 - val_acc: 0.8101\n",
      "Epoch 145/150\n",
      "443/443 [==============================] - 0s - loss: 0.3337 - acc: 0.8646 - val_loss: 0.4149 - val_acc: 0.8101\n",
      "Epoch 146/150\n",
      "443/443 [==============================] - 0s - loss: 0.3272 - acc: 0.8646 - val_loss: 0.4035 - val_acc: 0.8101\n",
      "Epoch 147/150\n",
      "443/443 [==============================] - 0s - loss: 0.3463 - acc: 0.8646 - val_loss: 0.4012 - val_acc: 0.8354\n",
      "Epoch 148/150\n",
      "443/443 [==============================] - 0s - loss: 0.3362 - acc: 0.8646 - val_loss: 0.4104 - val_acc: 0.8101\n",
      "Epoch 149/150\n",
      "443/443 [==============================] - 0s - loss: 0.3446 - acc: 0.8781 - val_loss: 0.4118 - val_acc: 0.8101\n",
      "Epoch 150/150\n",
      "443/443 [==============================] - 0s - loss: 0.3300 - acc: 0.8713 - val_loss: 0.3927 - val_acc: 0.8354\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fec4e116f10>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 50\n",
    "epochs = 150\n",
    "earlyStopping=keras.callbacks.EarlyStopping(monitor='val_loss', patience=15, verbose=0, mode='auto')\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['acc'])\n",
    "model.fit(x_train, y_train, nb_epoch=epochs,batch_size=batch_size, \n",
    "          callbacks=[earlyStopping], shuffle=True, validation_data = (x_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 0s     \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[7.2713375091552734, 0.21000000089406967]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_test, y_test, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
