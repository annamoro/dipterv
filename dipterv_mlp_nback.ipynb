{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import os\n",
    "import numpy as np\n",
    "from numpy import newaxis\n",
    "from sklearn import preprocessing\n",
    "np.random.seed(1337)\n",
    "\n",
    "import keras\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.layers import Dense, Input, Flatten\n",
    "from keras.layers import Dropout\n",
    "from keras.models import Model,Sequential\n",
    "import sys\n",
    "\n",
    "import pandas as pd\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data read started...\n",
      "Data read finished.\n",
      "(828, 11)\n"
     ]
    }
   ],
   "source": [
    "print(\"Data read started...\")\n",
    "data = pd.read_csv(\"result_nback.csv\")\n",
    "data = data.as_matrix()\n",
    "print (\"Data read finished.\")\n",
    "\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eliminate EEG data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(828, 7)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range (1,5):\n",
    "    data = np.delete(data, 1, 1) \n",
    "\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['nback gyakorlas', 7458.8, 933.41, 352.6, 4.9275, 307.67, 343.79], dtype=object)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dictionary for the levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "level=[\"nback gyakorlas\",\"0back\",\"1back\",\"2back\",\"3back\"]\n",
    "level2int = dict((p, i) for i, p in enumerate(level))\n",
    "int2level = dict((i, p) for i, p in enumerate(level))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Z normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/amoro/anaconda2/lib/python2.7/site-packages/sklearn/utils/validation.py:420: DataConversionWarning: Data with input dtype object was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/home/amoro/anaconda2/lib/python2.7/site-packages/sklearn/utils/validation.py:420: DataConversionWarning: Data with input dtype object was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/home/amoro/anaconda2/lib/python2.7/site-packages/sklearn/utils/validation.py:420: DataConversionWarning: Data with input dtype object was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/home/amoro/anaconda2/lib/python2.7/site-packages/sklearn/utils/validation.py:420: DataConversionWarning: Data with input dtype object was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/home/amoro/anaconda2/lib/python2.7/site-packages/sklearn/utils/validation.py:420: DataConversionWarning: Data with input dtype object was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/home/amoro/anaconda2/lib/python2.7/site-packages/sklearn/utils/validation.py:420: DataConversionWarning: Data with input dtype object was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "for i in range (1,7):\n",
    "    data[:, i] = preprocessing.scale(data[:, i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(len(data)):\n",
    "    data[i, 0] = level2int[data[i, 0]]\n",
    "\n",
    "x_data = data[:, 1:]\n",
    "y_data = data[:, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.  0.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0.  0.  1.]\n",
      " [ 0.  0.  0.  0.  1.]\n",
      " [ 0.  0.  0.  0.  1.]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "ohe = OneHotEncoder()\n",
    "y_one_hot = ohe.fit_transform(y_data.reshape(-1,1)).toarray()\n",
    "print(y_one_hot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shuffle data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "indices = np.arange(x_data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "x_data = x_data[indices]\n",
    "y_one_hot = y_one_hot[indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Divide into train, validation and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len_data = len(x_data)\n",
    "\n",
    "nb_test = int(len_data*0.15)\n",
    "nb_validation = int(len_data*0.15)\n",
    "nb_train = int(len_data*0.7)\n",
    "\n",
    "end_valid = nb_train+nb_validation\n",
    "\n",
    "x_train = x_data[0:nb_train]\n",
    "y_train = y_one_hot[0:nb_train]\n",
    "\n",
    "x_valid = x_data[nb_train:end_valid]\n",
    "y_valid = y_one_hot[nb_train:end_valid]\n",
    "\n",
    "x_test = x_data[end_valid:]\n",
    "y_test = y_one_hot[end_valid:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(579, 6) (579, 5) (124, 6) (124, 5) (125, 6) (125, 5)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape, y_train.shape, x_valid.shape, y_valid.shape, x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(200, input_shape=(6,)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(200, activation='relu'))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(200, activation='relu'))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(5, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 579 samples, validate on 124 samples\n",
      "Epoch 1/150\n",
      "579/579 [==============================] - 0s - loss: 1.4905 - acc: 0.3368 - val_loss: 1.3790 - val_acc: 0.3871\n",
      "Epoch 2/150\n",
      "579/579 [==============================] - 0s - loss: 1.3396 - acc: 0.4301 - val_loss: 1.2792 - val_acc: 0.4355\n",
      "Epoch 3/150\n",
      "579/579 [==============================] - 0s - loss: 1.2751 - acc: 0.4629 - val_loss: 1.2472 - val_acc: 0.4516\n",
      "Epoch 4/150\n",
      "579/579 [==============================] - 0s - loss: 1.2383 - acc: 0.5043 - val_loss: 1.1998 - val_acc: 0.4597\n",
      "Epoch 5/150\n",
      "579/579 [==============================] - 0s - loss: 1.2004 - acc: 0.4836 - val_loss: 1.1819 - val_acc: 0.5000\n",
      "Epoch 6/150\n",
      "579/579 [==============================] - 0s - loss: 1.1773 - acc: 0.4974 - val_loss: 1.1390 - val_acc: 0.5242\n",
      "Epoch 7/150\n",
      "579/579 [==============================] - 0s - loss: 1.1222 - acc: 0.5250 - val_loss: 1.1074 - val_acc: 0.5242\n",
      "Epoch 8/150\n",
      "579/579 [==============================] - 0s - loss: 1.0906 - acc: 0.5371 - val_loss: 1.0700 - val_acc: 0.5323\n",
      "Epoch 9/150\n",
      "579/579 [==============================] - 0s - loss: 1.0867 - acc: 0.5320 - val_loss: 1.0411 - val_acc: 0.5323\n",
      "Epoch 10/150\n",
      "579/579 [==============================] - 0s - loss: 1.0504 - acc: 0.5699 - val_loss: 1.0098 - val_acc: 0.6371\n",
      "Epoch 11/150\n",
      "579/579 [==============================] - 0s - loss: 1.0184 - acc: 0.5976 - val_loss: 0.9935 - val_acc: 0.5726\n",
      "Epoch 12/150\n",
      "579/579 [==============================] - 0s - loss: 1.0058 - acc: 0.5717 - val_loss: 0.9688 - val_acc: 0.5968\n",
      "Epoch 13/150\n",
      "579/579 [==============================] - 0s - loss: 0.9582 - acc: 0.6062 - val_loss: 0.9469 - val_acc: 0.6371\n",
      "Epoch 14/150\n",
      "579/579 [==============================] - 0s - loss: 0.9962 - acc: 0.5993 - val_loss: 0.9601 - val_acc: 0.6210\n",
      "Epoch 15/150\n",
      "579/579 [==============================] - 0s - loss: 0.9317 - acc: 0.6218 - val_loss: 0.9147 - val_acc: 0.6371\n",
      "Epoch 16/150\n",
      "579/579 [==============================] - 0s - loss: 0.8851 - acc: 0.6459 - val_loss: 0.9044 - val_acc: 0.6371\n",
      "Epoch 17/150\n",
      "579/579 [==============================] - 0s - loss: 0.8600 - acc: 0.6494 - val_loss: 0.9001 - val_acc: 0.6452\n",
      "Epoch 18/150\n",
      "579/579 [==============================] - 0s - loss: 0.8701 - acc: 0.6598 - val_loss: 0.8844 - val_acc: 0.6371\n",
      "Epoch 19/150\n",
      "579/579 [==============================] - 0s - loss: 0.8783 - acc: 0.6477 - val_loss: 0.8823 - val_acc: 0.6855\n",
      "Epoch 20/150\n",
      "579/579 [==============================] - 0s - loss: 0.8510 - acc: 0.6494 - val_loss: 0.8974 - val_acc: 0.5968\n",
      "Epoch 21/150\n",
      "579/579 [==============================] - 0s - loss: 0.8384 - acc: 0.6598 - val_loss: 0.8566 - val_acc: 0.6613\n",
      "Epoch 22/150\n",
      "579/579 [==============================] - 0s - loss: 0.8136 - acc: 0.6788 - val_loss: 0.8693 - val_acc: 0.6613\n",
      "Epoch 23/150\n",
      "579/579 [==============================] - 0s - loss: 0.8079 - acc: 0.6736 - val_loss: 0.8325 - val_acc: 0.7016\n",
      "Epoch 24/150\n",
      "579/579 [==============================] - 0s - loss: 0.8025 - acc: 0.6874 - val_loss: 0.8228 - val_acc: 0.7097\n",
      "Epoch 25/150\n",
      "579/579 [==============================] - 0s - loss: 0.7712 - acc: 0.6891 - val_loss: 0.8291 - val_acc: 0.7016\n",
      "Epoch 26/150\n",
      "579/579 [==============================] - 0s - loss: 0.7793 - acc: 0.6926 - val_loss: 0.8676 - val_acc: 0.6452\n",
      "Epoch 27/150\n",
      "579/579 [==============================] - 0s - loss: 0.7966 - acc: 0.6701 - val_loss: 0.8082 - val_acc: 0.6774\n",
      "Epoch 28/150\n",
      "579/579 [==============================] - 0s - loss: 0.7434 - acc: 0.7081 - val_loss: 0.7928 - val_acc: 0.7016\n",
      "Epoch 29/150\n",
      "579/579 [==============================] - 0s - loss: 0.7341 - acc: 0.7116 - val_loss: 0.7928 - val_acc: 0.7016\n",
      "Epoch 30/150\n",
      "579/579 [==============================] - 0s - loss: 0.7617 - acc: 0.6649 - val_loss: 0.7753 - val_acc: 0.7177\n",
      "Epoch 31/150\n",
      "579/579 [==============================] - 0s - loss: 0.7215 - acc: 0.6926 - val_loss: 0.7923 - val_acc: 0.6935\n",
      "Epoch 32/150\n",
      "579/579 [==============================] - 0s - loss: 0.7280 - acc: 0.7185 - val_loss: 0.7752 - val_acc: 0.7177\n",
      "Epoch 33/150\n",
      "579/579 [==============================] - 0s - loss: 0.7143 - acc: 0.7116 - val_loss: 0.7402 - val_acc: 0.7097\n",
      "Epoch 34/150\n",
      "579/579 [==============================] - 0s - loss: 0.6854 - acc: 0.7098 - val_loss: 0.7528 - val_acc: 0.7339\n",
      "Epoch 35/150\n",
      "579/579 [==============================] - 0s - loss: 0.7110 - acc: 0.7219 - val_loss: 0.7737 - val_acc: 0.6855\n",
      "Epoch 36/150\n",
      "579/579 [==============================] - 0s - loss: 0.6627 - acc: 0.7392 - val_loss: 0.7372 - val_acc: 0.6935\n",
      "Epoch 37/150\n",
      "579/579 [==============================] - 0s - loss: 0.7042 - acc: 0.7219 - val_loss: 0.7181 - val_acc: 0.6935\n",
      "Epoch 38/150\n",
      "579/579 [==============================] - 0s - loss: 0.6530 - acc: 0.7392 - val_loss: 0.7116 - val_acc: 0.7177\n",
      "Epoch 39/150\n",
      "579/579 [==============================] - 0s - loss: 0.6525 - acc: 0.7340 - val_loss: 0.7011 - val_acc: 0.7097\n",
      "Epoch 40/150\n",
      "579/579 [==============================] - 0s - loss: 0.5821 - acc: 0.7772 - val_loss: 0.6975 - val_acc: 0.7339\n",
      "Epoch 41/150\n",
      "579/579 [==============================] - 0s - loss: 0.6246 - acc: 0.7789 - val_loss: 0.7037 - val_acc: 0.7258\n",
      "Epoch 42/150\n",
      "579/579 [==============================] - 0s - loss: 0.6162 - acc: 0.7409 - val_loss: 0.6893 - val_acc: 0.7339\n",
      "Epoch 43/150\n",
      "579/579 [==============================] - 0s - loss: 0.6135 - acc: 0.7513 - val_loss: 0.6943 - val_acc: 0.7661\n",
      "Epoch 44/150\n",
      "579/579 [==============================] - 0s - loss: 0.6590 - acc: 0.7288 - val_loss: 0.7164 - val_acc: 0.7016\n",
      "Epoch 45/150\n",
      "579/579 [==============================] - 0s - loss: 0.5966 - acc: 0.7634 - val_loss: 0.6409 - val_acc: 0.7500\n",
      "Epoch 46/150\n",
      "579/579 [==============================] - 0s - loss: 0.6090 - acc: 0.7668 - val_loss: 0.6683 - val_acc: 0.7661\n",
      "Epoch 47/150\n",
      "579/579 [==============================] - 0s - loss: 0.6036 - acc: 0.7478 - val_loss: 0.6878 - val_acc: 0.7339\n",
      "Epoch 48/150\n",
      "579/579 [==============================] - 0s - loss: 0.6072 - acc: 0.7582 - val_loss: 0.6783 - val_acc: 0.7339\n",
      "Epoch 49/150\n",
      "579/579 [==============================] - 0s - loss: 0.5585 - acc: 0.7755 - val_loss: 0.6375 - val_acc: 0.7258\n",
      "Epoch 50/150\n",
      "579/579 [==============================] - 0s - loss: 0.5554 - acc: 0.7668 - val_loss: 0.6493 - val_acc: 0.7177\n",
      "Epoch 51/150\n",
      "579/579 [==============================] - 0s - loss: 0.5635 - acc: 0.7824 - val_loss: 0.6244 - val_acc: 0.7419\n",
      "Epoch 52/150\n",
      "579/579 [==============================] - 0s - loss: 0.5820 - acc: 0.7668 - val_loss: 0.6513 - val_acc: 0.7339\n",
      "Epoch 53/150\n",
      "579/579 [==============================] - 0s - loss: 0.6076 - acc: 0.7547 - val_loss: 0.6446 - val_acc: 0.7581\n",
      "Epoch 54/150\n",
      "579/579 [==============================] - 0s - loss: 0.5258 - acc: 0.7927 - val_loss: 0.6214 - val_acc: 0.7581\n",
      "Epoch 55/150\n",
      "579/579 [==============================] - 0s - loss: 0.5750 - acc: 0.7651 - val_loss: 0.6422 - val_acc: 0.7661\n",
      "Epoch 56/150\n",
      "579/579 [==============================] - 0s - loss: 0.5182 - acc: 0.7979 - val_loss: 0.6380 - val_acc: 0.7097\n",
      "Epoch 57/150\n",
      "579/579 [==============================] - 0s - loss: 0.5186 - acc: 0.7789 - val_loss: 0.6122 - val_acc: 0.7419\n",
      "Epoch 58/150\n",
      "579/579 [==============================] - 0s - loss: 0.5295 - acc: 0.7858 - val_loss: 0.6333 - val_acc: 0.7419\n",
      "Epoch 59/150\n",
      "579/579 [==============================] - 0s - loss: 0.5206 - acc: 0.7979 - val_loss: 0.6433 - val_acc: 0.7419\n",
      "Epoch 60/150\n",
      "579/579 [==============================] - 0s - loss: 0.5140 - acc: 0.7945 - val_loss: 0.6329 - val_acc: 0.7742\n",
      "Epoch 61/150\n",
      "579/579 [==============================] - 0s - loss: 0.5192 - acc: 0.7979 - val_loss: 0.6759 - val_acc: 0.6935\n",
      "Epoch 62/150\n",
      "579/579 [==============================] - 0s - loss: 0.5224 - acc: 0.7737 - val_loss: 0.6257 - val_acc: 0.7661\n",
      "Epoch 63/150\n",
      "579/579 [==============================] - 0s - loss: 0.5150 - acc: 0.7893 - val_loss: 0.6450 - val_acc: 0.7016\n",
      "Epoch 64/150\n",
      "579/579 [==============================] - 0s - loss: 0.4845 - acc: 0.7910 - val_loss: 0.6232 - val_acc: 0.7419\n",
      "Epoch 65/150\n",
      "579/579 [==============================] - 0s - loss: 0.5090 - acc: 0.8066 - val_loss: 0.6528 - val_acc: 0.7581\n",
      "Epoch 66/150\n",
      "579/579 [==============================] - 0s - loss: 0.4872 - acc: 0.7789 - val_loss: 0.5902 - val_acc: 0.7823\n",
      "Epoch 67/150\n",
      "579/579 [==============================] - 0s - loss: 0.5150 - acc: 0.7927 - val_loss: 0.5989 - val_acc: 0.7419\n",
      "Epoch 68/150\n",
      "579/579 [==============================] - 0s - loss: 0.4952 - acc: 0.7927 - val_loss: 0.6090 - val_acc: 0.7419\n",
      "Epoch 69/150\n",
      "579/579 [==============================] - 0s - loss: 0.4648 - acc: 0.8204 - val_loss: 0.5827 - val_acc: 0.7823\n",
      "Epoch 70/150\n",
      "579/579 [==============================] - 0s - loss: 0.4611 - acc: 0.8100 - val_loss: 0.5941 - val_acc: 0.7661\n",
      "Epoch 71/150\n",
      "579/579 [==============================] - 0s - loss: 0.5219 - acc: 0.7858 - val_loss: 0.6307 - val_acc: 0.7500\n",
      "Epoch 72/150\n",
      "579/579 [==============================] - 0s - loss: 0.5492 - acc: 0.7755 - val_loss: 0.6195 - val_acc: 0.7177\n",
      "Epoch 73/150\n",
      "579/579 [==============================] - 0s - loss: 0.4989 - acc: 0.7910 - val_loss: 0.6350 - val_acc: 0.7661\n",
      "Epoch 74/150\n",
      "579/579 [==============================] - 0s - loss: 0.4845 - acc: 0.7979 - val_loss: 0.6181 - val_acc: 0.7339\n",
      "Epoch 75/150\n",
      "579/579 [==============================] - 0s - loss: 0.5225 - acc: 0.7876 - val_loss: 0.6393 - val_acc: 0.7903\n",
      "Epoch 76/150\n",
      "579/579 [==============================] - 0s - loss: 0.4940 - acc: 0.7997 - val_loss: 0.5783 - val_acc: 0.7581\n",
      "Epoch 77/150\n",
      "579/579 [==============================] - 0s - loss: 0.4595 - acc: 0.7979 - val_loss: 0.5602 - val_acc: 0.8145\n",
      "Epoch 78/150\n",
      "579/579 [==============================] - 0s - loss: 0.4310 - acc: 0.8377 - val_loss: 0.5878 - val_acc: 0.7984\n",
      "Epoch 79/150\n",
      "579/579 [==============================] - 0s - loss: 0.4311 - acc: 0.8238 - val_loss: 0.5819 - val_acc: 0.7823\n",
      "Epoch 80/150\n",
      "579/579 [==============================] - 0s - loss: 0.4222 - acc: 0.8256 - val_loss: 0.5592 - val_acc: 0.7823\n",
      "Epoch 81/150\n",
      "579/579 [==============================] - 0s - loss: 0.4336 - acc: 0.8204 - val_loss: 0.5619 - val_acc: 0.8306\n",
      "Epoch 82/150\n",
      "579/579 [==============================] - 0s - loss: 0.4097 - acc: 0.8411 - val_loss: 0.5760 - val_acc: 0.7984\n",
      "Epoch 83/150\n",
      "579/579 [==============================] - 0s - loss: 0.4251 - acc: 0.8187 - val_loss: 0.5692 - val_acc: 0.7500\n",
      "Epoch 84/150\n",
      "579/579 [==============================] - 0s - loss: 0.4124 - acc: 0.8342 - val_loss: 0.5834 - val_acc: 0.7903\n",
      "Epoch 85/150\n",
      "579/579 [==============================] - 0s - loss: 0.4385 - acc: 0.8273 - val_loss: 0.5765 - val_acc: 0.7823\n",
      "Epoch 86/150\n",
      "579/579 [==============================] - 0s - loss: 0.4285 - acc: 0.8204 - val_loss: 0.5443 - val_acc: 0.7742\n",
      "Epoch 87/150\n",
      "579/579 [==============================] - 0s - loss: 0.4259 - acc: 0.8394 - val_loss: 0.5565 - val_acc: 0.7742\n",
      "Epoch 88/150\n",
      "579/579 [==============================] - 0s - loss: 0.4230 - acc: 0.8290 - val_loss: 0.5503 - val_acc: 0.7984\n",
      "Epoch 89/150\n",
      "579/579 [==============================] - 0s - loss: 0.4371 - acc: 0.8221 - val_loss: 0.5586 - val_acc: 0.7823\n",
      "Epoch 90/150\n",
      "579/579 [==============================] - 0s - loss: 0.4646 - acc: 0.8083 - val_loss: 0.5334 - val_acc: 0.7984\n",
      "Epoch 91/150\n",
      "579/579 [==============================] - 0s - loss: 0.4179 - acc: 0.8394 - val_loss: 0.5567 - val_acc: 0.7903\n",
      "Epoch 92/150\n",
      "579/579 [==============================] - 0s - loss: 0.4130 - acc: 0.8273 - val_loss: 0.5340 - val_acc: 0.8145\n",
      "Epoch 93/150\n",
      "579/579 [==============================] - 0s - loss: 0.3950 - acc: 0.8342 - val_loss: 0.5556 - val_acc: 0.7984\n",
      "Epoch 94/150\n",
      "579/579 [==============================] - 0s - loss: 0.4519 - acc: 0.8135 - val_loss: 0.5908 - val_acc: 0.7500\n",
      "Epoch 95/150\n",
      "579/579 [==============================] - 0s - loss: 0.4388 - acc: 0.8204 - val_loss: 0.5825 - val_acc: 0.7661\n",
      "Epoch 96/150\n",
      "579/579 [==============================] - 0s - loss: 0.3982 - acc: 0.8446 - val_loss: 0.5227 - val_acc: 0.8145\n",
      "Epoch 97/150\n",
      "579/579 [==============================] - 0s - loss: 0.3984 - acc: 0.8290 - val_loss: 0.5507 - val_acc: 0.7823\n",
      "Epoch 98/150\n",
      "579/579 [==============================] - 0s - loss: 0.3937 - acc: 0.8497 - val_loss: 0.5449 - val_acc: 0.7661\n",
      "Epoch 99/150\n",
      "579/579 [==============================] - 0s - loss: 0.3958 - acc: 0.8256 - val_loss: 0.5590 - val_acc: 0.7339\n",
      "Epoch 100/150\n",
      "579/579 [==============================] - 0s - loss: 0.3913 - acc: 0.8515 - val_loss: 0.5142 - val_acc: 0.8145\n",
      "Epoch 101/150\n",
      "579/579 [==============================] - 0s - loss: 0.3804 - acc: 0.8342 - val_loss: 0.5358 - val_acc: 0.7823\n",
      "Epoch 102/150\n",
      "579/579 [==============================] - 0s - loss: 0.3930 - acc: 0.8359 - val_loss: 0.5125 - val_acc: 0.7984\n",
      "Epoch 103/150\n",
      "579/579 [==============================] - 0s - loss: 0.3728 - acc: 0.8670 - val_loss: 0.5190 - val_acc: 0.7984\n",
      "Epoch 104/150\n",
      "579/579 [==============================] - 0s - loss: 0.3857 - acc: 0.8411 - val_loss: 0.5376 - val_acc: 0.7903\n",
      "Epoch 105/150\n",
      "579/579 [==============================] - 0s - loss: 0.3889 - acc: 0.8532 - val_loss: 0.5432 - val_acc: 0.7823\n",
      "Epoch 106/150\n",
      "579/579 [==============================] - 0s - loss: 0.4006 - acc: 0.8342 - val_loss: 0.5147 - val_acc: 0.8145\n",
      "Epoch 107/150\n",
      "579/579 [==============================] - 0s - loss: 0.3842 - acc: 0.8377 - val_loss: 0.5466 - val_acc: 0.7661\n",
      "Epoch 108/150\n",
      "579/579 [==============================] - 0s - loss: 0.3748 - acc: 0.8618 - val_loss: 0.5410 - val_acc: 0.8145\n",
      "Epoch 109/150\n",
      "579/579 [==============================] - 0s - loss: 0.4029 - acc: 0.8290 - val_loss: 0.5444 - val_acc: 0.7823\n",
      "Epoch 110/150\n",
      "579/579 [==============================] - 0s - loss: 0.3690 - acc: 0.8601 - val_loss: 0.5420 - val_acc: 0.8387\n",
      "Epoch 111/150\n",
      "579/579 [==============================] - 0s - loss: 0.3731 - acc: 0.8480 - val_loss: 0.5447 - val_acc: 0.7823\n",
      "Epoch 112/150\n",
      "579/579 [==============================] - 0s - loss: 0.4010 - acc: 0.8377 - val_loss: 0.5814 - val_acc: 0.7984\n",
      "Epoch 113/150\n",
      "579/579 [==============================] - 0s - loss: 0.3757 - acc: 0.8394 - val_loss: 0.5174 - val_acc: 0.7984\n",
      "Epoch 114/150\n",
      "579/579 [==============================] - 0s - loss: 0.3740 - acc: 0.8342 - val_loss: 0.5589 - val_acc: 0.7823\n",
      "Epoch 115/150\n",
      "579/579 [==============================] - 0s - loss: 0.3467 - acc: 0.8687 - val_loss: 0.5688 - val_acc: 0.7984\n",
      "Epoch 116/150\n",
      "579/579 [==============================] - 0s - loss: 0.3959 - acc: 0.8359 - val_loss: 0.5093 - val_acc: 0.8145\n",
      "Epoch 117/150\n",
      "579/579 [==============================] - 0s - loss: 0.3597 - acc: 0.8532 - val_loss: 0.4948 - val_acc: 0.8387\n",
      "Epoch 118/150\n",
      "579/579 [==============================] - 0s - loss: 0.3857 - acc: 0.8411 - val_loss: 0.5207 - val_acc: 0.8226\n",
      "Epoch 119/150\n",
      "579/579 [==============================] - 0s - loss: 0.3480 - acc: 0.8601 - val_loss: 0.5399 - val_acc: 0.7823\n",
      "Epoch 120/150\n",
      "579/579 [==============================] - 0s - loss: 0.3578 - acc: 0.8497 - val_loss: 0.5445 - val_acc: 0.7903\n",
      "Epoch 121/150\n",
      "579/579 [==============================] - 0s - loss: 0.3620 - acc: 0.8584 - val_loss: 0.5088 - val_acc: 0.7661\n",
      "Epoch 122/150\n",
      "579/579 [==============================] - 0s - loss: 0.3609 - acc: 0.8670 - val_loss: 0.4713 - val_acc: 0.8226\n",
      "Epoch 123/150\n",
      "579/579 [==============================] - 0s - loss: 0.3802 - acc: 0.8428 - val_loss: 0.5357 - val_acc: 0.7500\n",
      "Epoch 124/150\n",
      "579/579 [==============================] - 0s - loss: 0.3519 - acc: 0.8549 - val_loss: 0.4802 - val_acc: 0.8226\n",
      "Epoch 125/150\n",
      "579/579 [==============================] - 0s - loss: 0.3550 - acc: 0.8515 - val_loss: 0.5129 - val_acc: 0.7984\n",
      "Epoch 126/150\n",
      "579/579 [==============================] - 0s - loss: 0.3642 - acc: 0.8497 - val_loss: 0.5057 - val_acc: 0.7984\n",
      "Epoch 127/150\n",
      "579/579 [==============================] - 0s - loss: 0.3595 - acc: 0.8515 - val_loss: 0.4911 - val_acc: 0.8145\n",
      "Epoch 128/150\n",
      "579/579 [==============================] - 0s - loss: 0.3133 - acc: 0.8826 - val_loss: 0.4679 - val_acc: 0.8629\n",
      "Epoch 129/150\n",
      "579/579 [==============================] - 0s - loss: 0.3408 - acc: 0.8549 - val_loss: 0.4874 - val_acc: 0.8387\n",
      "Epoch 130/150\n",
      "579/579 [==============================] - 0s - loss: 0.3387 - acc: 0.8584 - val_loss: 0.4733 - val_acc: 0.8145\n",
      "Epoch 131/150\n",
      "579/579 [==============================] - 0s - loss: 0.3520 - acc: 0.8532 - val_loss: 0.4789 - val_acc: 0.8306\n",
      "Epoch 132/150\n",
      "579/579 [==============================] - 0s - loss: 0.3788 - acc: 0.8428 - val_loss: 0.5356 - val_acc: 0.7984\n",
      "Epoch 133/150\n",
      "579/579 [==============================] - 0s - loss: 0.3304 - acc: 0.8584 - val_loss: 0.5069 - val_acc: 0.8145\n",
      "Epoch 134/150\n",
      "579/579 [==============================] - 0s - loss: 0.3232 - acc: 0.8756 - val_loss: 0.4539 - val_acc: 0.8306\n",
      "Epoch 135/150\n",
      "579/579 [==============================] - 0s - loss: 0.3506 - acc: 0.8601 - val_loss: 0.5083 - val_acc: 0.8306\n",
      "Epoch 136/150\n",
      "579/579 [==============================] - 0s - loss: 0.3384 - acc: 0.8549 - val_loss: 0.4545 - val_acc: 0.8065\n",
      "Epoch 137/150\n",
      "579/579 [==============================] - 0s - loss: 0.3418 - acc: 0.8601 - val_loss: 0.4904 - val_acc: 0.8387\n",
      "Epoch 138/150\n",
      "579/579 [==============================] - 0s - loss: 0.3464 - acc: 0.8618 - val_loss: 0.5180 - val_acc: 0.8226\n",
      "Epoch 139/150\n",
      "579/579 [==============================] - 0s - loss: 0.3827 - acc: 0.8549 - val_loss: 0.5639 - val_acc: 0.7500\n",
      "Epoch 140/150\n",
      "579/579 [==============================] - 0s - loss: 0.3016 - acc: 0.8670 - val_loss: 0.4739 - val_acc: 0.8065\n",
      "Epoch 141/150\n",
      "579/579 [==============================] - 0s - loss: 0.3172 - acc: 0.8722 - val_loss: 0.5350 - val_acc: 0.7903\n",
      "Epoch 142/150\n",
      "579/579 [==============================] - 0s - loss: 0.3376 - acc: 0.8722 - val_loss: 0.5266 - val_acc: 0.7903\n",
      "Epoch 143/150\n",
      "579/579 [==============================] - 0s - loss: 0.3097 - acc: 0.8636 - val_loss: 0.4612 - val_acc: 0.8226\n",
      "Epoch 144/150\n",
      "579/579 [==============================] - 0s - loss: 0.3026 - acc: 0.8774 - val_loss: 0.5280 - val_acc: 0.8387\n",
      "Epoch 145/150\n",
      "579/579 [==============================] - 0s - loss: 0.3156 - acc: 0.8705 - val_loss: 0.4841 - val_acc: 0.8306\n",
      "Epoch 146/150\n",
      "579/579 [==============================] - 0s - loss: 0.2914 - acc: 0.8739 - val_loss: 0.4760 - val_acc: 0.8145\n",
      "Epoch 147/150\n",
      "579/579 [==============================] - 0s - loss: 0.2965 - acc: 0.8774 - val_loss: 0.4972 - val_acc: 0.8065\n",
      "Epoch 148/150\n",
      "579/579 [==============================] - 0s - loss: 0.3370 - acc: 0.8687 - val_loss: 0.5033 - val_acc: 0.7903\n",
      "Epoch 149/150\n",
      "579/579 [==============================] - 0s - loss: 0.3234 - acc: 0.8705 - val_loss: 0.4892 - val_acc: 0.8145\n",
      "Epoch 150/150\n",
      "579/579 [==============================] - 0s - loss: 0.3132 - acc: 0.8705 - val_loss: 0.4807 - val_acc: 0.8145\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fa5a4bdc2d0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 50\n",
    "epochs = 150\n",
    "earlyStopping=keras.callbacks.EarlyStopping(monitor='val_loss', patience=15, verbose=0, mode='auto')\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['acc'])\n",
    "model.fit(x_train, y_train, nb_epoch=epochs,batch_size=batch_size, \n",
    "          callbacks=[earlyStopping], shuffle=True, validation_data = (x_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125/125 [==============================] - 0s     \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.40236178338527678, 0.8639999985694885]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_test, y_test, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 32/125 [======>.......................] - ETA: 0s[2 3 3 3 4 1 1 4 2 2 4 0 4 1 1 3 2 4 3 1 3 3 4 4 3 1 0 1 2 4 1 4 0 2 4 0 1\n",
      " 0 3 0 1 4 1 0 0 4 3 4 4 4 3 4 0 0 4 3 0 4 1 0 2 0 3 1 4 4 2 2 2 0 2 4 4 4\n",
      " 4 3 4 3 1 4 1 4 4 2 4 4 2 1 2 2 2 1 3 4 4 2 3 4 1 3 0 4 2 3 0 4 0 4 0 3 0\n",
      " 4 4 3 2 0 0 0 2 3 3 4 1 4 0]\n",
      " 32/125 [======>.......................] - ETA: 0s                          precision    recall  f1-score   support\n",
      "\n",
      "class 0(nback gyakorlas)       0.83      0.95      0.88        20\n",
      "          class 1(0back)       0.74      0.74      0.74        19\n",
      "          class 2(1back)       0.80      0.80      0.80        20\n",
      "          class 3(2back)       0.91      0.78      0.84        27\n",
      "          class 4(3back)       0.95      0.97      0.96        39\n",
      "\n",
      "             avg / total       0.87      0.86      0.86       125\n",
      "\n",
      "[[19  1  0  0  0]\n",
      " [ 1 14  4  0  0]\n",
      " [ 1  2 16  1  0]\n",
      " [ 2  2  0 21  2]\n",
      " [ 0  0  0  1 38]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "\n",
    "y_pred = model.predict_classes(x_test)\n",
    "print(y_pred)\n",
    "\n",
    "p=model.predict_proba(x_test)\n",
    "\n",
    "target_names = ['class 0(nback gyakorlas)', 'class 1(0back)', 'class 2(1back)', 'class 3(2back)', 'class 4(3back)']\n",
    "print(classification_report(np.argmax(y_test,axis=1), y_pred,target_names=target_names))\n",
    "print(confusion_matrix(np.argmax(y_test,axis=1), y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
