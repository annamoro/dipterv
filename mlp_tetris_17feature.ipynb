{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import os\n",
    "import numpy as np\n",
    "from numpy import newaxis\n",
    "from sklearn import preprocessing\n",
    "np.random.seed(1337)\n",
    "\n",
    "import keras\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.layers import Dense, Input, Flatten\n",
    "from keras.layers import Dropout\n",
    "from keras.models import Model,Sequential\n",
    "import sys\n",
    "\n",
    "import pandas as pd\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data read started...\n",
      "Data read finished.\n",
      "(3560, 19)\n"
     ]
    }
   ],
   "source": [
    "print(\"Data read started...\")\n",
    "data = pd.read_csv(\"tetris_full_1.csv\")\n",
    "data = data.as_matrix()\n",
    "print (\"Data read finished.\")\n",
    "\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eliminate EEG data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3560, 5)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range (1,5):\n",
    "    data = np.delete(data, 1, 1) \n",
    "    \n",
    "\n",
    "data = data[:,0:5]\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['tetris1', 2093.8, 635.76, 27.463, 3.883050847457624], dtype=object)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dictionary for the levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#level=[\"0back\",\"1back\",\"2back\",\"3back\"]\n",
    "level=[\"tetris1\",\"tetris2\",\"tetris3\"]\n",
    "level2int = dict((p, i) for i, p in enumerate(level))\n",
    "int2level = dict((i, p) for i, p in enumerate(level))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Z normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/amoro/anaconda2/lib/python2.7/site-packages/sklearn/utils/validation.py:420: DataConversionWarning: Data with input dtype object was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/home/amoro/anaconda2/lib/python2.7/site-packages/sklearn/utils/validation.py:420: DataConversionWarning: Data with input dtype object was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/home/amoro/anaconda2/lib/python2.7/site-packages/sklearn/utils/validation.py:420: DataConversionWarning: Data with input dtype object was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/home/amoro/anaconda2/lib/python2.7/site-packages/sklearn/utils/validation.py:420: DataConversionWarning: Data with input dtype object was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "for i in range (1,5):\n",
    "    data[:, i] = preprocessing.scale(data[:, i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(len(data)):\n",
    "    data[i, 0] = level2int[data[i, 0]]\n",
    "\n",
    "x_data = data[:, 1:]\n",
    "y_data = data[:, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "ohe = OneHotEncoder()\n",
    "y_one_hot = ohe.fit_transform(y_data.reshape(-1,1)).toarray()\n",
    "print(y_one_hot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shuffle data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "indices = np.arange(x_data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "x_data = x_data[indices]\n",
    "y_one_hot = y_one_hot[indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Divide into train, validation and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len_data = len(x_data)\n",
    "\n",
    "nb_test = int(len_data*0.15)\n",
    "nb_validation = int(len_data*0.15)\n",
    "nb_train = int(len_data*0.7)\n",
    "\n",
    "end_valid = nb_train+nb_validation\n",
    "\n",
    "x_train = x_data[0:nb_train]\n",
    "y_train = y_one_hot[0:nb_train]\n",
    "\n",
    "x_valid = x_data[nb_train:end_valid]\n",
    "y_valid = y_one_hot[nb_train:end_valid]\n",
    "\n",
    "x_test = x_data[end_valid:]\n",
    "y_test = y_one_hot[end_valid:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2492, 4) (2492, 3) (534, 4) (534, 3) (534, 4) (534, 3)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape, y_train.shape, x_valid.shape, y_valid.shape, x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(200, input_shape=(4,)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(200, activation='relu'))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(700, activation='relu'))\n",
    "model.add(Dense(200, activation='relu'))\n",
    "model.add(Dense(200, activation='relu'))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(200, activation='relu'))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(3, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2492 samples, validate on 534 samples\n",
      "Epoch 1/300\n",
      "2492/2492 [==============================] - 1s - loss: 1.0752 - acc: 0.3696 - val_loss: 1.0552 - val_acc: 0.4307\n",
      "Epoch 2/300\n",
      "2492/2492 [==============================] - 0s - loss: 1.0433 - acc: 0.3997 - val_loss: 1.0416 - val_acc: 0.4326\n",
      "Epoch 3/300\n",
      "2492/2492 [==============================] - 0s - loss: 1.0231 - acc: 0.4402 - val_loss: 1.0303 - val_acc: 0.4513\n",
      "Epoch 4/300\n",
      "2492/2492 [==============================] - 0s - loss: 1.0155 - acc: 0.4498 - val_loss: 1.0230 - val_acc: 0.4345\n",
      "Epoch 5/300\n",
      "2492/2492 [==============================] - 0s - loss: 0.9996 - acc: 0.4579 - val_loss: 1.0095 - val_acc: 0.4738\n",
      "Epoch 6/300\n",
      "2492/2492 [==============================] - 0s - loss: 0.9876 - acc: 0.4755 - val_loss: 1.0042 - val_acc: 0.5056\n",
      "Epoch 7/300\n",
      "2492/2492 [==============================] - 0s - loss: 0.9708 - acc: 0.5056 - val_loss: 1.0008 - val_acc: 0.4831\n",
      "Epoch 8/300\n",
      "2492/2492 [==============================] - 0s - loss: 0.9709 - acc: 0.4952 - val_loss: 1.0023 - val_acc: 0.4850\n",
      "Epoch 9/300\n",
      "2492/2492 [==============================] - 0s - loss: 0.9636 - acc: 0.5068 - val_loss: 0.9908 - val_acc: 0.4813\n",
      "Epoch 10/300\n",
      "2492/2492 [==============================] - 0s - loss: 0.9614 - acc: 0.5136 - val_loss: 0.9884 - val_acc: 0.5187\n",
      "Epoch 11/300\n",
      "2492/2492 [==============================] - 0s - loss: 0.9497 - acc: 0.5205 - val_loss: 0.9715 - val_acc: 0.5112\n",
      "Epoch 12/300\n",
      "2492/2492 [==============================] - 0s - loss: 0.9480 - acc: 0.5269 - val_loss: 0.9714 - val_acc: 0.5169\n",
      "Epoch 13/300\n",
      "2492/2492 [==============================] - 0s - loss: 0.9436 - acc: 0.5293 - val_loss: 0.9612 - val_acc: 0.5243\n",
      "Epoch 14/300\n",
      "2492/2492 [==============================] - 0s - loss: 0.9375 - acc: 0.5241 - val_loss: 0.9578 - val_acc: 0.5187\n",
      "Epoch 15/300\n",
      "2492/2492 [==============================] - 0s - loss: 0.9287 - acc: 0.5353 - val_loss: 0.9503 - val_acc: 0.5243\n",
      "Epoch 16/300\n",
      "2492/2492 [==============================] - 0s - loss: 0.9241 - acc: 0.5161 - val_loss: 0.9549 - val_acc: 0.5375\n",
      "Epoch 17/300\n",
      "2492/2492 [==============================] - 0s - loss: 0.9152 - acc: 0.5413 - val_loss: 0.9394 - val_acc: 0.5131\n",
      "Epoch 18/300\n",
      "2492/2492 [==============================] - 0s - loss: 0.9270 - acc: 0.5269 - val_loss: 0.9355 - val_acc: 0.5187\n",
      "Epoch 19/300\n",
      "2492/2492 [==============================] - 0s - loss: 0.9103 - acc: 0.5417 - val_loss: 0.9251 - val_acc: 0.5337\n",
      "Epoch 20/300\n",
      "2492/2492 [==============================] - 0s - loss: 0.9075 - acc: 0.5429 - val_loss: 0.9253 - val_acc: 0.5562\n",
      "Epoch 21/300\n",
      "2492/2492 [==============================] - 0s - loss: 0.8996 - acc: 0.5534 - val_loss: 0.9119 - val_acc: 0.5618\n",
      "Epoch 22/300\n",
      "2492/2492 [==============================] - 0s - loss: 0.8992 - acc: 0.5457 - val_loss: 0.9068 - val_acc: 0.5468\n",
      "Epoch 23/300\n",
      "2492/2492 [==============================] - 0s - loss: 0.8861 - acc: 0.5542 - val_loss: 0.9150 - val_acc: 0.5337\n",
      "Epoch 24/300\n",
      "2492/2492 [==============================] - 0s - loss: 0.8890 - acc: 0.5494 - val_loss: 0.8985 - val_acc: 0.5468\n",
      "Epoch 25/300\n",
      "2492/2492 [==============================] - 0s - loss: 0.8923 - acc: 0.5341 - val_loss: 0.8933 - val_acc: 0.5543\n",
      "Epoch 26/300\n",
      "2492/2492 [==============================] - 0s - loss: 0.8762 - acc: 0.5662 - val_loss: 0.8828 - val_acc: 0.5861\n",
      "Epoch 27/300\n",
      "2492/2492 [==============================] - 0s - loss: 0.8681 - acc: 0.5538 - val_loss: 0.8784 - val_acc: 0.5674\n",
      "Epoch 28/300\n",
      "2492/2492 [==============================] - 0s - loss: 0.8663 - acc: 0.5718 - val_loss: 0.8718 - val_acc: 0.5674\n",
      "Epoch 29/300\n",
      "2492/2492 [==============================] - 0s - loss: 0.8565 - acc: 0.5738 - val_loss: 0.8689 - val_acc: 0.5899\n",
      "Epoch 30/300\n",
      "2492/2492 [==============================] - 0s - loss: 0.8612 - acc: 0.5831 - val_loss: 0.8672 - val_acc: 0.5974\n",
      "Epoch 31/300\n",
      "2492/2492 [==============================] - 0s - loss: 0.8476 - acc: 0.5811 - val_loss: 0.8535 - val_acc: 0.6124\n",
      "Epoch 32/300\n",
      "2492/2492 [==============================] - 0s - loss: 0.8534 - acc: 0.5726 - val_loss: 0.8643 - val_acc: 0.5712\n",
      "Epoch 33/300\n",
      "2492/2492 [==============================] - 0s - loss: 0.8464 - acc: 0.5827 - val_loss: 0.8572 - val_acc: 0.5730\n",
      "Epoch 34/300\n",
      "2492/2492 [==============================] - 0s - loss: 0.8508 - acc: 0.5754 - val_loss: 0.8747 - val_acc: 0.5468\n",
      "Epoch 35/300\n",
      "2492/2492 [==============================] - 0s - loss: 0.8452 - acc: 0.5815 - val_loss: 0.8436 - val_acc: 0.5955\n",
      "Epoch 36/300\n",
      "2492/2492 [==============================] - 0s - loss: 0.8422 - acc: 0.5875 - val_loss: 0.8288 - val_acc: 0.6199\n",
      "Epoch 37/300\n",
      "2492/2492 [==============================] - 0s - loss: 0.8255 - acc: 0.6023 - val_loss: 0.8392 - val_acc: 0.6030\n",
      "Epoch 38/300\n",
      "2492/2492 [==============================] - 0s - loss: 0.8273 - acc: 0.5895 - val_loss: 0.8271 - val_acc: 0.6049\n",
      "Epoch 39/300\n",
      "2492/2492 [==============================] - 0s - loss: 0.8335 - acc: 0.6019 - val_loss: 0.8414 - val_acc: 0.5974\n",
      "Epoch 40/300\n",
      "2492/2492 [==============================] - 0s - loss: 0.8325 - acc: 0.5919 - val_loss: 0.8522 - val_acc: 0.5712\n",
      "Epoch 41/300\n",
      "2492/2492 [==============================] - 0s - loss: 0.8216 - acc: 0.5999 - val_loss: 0.8353 - val_acc: 0.5861\n",
      "Epoch 42/300\n",
      "2492/2492 [==============================] - 0s - loss: 0.8105 - acc: 0.6035 - val_loss: 0.8270 - val_acc: 0.6124\n",
      "Epoch 43/300\n",
      "2492/2492 [==============================] - 0s - loss: 0.8087 - acc: 0.6055 - val_loss: 0.8164 - val_acc: 0.6086\n",
      "Epoch 44/300\n",
      "2492/2492 [==============================] - 0s - loss: 0.8011 - acc: 0.6112 - val_loss: 0.8194 - val_acc: 0.6142\n",
      "Epoch 45/300\n",
      "2492/2492 [==============================] - 0s - loss: 0.7977 - acc: 0.6083 - val_loss: 0.7995 - val_acc: 0.6199\n",
      "Epoch 46/300\n",
      "2492/2492 [==============================] - 0s - loss: 0.7913 - acc: 0.6063 - val_loss: 0.8103 - val_acc: 0.6255\n",
      "Epoch 47/300\n",
      "2492/2492 [==============================] - 0s - loss: 0.7837 - acc: 0.6172 - val_loss: 0.7916 - val_acc: 0.6236\n",
      "Epoch 48/300\n",
      "2492/2492 [==============================] - 0s - loss: 0.7685 - acc: 0.6232 - val_loss: 0.7936 - val_acc: 0.6086\n",
      "Epoch 49/300\n",
      "2492/2492 [==============================] - 0s - loss: 0.7814 - acc: 0.6180 - val_loss: 0.7966 - val_acc: 0.6217\n",
      "Epoch 50/300\n",
      "2492/2492 [==============================] - 0s - loss: 0.7803 - acc: 0.6296 - val_loss: 0.7781 - val_acc: 0.6273\n",
      "Epoch 51/300\n",
      "2492/2492 [==============================] - 0s - loss: 0.7734 - acc: 0.6232 - val_loss: 0.7908 - val_acc: 0.6311\n",
      "Epoch 52/300\n",
      "2492/2492 [==============================] - 0s - loss: 0.7648 - acc: 0.6388 - val_loss: 0.7865 - val_acc: 0.6367\n",
      "Epoch 53/300\n",
      "2492/2492 [==============================] - 0s - loss: 0.7566 - acc: 0.6344 - val_loss: 0.7886 - val_acc: 0.6142\n",
      "Epoch 54/300\n",
      "2492/2492 [==============================] - 0s - loss: 0.7715 - acc: 0.6316 - val_loss: 0.7977 - val_acc: 0.6255\n",
      "Epoch 55/300\n",
      "2492/2492 [==============================] - 0s - loss: 0.7677 - acc: 0.6288 - val_loss: 0.7960 - val_acc: 0.6330\n",
      "Epoch 56/300\n",
      "2492/2492 [==============================] - 0s - loss: 0.7677 - acc: 0.6252 - val_loss: 0.7839 - val_acc: 0.6667\n",
      "Epoch 57/300\n",
      "2492/2492 [==============================] - 0s - loss: 0.7480 - acc: 0.6336 - val_loss: 0.7893 - val_acc: 0.6461\n",
      "Epoch 58/300\n",
      "2492/2492 [==============================] - 0s - loss: 0.7446 - acc: 0.6469 - val_loss: 0.7801 - val_acc: 0.6423\n",
      "Epoch 59/300\n",
      "2492/2492 [==============================] - 0s - loss: 0.7478 - acc: 0.6388 - val_loss: 0.7712 - val_acc: 0.6536\n",
      "Epoch 60/300\n",
      "2492/2492 [==============================] - 0s - loss: 0.7421 - acc: 0.6457 - val_loss: 0.8060 - val_acc: 0.6292\n",
      "Epoch 61/300\n",
      "2492/2492 [==============================] - 0s - loss: 0.7531 - acc: 0.6453 - val_loss: 0.7703 - val_acc: 0.6311\n",
      "Epoch 62/300\n",
      "2492/2492 [==============================] - 0s - loss: 0.7391 - acc: 0.6409 - val_loss: 0.7994 - val_acc: 0.6292\n",
      "Epoch 63/300\n",
      "2492/2492 [==============================] - 0s - loss: 0.7514 - acc: 0.6368 - val_loss: 0.7587 - val_acc: 0.6461\n",
      "Epoch 64/300\n",
      "2492/2492 [==============================] - 0s - loss: 0.7363 - acc: 0.6417 - val_loss: 0.7634 - val_acc: 0.6217\n",
      "Epoch 65/300\n",
      "2492/2492 [==============================] - 0s - loss: 0.7410 - acc: 0.6445 - val_loss: 0.7452 - val_acc: 0.6461\n",
      "Epoch 66/300\n",
      "2492/2492 [==============================] - 0s - loss: 0.7137 - acc: 0.6545 - val_loss: 0.7556 - val_acc: 0.6479\n",
      "Epoch 67/300\n",
      "2492/2492 [==============================] - 0s - loss: 0.7334 - acc: 0.6413 - val_loss: 0.7756 - val_acc: 0.6386\n",
      "Epoch 68/300\n",
      "2492/2492 [==============================] - 0s - loss: 0.7355 - acc: 0.6485 - val_loss: 0.7454 - val_acc: 0.6685\n",
      "Epoch 69/300\n",
      "2492/2492 [==============================] - 0s - loss: 0.7462 - acc: 0.6392 - val_loss: 0.7521 - val_acc: 0.6479\n",
      "Epoch 70/300\n",
      "2492/2492 [==============================] - 0s - loss: 0.7263 - acc: 0.6437 - val_loss: 0.7514 - val_acc: 0.6442\n",
      "Epoch 71/300\n",
      "2492/2492 [==============================] - 0s - loss: 0.7170 - acc: 0.6585 - val_loss: 0.7597 - val_acc: 0.6573\n",
      "Epoch 72/300\n",
      "2492/2492 [==============================] - 0s - loss: 0.7184 - acc: 0.6493 - val_loss: 0.7550 - val_acc: 0.6554\n",
      "Epoch 73/300\n",
      "2492/2492 [==============================] - 0s - loss: 0.7209 - acc: 0.6537 - val_loss: 0.7279 - val_acc: 0.6760\n",
      "Epoch 74/300\n",
      "2492/2492 [==============================] - 0s - loss: 0.7019 - acc: 0.6553 - val_loss: 0.7359 - val_acc: 0.6648\n",
      "Epoch 75/300\n",
      "2492/2492 [==============================] - 0s - loss: 0.7020 - acc: 0.6609 - val_loss: 0.7353 - val_acc: 0.6667\n",
      "Epoch 76/300\n",
      "2492/2492 [==============================] - 0s - loss: 0.7087 - acc: 0.6509 - val_loss: 0.7371 - val_acc: 0.6610\n",
      "Epoch 77/300\n",
      "2492/2492 [==============================] - 0s - loss: 0.7057 - acc: 0.6501 - val_loss: 0.7243 - val_acc: 0.6648\n",
      "Epoch 78/300\n",
      "2492/2492 [==============================] - 0s - loss: 0.7039 - acc: 0.6601 - val_loss: 0.7293 - val_acc: 0.6479\n",
      "Epoch 79/300\n",
      "2492/2492 [==============================] - 0s - loss: 0.6964 - acc: 0.6665 - val_loss: 0.7110 - val_acc: 0.6685\n",
      "Epoch 80/300\n",
      "2492/2492 [==============================] - 0s - loss: 0.6856 - acc: 0.6677 - val_loss: 0.7075 - val_acc: 0.6685\n",
      "Epoch 81/300\n",
      "2492/2492 [==============================] - 0s - loss: 0.6952 - acc: 0.6565 - val_loss: 0.7156 - val_acc: 0.6554\n",
      "Epoch 82/300\n",
      "2492/2492 [==============================] - 0s - loss: 0.6948 - acc: 0.6673 - val_loss: 0.7259 - val_acc: 0.6367\n",
      "Epoch 83/300\n",
      "2492/2492 [==============================] - 0s - loss: 0.7162 - acc: 0.6617 - val_loss: 0.7368 - val_acc: 0.6610\n",
      "Epoch 84/300\n",
      "2492/2492 [==============================] - 0s - loss: 0.7087 - acc: 0.6453 - val_loss: 0.7245 - val_acc: 0.6442\n",
      "Epoch 85/300\n",
      "2492/2492 [==============================] - 0s - loss: 0.6760 - acc: 0.6693 - val_loss: 0.7273 - val_acc: 0.6517\n",
      "Epoch 86/300\n",
      "2492/2492 [==============================] - 0s - loss: 0.6866 - acc: 0.6693 - val_loss: 0.7247 - val_acc: 0.6610\n",
      "Epoch 87/300\n",
      "2492/2492 [==============================] - 0s - loss: 0.6913 - acc: 0.6657 - val_loss: 0.7241 - val_acc: 0.6536\n",
      "Epoch 88/300\n",
      "2492/2492 [==============================] - 0s - loss: 0.6832 - acc: 0.6613 - val_loss: 0.6977 - val_acc: 0.6573\n",
      "Epoch 89/300\n",
      "2492/2492 [==============================] - 0s - loss: 0.6814 - acc: 0.6673 - val_loss: 0.7165 - val_acc: 0.6498\n",
      "Epoch 90/300\n",
      "2492/2492 [==============================] - 0s - loss: 0.6626 - acc: 0.6685 - val_loss: 0.7070 - val_acc: 0.6592\n",
      "Epoch 91/300\n",
      "2492/2492 [==============================] - 0s - loss: 0.6778 - acc: 0.6693 - val_loss: 0.7087 - val_acc: 0.6629\n",
      "Epoch 92/300\n",
      "2492/2492 [==============================] - 0s - loss: 0.6731 - acc: 0.6685 - val_loss: 0.6881 - val_acc: 0.6742\n",
      "Epoch 93/300\n",
      "2492/2492 [==============================] - 0s - loss: 0.6714 - acc: 0.6730 - val_loss: 0.6858 - val_acc: 0.6610\n",
      "Epoch 94/300\n",
      "2492/2492 [==============================] - 0s - loss: 0.6914 - acc: 0.6617 - val_loss: 0.7178 - val_acc: 0.6498\n",
      "Epoch 95/300\n",
      "2492/2492 [==============================] - 0s - loss: 0.6800 - acc: 0.6742 - val_loss: 0.6998 - val_acc: 0.6629\n",
      "Epoch 96/300\n",
      "2492/2492 [==============================] - 0s - loss: 0.6722 - acc: 0.6734 - val_loss: 0.7143 - val_acc: 0.6610\n",
      "Epoch 97/300\n",
      "2492/2492 [==============================] - 0s - loss: 0.6848 - acc: 0.6742 - val_loss: 0.6864 - val_acc: 0.6760\n",
      "Epoch 98/300\n",
      "2492/2492 [==============================] - 0s - loss: 0.6744 - acc: 0.6645 - val_loss: 0.6934 - val_acc: 0.6742\n",
      "Epoch 99/300\n",
      "2492/2492 [==============================] - 0s - loss: 0.6566 - acc: 0.6822 - val_loss: 0.6856 - val_acc: 0.6648\n",
      "Epoch 100/300\n",
      "2492/2492 [==============================] - 0s - loss: 0.6567 - acc: 0.6806 - val_loss: 0.6950 - val_acc: 0.6779\n",
      "Epoch 101/300\n",
      "2492/2492 [==============================] - 0s - loss: 0.6518 - acc: 0.6834 - val_loss: 0.6887 - val_acc: 0.6610\n",
      "Epoch 102/300\n",
      "2492/2492 [==============================] - 0s - loss: 0.6549 - acc: 0.6822 - val_loss: 0.6893 - val_acc: 0.6723\n",
      "Epoch 103/300\n",
      "2492/2492 [==============================] - 0s - loss: 0.6556 - acc: 0.6902 - val_loss: 0.6821 - val_acc: 0.6685\n",
      "Epoch 104/300\n",
      "2492/2492 [==============================] - 0s - loss: 0.6586 - acc: 0.6786 - val_loss: 0.6740 - val_acc: 0.6854\n",
      "Epoch 105/300\n",
      "2492/2492 [==============================] - 0s - loss: 0.6513 - acc: 0.6822 - val_loss: 0.6794 - val_acc: 0.6742\n",
      "Epoch 106/300\n",
      "2492/2492 [==============================] - 0s - loss: 0.6534 - acc: 0.6846 - val_loss: 0.6793 - val_acc: 0.6779\n",
      "Epoch 107/300\n",
      "2492/2492 [==============================] - 0s - loss: 0.6568 - acc: 0.6838 - val_loss: 0.6836 - val_acc: 0.6760\n",
      "Epoch 108/300\n",
      "2492/2492 [==============================] - 0s - loss: 0.6666 - acc: 0.6677 - val_loss: 0.6682 - val_acc: 0.6760\n",
      "Epoch 109/300\n",
      "2492/2492 [==============================] - 0s - loss: 0.6635 - acc: 0.6818 - val_loss: 0.6603 - val_acc: 0.6948\n",
      "Epoch 110/300\n",
      "2492/2492 [==============================] - 0s - loss: 0.6441 - acc: 0.6790 - val_loss: 0.6698 - val_acc: 0.6873\n",
      "Epoch 111/300\n",
      "2492/2492 [==============================] - 0s - loss: 0.6590 - acc: 0.6810 - val_loss: 0.6690 - val_acc: 0.6816\n",
      "Epoch 112/300\n",
      "2492/2492 [==============================] - 0s - loss: 0.6499 - acc: 0.6870 - val_loss: 0.6633 - val_acc: 0.7041\n",
      "Epoch 113/300\n",
      "2492/2492 [==============================] - 0s - loss: 0.6403 - acc: 0.6814 - val_loss: 0.6739 - val_acc: 0.7022\n",
      "Epoch 114/300\n",
      "2492/2492 [==============================] - 0s - loss: 0.6309 - acc: 0.6866 - val_loss: 0.6562 - val_acc: 0.6985\n",
      "Epoch 115/300\n",
      "2492/2492 [==============================] - 0s - loss: 0.6489 - acc: 0.6750 - val_loss: 0.6498 - val_acc: 0.7041\n",
      "Epoch 116/300\n",
      "2492/2492 [==============================] - 0s - loss: 0.6385 - acc: 0.6858 - val_loss: 0.6691 - val_acc: 0.7022\n",
      "Epoch 117/300\n",
      "2492/2492 [==============================] - 0s - loss: 0.6344 - acc: 0.6902 - val_loss: 0.6573 - val_acc: 0.7004\n",
      "Epoch 118/300\n",
      "2492/2492 [==============================] - 0s - loss: 0.6322 - acc: 0.6926 - val_loss: 0.6577 - val_acc: 0.7172\n",
      "Epoch 119/300\n",
      "2492/2492 [==============================] - 0s - loss: 0.6315 - acc: 0.6886 - val_loss: 0.6520 - val_acc: 0.6779\n",
      "Epoch 120/300\n",
      "2492/2492 [==============================] - 0s - loss: 0.6379 - acc: 0.6918 - val_loss: 0.6503 - val_acc: 0.7022\n",
      "Epoch 121/300\n",
      "2492/2492 [==============================] - 0s - loss: 0.6356 - acc: 0.6922 - val_loss: 0.6389 - val_acc: 0.7266\n",
      "Epoch 122/300\n",
      "2492/2492 [==============================] - 0s - loss: 0.6295 - acc: 0.7039 - val_loss: 0.6557 - val_acc: 0.7079\n",
      "Epoch 123/300\n",
      "2492/2492 [==============================] - 0s - loss: 0.6371 - acc: 0.6922 - val_loss: 0.6542 - val_acc: 0.7079\n",
      "Epoch 124/300\n",
      "2492/2492 [==============================] - 0s - loss: 0.6166 - acc: 0.7071 - val_loss: 0.6432 - val_acc: 0.7060\n",
      "Epoch 125/300\n",
      "2492/2492 [==============================] - 0s - loss: 0.6486 - acc: 0.6894 - val_loss: 0.6582 - val_acc: 0.7022\n",
      "Epoch 126/300\n",
      "2492/2492 [==============================] - 0s - loss: 0.6395 - acc: 0.6946 - val_loss: 0.6534 - val_acc: 0.7135\n",
      "Epoch 127/300\n",
      "2492/2492 [==============================] - 0s - loss: 0.6425 - acc: 0.6958 - val_loss: 0.6478 - val_acc: 0.6948\n",
      "Epoch 128/300\n",
      "2492/2492 [==============================] - 0s - loss: 0.6240 - acc: 0.7063 - val_loss: 0.6357 - val_acc: 0.7041\n",
      "Epoch 129/300\n",
      "2492/2492 [==============================] - 0s - loss: 0.6261 - acc: 0.7026 - val_loss: 0.6313 - val_acc: 0.7266\n",
      "Epoch 130/300\n",
      "2492/2492 [==============================] - 0s - loss: 0.6187 - acc: 0.7047 - val_loss: 0.6621 - val_acc: 0.6948\n",
      "Epoch 131/300\n",
      "2492/2492 [==============================] - 0s - loss: 0.6246 - acc: 0.6930 - val_loss: 0.6314 - val_acc: 0.7135\n",
      "Epoch 132/300\n",
      "2492/2492 [==============================] - 0s - loss: 0.6205 - acc: 0.7035 - val_loss: 0.6358 - val_acc: 0.7116\n",
      "Epoch 133/300\n",
      "2492/2492 [==============================] - 0s - loss: 0.6337 - acc: 0.6998 - val_loss: 0.6326 - val_acc: 0.7079\n",
      "Epoch 134/300\n",
      "2492/2492 [==============================] - 0s - loss: 0.6208 - acc: 0.7006 - val_loss: 0.6290 - val_acc: 0.7022\n",
      "Epoch 135/300\n",
      "2492/2492 [==============================] - 0s - loss: 0.6207 - acc: 0.7147 - val_loss: 0.6335 - val_acc: 0.7022\n",
      "Epoch 136/300\n",
      "2492/2492 [==============================] - 0s - loss: 0.6180 - acc: 0.7047 - val_loss: 0.6336 - val_acc: 0.7360\n",
      "Epoch 137/300\n",
      "2492/2492 [==============================] - 0s - loss: 0.6097 - acc: 0.7055 - val_loss: 0.6234 - val_acc: 0.7135\n",
      "Epoch 138/300\n",
      "2492/2492 [==============================] - 0s - loss: 0.5965 - acc: 0.7171 - val_loss: 0.6146 - val_acc: 0.7135\n",
      "Epoch 139/300\n",
      "2492/2492 [==============================] - 0s - loss: 0.5981 - acc: 0.7083 - val_loss: 0.6241 - val_acc: 0.7247\n",
      "Epoch 140/300\n",
      "2492/2492 [==============================] - 0s - loss: 0.6114 - acc: 0.7063 - val_loss: 0.6275 - val_acc: 0.7191\n",
      "Epoch 141/300\n",
      "2492/2492 [==============================] - 0s - loss: 0.5948 - acc: 0.7079 - val_loss: 0.6160 - val_acc: 0.7191\n",
      "Epoch 142/300\n",
      "2492/2492 [==============================] - 0s - loss: 0.6216 - acc: 0.7071 - val_loss: 0.6408 - val_acc: 0.6929\n",
      "Epoch 143/300\n",
      "2492/2492 [==============================] - 0s - loss: 0.6039 - acc: 0.7159 - val_loss: 0.6348 - val_acc: 0.7210\n",
      "Epoch 144/300\n",
      "2492/2492 [==============================] - 0s - loss: 0.6060 - acc: 0.7030 - val_loss: 0.6220 - val_acc: 0.7060\n",
      "Epoch 145/300\n",
      "2492/2492 [==============================] - 0s - loss: 0.6156 - acc: 0.7063 - val_loss: 0.6362 - val_acc: 0.7154\n",
      "Epoch 146/300\n",
      "2492/2492 [==============================] - 0s - loss: 0.6004 - acc: 0.7071 - val_loss: 0.6343 - val_acc: 0.7191\n",
      "Epoch 147/300\n",
      "2492/2492 [==============================] - 0s - loss: 0.6082 - acc: 0.7159 - val_loss: 0.6280 - val_acc: 0.7097\n",
      "Epoch 148/300\n",
      "2492/2492 [==============================] - 0s - loss: 0.6041 - acc: 0.7143 - val_loss: 0.6112 - val_acc: 0.7416\n",
      "Epoch 149/300\n",
      "2492/2492 [==============================] - 0s - loss: 0.6037 - acc: 0.7119 - val_loss: 0.6099 - val_acc: 0.7303\n",
      "Epoch 150/300\n",
      "2492/2492 [==============================] - 0s - loss: 0.5967 - acc: 0.7071 - val_loss: 0.6042 - val_acc: 0.7060\n",
      "Epoch 151/300\n",
      "2492/2492 [==============================] - 0s - loss: 0.6070 - acc: 0.7067 - val_loss: 0.6158 - val_acc: 0.7097\n",
      "Epoch 152/300\n",
      "2492/2492 [==============================] - 0s - loss: 0.6052 - acc: 0.7143 - val_loss: 0.6015 - val_acc: 0.7341\n",
      "Epoch 153/300\n",
      "2492/2492 [==============================] - 0s - loss: 0.5868 - acc: 0.7239 - val_loss: 0.6096 - val_acc: 0.7341\n",
      "Epoch 154/300\n",
      "2492/2492 [==============================] - 0s - loss: 0.6068 - acc: 0.6986 - val_loss: 0.5925 - val_acc: 0.7360\n",
      "Epoch 155/300\n",
      "2492/2492 [==============================] - 0s - loss: 0.6083 - acc: 0.7014 - val_loss: 0.6189 - val_acc: 0.7004\n",
      "Epoch 156/300\n",
      "2492/2492 [==============================] - 0s - loss: 0.6043 - acc: 0.7167 - val_loss: 0.6052 - val_acc: 0.7210\n",
      "Epoch 157/300\n",
      "2492/2492 [==============================] - 0s - loss: 0.6102 - acc: 0.7043 - val_loss: 0.6248 - val_acc: 0.7172\n",
      "Epoch 158/300\n",
      "2492/2492 [==============================] - 0s - loss: 0.5975 - acc: 0.7147 - val_loss: 0.6134 - val_acc: 0.7247\n",
      "Epoch 159/300\n",
      "2492/2492 [==============================] - 0s - loss: 0.6015 - acc: 0.7111 - val_loss: 0.6103 - val_acc: 0.7116\n",
      "Epoch 160/300\n",
      "2492/2492 [==============================] - 0s - loss: 0.5987 - acc: 0.7143 - val_loss: 0.5849 - val_acc: 0.7341\n",
      "Epoch 161/300\n",
      "2492/2492 [==============================] - 0s - loss: 0.5864 - acc: 0.7143 - val_loss: 0.6040 - val_acc: 0.7341\n",
      "Epoch 162/300\n",
      "2492/2492 [==============================] - 0s - loss: 0.5757 - acc: 0.7183 - val_loss: 0.6059 - val_acc: 0.7191\n",
      "Epoch 163/300\n",
      "2492/2492 [==============================] - 0s - loss: 0.5847 - acc: 0.7223 - val_loss: 0.5780 - val_acc: 0.7472\n",
      "Epoch 164/300\n",
      "2492/2492 [==============================] - 0s - loss: 0.5870 - acc: 0.7219 - val_loss: 0.5941 - val_acc: 0.7247\n",
      "Epoch 165/300\n",
      "2492/2492 [==============================] - 0s - loss: 0.5819 - acc: 0.7263 - val_loss: 0.6085 - val_acc: 0.7172\n",
      "Epoch 166/300\n",
      "2492/2492 [==============================] - 0s - loss: 0.5909 - acc: 0.7191 - val_loss: 0.5845 - val_acc: 0.7434\n",
      "Epoch 167/300\n",
      "2492/2492 [==============================] - 0s - loss: 0.5869 - acc: 0.7239 - val_loss: 0.5806 - val_acc: 0.7416\n",
      "Epoch 168/300\n",
      "2492/2492 [==============================] - 0s - loss: 0.5857 - acc: 0.7211 - val_loss: 0.5880 - val_acc: 0.7397\n",
      "Epoch 169/300\n",
      "2492/2492 [==============================] - 0s - loss: 0.5916 - acc: 0.7211 - val_loss: 0.5994 - val_acc: 0.7303\n",
      "Epoch 170/300\n",
      "2492/2492 [==============================] - 0s - loss: 0.5757 - acc: 0.7271 - val_loss: 0.6188 - val_acc: 0.7247\n",
      "Epoch 171/300\n",
      "2492/2492 [==============================] - 0s - loss: 0.5971 - acc: 0.7151 - val_loss: 0.5814 - val_acc: 0.7360\n",
      "Epoch 172/300\n",
      "2492/2492 [==============================] - 0s - loss: 0.5733 - acc: 0.7263 - val_loss: 0.5954 - val_acc: 0.7453\n",
      "Epoch 173/300\n",
      "2492/2492 [==============================] - 0s - loss: 0.5615 - acc: 0.7380 - val_loss: 0.5918 - val_acc: 0.7191\n",
      "Epoch 174/300\n",
      "2492/2492 [==============================] - 0s - loss: 0.5811 - acc: 0.7315 - val_loss: 0.5869 - val_acc: 0.7247\n",
      "Epoch 175/300\n",
      "2492/2492 [==============================] - 0s - loss: 0.5632 - acc: 0.7299 - val_loss: 0.5907 - val_acc: 0.7378\n",
      "Epoch 176/300\n",
      "2492/2492 [==============================] - 0s - loss: 0.5669 - acc: 0.7279 - val_loss: 0.5887 - val_acc: 0.7172\n",
      "Epoch 177/300\n",
      "2492/2492 [==============================] - 0s - loss: 0.5733 - acc: 0.7263 - val_loss: 0.5802 - val_acc: 0.7360\n",
      "Epoch 178/300\n",
      "2492/2492 [==============================] - 0s - loss: 0.5694 - acc: 0.7231 - val_loss: 0.5840 - val_acc: 0.7434\n",
      "Epoch 179/300\n",
      "2492/2492 [==============================] - 0s - loss: 0.5777 - acc: 0.7279 - val_loss: 0.5764 - val_acc: 0.7285\n",
      "Epoch 180/300\n",
      "2492/2492 [==============================] - 0s - loss: 0.5668 - acc: 0.7299 - val_loss: 0.5833 - val_acc: 0.7341\n",
      "Epoch 181/300\n",
      "2492/2492 [==============================] - 0s - loss: 0.5699 - acc: 0.7275 - val_loss: 0.5884 - val_acc: 0.7341\n",
      "Epoch 182/300\n",
      "2492/2492 [==============================] - 0s - loss: 0.5686 - acc: 0.7428 - val_loss: 0.5896 - val_acc: 0.7416\n",
      "Epoch 183/300\n",
      "2492/2492 [==============================] - 0s - loss: 0.5606 - acc: 0.7384 - val_loss: 0.5842 - val_acc: 0.7453\n",
      "Epoch 184/300\n",
      "2492/2492 [==============================] - 0s - loss: 0.5673 - acc: 0.7380 - val_loss: 0.5738 - val_acc: 0.7341\n",
      "Epoch 185/300\n",
      "2492/2492 [==============================] - 0s - loss: 0.5609 - acc: 0.7303 - val_loss: 0.5853 - val_acc: 0.7472\n",
      "Epoch 186/300\n",
      "2492/2492 [==============================] - 0s - loss: 0.5535 - acc: 0.7416 - val_loss: 0.5906 - val_acc: 0.7378\n",
      "Epoch 187/300\n",
      "2492/2492 [==============================] - 0s - loss: 0.5685 - acc: 0.7227 - val_loss: 0.5767 - val_acc: 0.7191\n",
      "Epoch 188/300\n",
      "2492/2492 [==============================] - 0s - loss: 0.5463 - acc: 0.7404 - val_loss: 0.5722 - val_acc: 0.7453\n",
      "Epoch 189/300\n",
      "2492/2492 [==============================] - 0s - loss: 0.5614 - acc: 0.7368 - val_loss: 0.5990 - val_acc: 0.7397\n",
      "Epoch 190/300\n",
      "2492/2492 [==============================] - 0s - loss: 0.5639 - acc: 0.7352 - val_loss: 0.5892 - val_acc: 0.7397\n",
      "Epoch 191/300\n",
      "2492/2492 [==============================] - 0s - loss: 0.5721 - acc: 0.7460 - val_loss: 0.5905 - val_acc: 0.7434\n",
      "Epoch 192/300\n",
      "2492/2492 [==============================] - 0s - loss: 0.5584 - acc: 0.7368 - val_loss: 0.5881 - val_acc: 0.7191\n",
      "Epoch 193/300\n",
      "2492/2492 [==============================] - 0s - loss: 0.5521 - acc: 0.7412 - val_loss: 0.5677 - val_acc: 0.7416\n",
      "Epoch 194/300\n",
      "2492/2492 [==============================] - 0s - loss: 0.5591 - acc: 0.7311 - val_loss: 0.5942 - val_acc: 0.7210\n",
      "Epoch 195/300\n",
      "2492/2492 [==============================] - 0s - loss: 0.5608 - acc: 0.7352 - val_loss: 0.5724 - val_acc: 0.7472\n",
      "Epoch 196/300\n",
      "2492/2492 [==============================] - 0s - loss: 0.5652 - acc: 0.7307 - val_loss: 0.5816 - val_acc: 0.7154\n",
      "Epoch 197/300\n",
      "2492/2492 [==============================] - 0s - loss: 0.5497 - acc: 0.7436 - val_loss: 0.5728 - val_acc: 0.7434\n",
      "Epoch 198/300\n",
      "2492/2492 [==============================] - 0s - loss: 0.5648 - acc: 0.7404 - val_loss: 0.5684 - val_acc: 0.7416\n",
      "Epoch 199/300\n",
      "2492/2492 [==============================] - 0s - loss: 0.5525 - acc: 0.7384 - val_loss: 0.5462 - val_acc: 0.7434\n",
      "Epoch 200/300\n",
      "2492/2492 [==============================] - 0s - loss: 0.5453 - acc: 0.7476 - val_loss: 0.5442 - val_acc: 0.7509\n",
      "Epoch 201/300\n",
      "2492/2492 [==============================] - 0s - loss: 0.5321 - acc: 0.7560 - val_loss: 0.5575 - val_acc: 0.7603\n",
      "Epoch 202/300\n",
      "2492/2492 [==============================] - 0s - loss: 0.5456 - acc: 0.7400 - val_loss: 0.5401 - val_acc: 0.7303\n",
      "Epoch 203/300\n",
      "2492/2492 [==============================] - 0s - loss: 0.5476 - acc: 0.7460 - val_loss: 0.5609 - val_acc: 0.7528\n",
      "Epoch 204/300\n",
      "2492/2492 [==============================] - 0s - loss: 0.5464 - acc: 0.7524 - val_loss: 0.5542 - val_acc: 0.7603\n",
      "Epoch 205/300\n",
      "2492/2492 [==============================] - 0s - loss: 0.5534 - acc: 0.7468 - val_loss: 0.5577 - val_acc: 0.7434\n",
      "Epoch 206/300\n",
      "2492/2492 [==============================] - 0s - loss: 0.5589 - acc: 0.7372 - val_loss: 0.5486 - val_acc: 0.7434\n",
      "Epoch 207/300\n",
      "2492/2492 [==============================] - 0s - loss: 0.5361 - acc: 0.7452 - val_loss: 0.5696 - val_acc: 0.7341\n",
      "Epoch 208/300\n",
      "2492/2492 [==============================] - 0s - loss: 0.5479 - acc: 0.7452 - val_loss: 0.5559 - val_acc: 0.7378\n",
      "Epoch 209/300\n",
      "2492/2492 [==============================] - 0s - loss: 0.5466 - acc: 0.7432 - val_loss: 0.5417 - val_acc: 0.7640\n",
      "Epoch 210/300\n",
      "2492/2492 [==============================] - 0s - loss: 0.5458 - acc: 0.7444 - val_loss: 0.5513 - val_acc: 0.7397\n",
      "Epoch 211/300\n",
      "2492/2492 [==============================] - 0s - loss: 0.5328 - acc: 0.7572 - val_loss: 0.5491 - val_acc: 0.7453\n",
      "Epoch 212/300\n",
      "2492/2492 [==============================] - 0s - loss: 0.5444 - acc: 0.7452 - val_loss: 0.5680 - val_acc: 0.7678\n",
      "Epoch 213/300\n",
      "2492/2492 [==============================] - 0s - loss: 0.5454 - acc: 0.7376 - val_loss: 0.5586 - val_acc: 0.7584\n",
      "Epoch 214/300\n",
      "2492/2492 [==============================] - 0s - loss: 0.5349 - acc: 0.7416 - val_loss: 0.5549 - val_acc: 0.7584\n",
      "Epoch 215/300\n",
      "2492/2492 [==============================] - 0s - loss: 0.5327 - acc: 0.7488 - val_loss: 0.5672 - val_acc: 0.7360\n",
      "Epoch 216/300\n",
      "2492/2492 [==============================] - 0s - loss: 0.5348 - acc: 0.7472 - val_loss: 0.5589 - val_acc: 0.7659\n",
      "Epoch 217/300\n",
      "2492/2492 [==============================] - 0s - loss: 0.5449 - acc: 0.7436 - val_loss: 0.5616 - val_acc: 0.7622\n",
      "Epoch 218/300\n",
      "2492/2492 [==============================] - 0s - loss: 0.5238 - acc: 0.7476 - val_loss: 0.5691 - val_acc: 0.7566\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f1b773bd8d0>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 500\n",
    "epochs = 300\n",
    "earlyStopping=keras.callbacks.EarlyStopping(monitor='val_loss', patience=15, verbose=0, mode='auto')\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['acc'])\n",
    "model.fit(x_train, y_train, nb_epoch=epochs,batch_size=batch_size, \n",
    "          callbacks=[earlyStopping], shuffle=True, validation_data = (x_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "500/534 [===========================>..] - ETA: 0s"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.61489543285262716, 0.74344566832767445]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_test, y_test, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "534/534 [==============================] - 0s     \n",
      "[1 0 1 2 0 0 1 0 1 0 2 1 0 0 1 1 1 1 0 2 2 0 0 1 0 1 0 2 0 2 1 0 2 1 0 0 0\n",
      " 2 2 2 2 2 2 2 2 0 0 2 2 0 1 2 1 0 2 0 2 0 2 1 1 2 2 2 2 0 2 2 1 1 1 0 2 0\n",
      " 1 2 1 2 0 1 2 2 2 0 1 0 2 1 2 1 1 1 0 1 1 1 1 0 0 2 2 0 2 0 2 0 0 1 0 0 1\n",
      " 2 1 1 2 1 0 2 1 1 2 1 1 0 1 1 2 1 0 2 1 0 0 1 0 2 1 0 1 1 1 0 1 1 1 1 1 1\n",
      " 1 1 0 0 0 2 2 1 1 1 1 2 0 0 0 1 0 0 1 1 2 0 1 2 0 2 0 1 0 1 0 0 2 1 0 0 2\n",
      " 2 2 1 2 1 2 1 1 1 1 0 0 2 2 0 2 0 1 0 2 0 1 2 2 1 2 0 0 1 2 0 1 1 0 0 0 0\n",
      " 2 1 0 0 0 0 0 0 1 2 1 0 0 0 1 0 0 2 2 0 1 1 0 1 2 0 0 1 2 0 1 2 2 2 0 0 1\n",
      " 0 0 2 2 1 1 1 2 1 0 0 2 2 0 2 1 0 0 1 1 1 2 1 0 1 1 2 0 1 2 1 1 0 1 2 1 0\n",
      " 1 0 0 1 2 2 1 0 0 2 1 1 0 2 0 2 1 1 0 0 0 1 1 0 2 2 2 2 1 1 2 1 0 2 2 1 1\n",
      " 0 0 2 1 0 1 1 0 0 1 0 2 1 0 1 1 1 1 1 2 0 2 1 1 2 2 1 0 2 2 1 0 2 0 0 1 2\n",
      " 2 0 2 1 2 0 1 1 1 2 1 2 1 2 0 1 1 2 1 1 0 2 0 0 2 2 0 2 1 0 2 2 2 1 0 2 1\n",
      " 1 0 2 1 0 2 1 2 0 1 0 1 0 1 1 1 2 0 1 1 1 0 1 2 2 1 0 0 0 2 0 0 2 2 1 2 1\n",
      " 0 2 2 2 0 2 1 0 0 2 0 0 1 1 0 1 1 2 0 0 1 1 2 1 0 0 1 1 2 0 0 1 2 1 2 1 0\n",
      " 2 0 1 0 2 1 1 1 1 0 2 2 1 0 2 2 2 2 2 2 0 1 1 0 2 0 2 2 2 2 0 2 1 1 2 0 2\n",
      " 0 1 0 1 0 2 2 0 0 1 2 0 1 2 0 2]\n",
      "512/534 [===========================>..] - ETA: 0s                  precision    recall  f1-score   support\n",
      "\n",
      "class 0(tetris1)       0.80      0.81      0.80       175\n",
      "class 1(tetris2)       0.67      0.72      0.70       177\n",
      "class 2(tetris3)       0.77      0.70      0.73       182\n",
      "\n",
      "     avg / total       0.75      0.74      0.74       534\n",
      "\n",
      "[[142  22  11]\n",
      " [ 22 128  27]\n",
      " [ 14  41 127]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "\n",
    "y_pred = model.predict_classes(x_test)\n",
    "print(y_pred)\n",
    "\n",
    "p=model.predict_proba(x_test)\n",
    "\n",
    "#target_names = ['class 0(0back)', 'class 1(1back)', 'class 2(2back)', 'class 3(3back)']\n",
    "target_names = ['class 0(tetris1)', 'class 1(tetris2)', 'class 2(tetris3)']\n",
    "print(classification_report(np.argmax(y_test,axis=1), y_pred,target_names=target_names))\n",
    "print(confusion_matrix(np.argmax(y_test,axis=1), y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
