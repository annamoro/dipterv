{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import os\n",
    "import numpy as np\n",
    "from numpy import newaxis\n",
    "from sklearn import preprocessing\n",
    "np.random.seed(1337)\n",
    "\n",
    "import keras\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.layers import Dense, Input, Flatten\n",
    "from keras.layers import Dropout\n",
    "from keras.models import Model,Sequential\n",
    "from keras.layers import LSTM\n",
    "import sys\n",
    "\n",
    "import pandas as pd\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data read started...\n",
      "Data read finished.\n",
      "(622, 19)\n"
     ]
    }
   ],
   "source": [
    "print(\"Data read started...\")\n",
    "data = pd.read_csv(\"nback/result1.csv\")\n",
    "data = data.as_matrix()\n",
    "print (\"Data read finished.\")\n",
    "\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eliminate EEG data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(622, 7)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range (1,5):\n",
    "    data = np.delete(data, 1, 1) \n",
    "    \n",
    "#data = np.delete(data, 1, 1) \n",
    "data = data[:,0:7]\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['3back', 1537.7, 796.38, 36.961999999999996, 4.696727272727268,\n",
       "       8.947000000000001, 16.387999999999998], dtype=object)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zero: 206  one:  130  two:  139  three:  147\n"
     ]
    }
   ],
   "source": [
    "zero = 0\n",
    "one = 0\n",
    "two = 0\n",
    "three = 0\n",
    "\n",
    "for i in range(len(data)):\n",
    "    if (data[i, 0] == '0back'):\n",
    "        zero = zero + 1\n",
    "    if (data[i, 0] == '1back'):\n",
    "        one = one + 1\n",
    "    if (data[i, 0] == '2back'):\n",
    "        two = two + 1\n",
    "    if (data[i, 0] == '3back'):\n",
    "        three = three + 1\n",
    "\n",
    "print('zero:', zero, ' one: ', one, ' two: ', two, ' three: ', three)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dictionary for the levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "level=[\"0back\",\"1back\",\"2back\",\"3back\"]\n",
    "level2int = dict((p, i) for i, p in enumerate(level))\n",
    "int2level = dict((i, p) for i, p in enumerate(level))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Z normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/amoro/anaconda2/lib/python2.7/site-packages/sklearn/utils/validation.py:420: DataConversionWarning: Data with input dtype object was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/home/amoro/anaconda2/lib/python2.7/site-packages/sklearn/utils/validation.py:420: DataConversionWarning: Data with input dtype object was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/home/amoro/anaconda2/lib/python2.7/site-packages/sklearn/utils/validation.py:420: DataConversionWarning: Data with input dtype object was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/home/amoro/anaconda2/lib/python2.7/site-packages/sklearn/utils/validation.py:420: DataConversionWarning: Data with input dtype object was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/home/amoro/anaconda2/lib/python2.7/site-packages/sklearn/utils/validation.py:420: DataConversionWarning: Data with input dtype object was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/home/amoro/anaconda2/lib/python2.7/site-packages/sklearn/utils/validation.py:420: DataConversionWarning: Data with input dtype object was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "for i in range (1,7):\n",
    "    data[:, i] = preprocessing.scale(data[:, i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "back0 = np.zeros((zero,6))\n",
    "back1 = np.zeros((one,6))\n",
    "back2 = np.zeros((two,6))\n",
    "back3 = np.zeros((three,6))\n",
    "j = 0\n",
    "k = 0\n",
    "l = 0\n",
    "m = 0\n",
    "\n",
    "for i in range(len(data)):\n",
    "    if (data[i, 0] == '0back'):\n",
    "        back0[j] = data[i, 1:]\n",
    "        j = j+1\n",
    "    if (data[i, 0] == '1back'):\n",
    "        back1[k] = data[i, 1:]\n",
    "        k = k+1\n",
    "    if (data[i, 0] == '2back'):\n",
    "        back2[l] = data[i, 1:]\n",
    "        l = l+1\n",
    "    if (data[i, 0] == '3back'):\n",
    "        back3[m] = data[i, 1:]\n",
    "        m = m+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_data = data[:, 0]\n",
    "for i in range(len(data)):\n",
    "    y_data[i] = level2int[y_data[i]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  0.  0.  1.]\n",
      " [ 0.  0.  0.  1.]\n",
      " [ 0.  0.  0.  1.]\n",
      " ..., \n",
      " [ 1.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "ohe = OneHotEncoder()\n",
    "one_hot = ohe.fit_transform(y_data.reshape(-1,1)).toarray()\n",
    "print(one_hot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Group data for recurrent network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "window = 8\n",
    "null = np.array([0,0,0,0,0,0])\n",
    "\n",
    "for i in range(window-1):\n",
    "        back0 = np.vstack([back0, null])\n",
    "        back1 = np.vstack([back1, null])\n",
    "        back2 = np.vstack([back2, null])\n",
    "        back3 = np.vstack([back3, null])\n",
    "        \n",
    "for i in range(window-1):\n",
    "        back0 = np.vstack([null, back0])\n",
    "        back1 = np.vstack([null, back1])\n",
    "        back2 = np.vstack([null, back2])\n",
    "        back3 = np.vstack([null, back3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_length = len(back0) + window - 1 + len(back1) + window - 1 + len(back2) + window - 1 + len(back3) + window - 1\n",
    "\n",
    "x_data = np.zeros((data_length,window, 6))         # final input data for the network\n",
    "y_one_hot = np.zeros((data_length,4))             # to store one-hot data groupped\n",
    "\n",
    "\n",
    "for i in range(len(back0)-window):\n",
    "    for j in range(window):\n",
    "        x_data[i, j, :] = back0[i+j]\n",
    "    y_one_hot[i] = [0,0,0,1]\n",
    "    \n",
    "\n",
    "index = 0\n",
    "for i in range(len(back0)-window, len(back0)-window+ len(back1)-window):\n",
    "    for j in range(window):\n",
    "        x_data[i, j, :] = back1[index+j]\n",
    "    y_one_hot[i] = [0,0,1,0]\n",
    "    index = index+1\n",
    "\n",
    "index = 0\n",
    "for i in range(len(back0)-window+ len(back1)-window, len(back0)-window + len(back1)-window + len(back2)-window):\n",
    "    for j in range(window):\n",
    "        x_data[i, j, :] = back2[index+j]\n",
    "    y_one_hot[i] = [0,1,0,0]\n",
    "        \n",
    "    index = index+1\n",
    "    \n",
    "\n",
    "index = 0\n",
    "for i in range(len(back0)-window+ len(back1)-window + len(back2)-window, \n",
    "                   len(back0)-window + len(back1)-window + len(back2)-window + len(back3)-window):\n",
    "    for j in range(window):\n",
    "        x_data[i, j, :] = back3[index+j]\n",
    "    y_one_hot[i] = [1,0,0,0]\n",
    "    index = index+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shuffle data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "indices = np.arange(x_data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "x_data = x_data[indices]\n",
    "y_one_hot = y_one_hot[indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Divide into train, validation and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len_data = len(x_data)\n",
    "\n",
    "nb_test = int(len_data*0.15)\n",
    "nb_validation = int(len_data*0.15)\n",
    "nb_train = int(len_data*0.7)\n",
    "\n",
    "end_valid = nb_train+nb_validation\n",
    "\n",
    "x_train = x_data[0:nb_train]\n",
    "y_train = y_one_hot[0:nb_train]\n",
    "\n",
    "x_valid = x_data[nb_train:end_valid]\n",
    "y_valid = y_one_hot[nb_train:end_valid]\n",
    "\n",
    "x_test = x_data[end_valid:]\n",
    "y_test = y_one_hot[end_valid:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(494, 8, 6) (494, 4) (105, 8, 6) (105, 4) (107, 8, 6) (107, 4)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape, y_train.shape, x_valid.shape, y_valid.shape, x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "earlyStopping=keras.callbacks.EarlyStopping(monitor='val_loss', patience=15, verbose=0, mode='auto')\n",
    "\n",
    "model.add(LSTM(64, input_dim=6, input_length=window, return_sequences=True))\n",
    "model.add(Dropout(0.5))\n",
    "    \n",
    "\n",
    "model.add(LSTM(32,return_sequences=False))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(output_dim=y_one_hot.shape[1], activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 494 samples, validate on 105 samples\n",
      "Epoch 1/200\n",
      "494/494 [==============================] - 4s - loss: 1.1921 - acc: 0.4393 - val_loss: 1.1035 - val_acc: 0.5048\n",
      "Epoch 2/200\n",
      "494/494 [==============================] - 0s - loss: 1.0741 - acc: 0.4879 - val_loss: 1.0247 - val_acc: 0.4571\n",
      "Epoch 3/200\n",
      "494/494 [==============================] - 0s - loss: 1.0131 - acc: 0.4980 - val_loss: 0.9646 - val_acc: 0.5429\n",
      "Epoch 4/200\n",
      "494/494 [==============================] - 0s - loss: 0.9646 - acc: 0.5101 - val_loss: 0.9178 - val_acc: 0.5619\n",
      "Epoch 5/200\n",
      "494/494 [==============================] - 0s - loss: 0.9032 - acc: 0.5283 - val_loss: 0.8713 - val_acc: 0.5810\n",
      "Epoch 6/200\n",
      "494/494 [==============================] - 0s - loss: 0.8453 - acc: 0.5810 - val_loss: 0.8298 - val_acc: 0.5810\n",
      "Epoch 7/200\n",
      "494/494 [==============================] - 0s - loss: 0.8005 - acc: 0.6093 - val_loss: 0.8004 - val_acc: 0.6095\n",
      "Epoch 8/200\n",
      "494/494 [==============================] - 0s - loss: 0.7731 - acc: 0.6093 - val_loss: 0.7721 - val_acc: 0.6190\n",
      "Epoch 9/200\n",
      "494/494 [==============================] - 0s - loss: 0.7436 - acc: 0.6134 - val_loss: 0.7462 - val_acc: 0.6000\n",
      "Epoch 10/200\n",
      "494/494 [==============================] - 0s - loss: 0.6996 - acc: 0.6478 - val_loss: 0.7161 - val_acc: 0.6381\n",
      "Epoch 11/200\n",
      "494/494 [==============================] - 0s - loss: 0.6961 - acc: 0.6538 - val_loss: 0.6891 - val_acc: 0.6667\n",
      "Epoch 12/200\n",
      "494/494 [==============================] - 0s - loss: 0.6376 - acc: 0.6619 - val_loss: 0.6641 - val_acc: 0.6667\n",
      "Epoch 13/200\n",
      "494/494 [==============================] - 0s - loss: 0.6175 - acc: 0.6680 - val_loss: 0.6389 - val_acc: 0.6952\n",
      "Epoch 14/200\n",
      "494/494 [==============================] - 0s - loss: 0.5763 - acc: 0.6842 - val_loss: 0.6318 - val_acc: 0.6857\n",
      "Epoch 15/200\n",
      "494/494 [==============================] - 0s - loss: 0.5595 - acc: 0.6680 - val_loss: 0.6093 - val_acc: 0.7143\n",
      "Epoch 16/200\n",
      "494/494 [==============================] - 0s - loss: 0.5314 - acc: 0.7348 - val_loss: 0.5500 - val_acc: 0.7429\n",
      "Epoch 17/200\n",
      "494/494 [==============================] - 0s - loss: 0.5188 - acc: 0.7247 - val_loss: 0.5783 - val_acc: 0.7524\n",
      "Epoch 18/200\n",
      "494/494 [==============================] - 0s - loss: 0.5050 - acc: 0.7409 - val_loss: 0.5031 - val_acc: 0.7238\n",
      "Epoch 19/200\n",
      "494/494 [==============================] - 0s - loss: 0.4725 - acc: 0.7368 - val_loss: 0.4742 - val_acc: 0.7619\n",
      "Epoch 20/200\n",
      "494/494 [==============================] - 0s - loss: 0.4482 - acc: 0.7672 - val_loss: 0.4272 - val_acc: 0.7714\n",
      "Epoch 21/200\n",
      "494/494 [==============================] - 0s - loss: 0.4224 - acc: 0.7713 - val_loss: 0.3946 - val_acc: 0.7905\n",
      "Epoch 22/200\n",
      "494/494 [==============================] - 0s - loss: 0.4266 - acc: 0.7510 - val_loss: 0.4092 - val_acc: 0.7905\n",
      "Epoch 23/200\n",
      "494/494 [==============================] - 0s - loss: 0.4151 - acc: 0.7692 - val_loss: 0.3896 - val_acc: 0.7714\n",
      "Epoch 24/200\n",
      "494/494 [==============================] - 0s - loss: 0.3634 - acc: 0.7874 - val_loss: 0.3579 - val_acc: 0.7810\n",
      "Epoch 25/200\n",
      "494/494 [==============================] - 0s - loss: 0.3469 - acc: 0.7834 - val_loss: 0.3347 - val_acc: 0.8286\n",
      "Epoch 26/200\n",
      "494/494 [==============================] - 0s - loss: 0.3698 - acc: 0.7935 - val_loss: 0.3293 - val_acc: 0.8286\n",
      "Epoch 27/200\n",
      "494/494 [==============================] - 0s - loss: 0.3314 - acc: 0.8036 - val_loss: 0.3516 - val_acc: 0.8190\n",
      "Epoch 28/200\n",
      "494/494 [==============================] - 0s - loss: 0.2844 - acc: 0.8057 - val_loss: 0.3185 - val_acc: 0.8095\n",
      "Epoch 29/200\n",
      "494/494 [==============================] - 0s - loss: 0.2970 - acc: 0.7955 - val_loss: 0.2829 - val_acc: 0.8381\n",
      "Epoch 30/200\n",
      "494/494 [==============================] - 0s - loss: 0.3182 - acc: 0.8057 - val_loss: 0.2735 - val_acc: 0.8381\n",
      "Epoch 31/200\n",
      "494/494 [==============================] - 0s - loss: 0.2947 - acc: 0.8097 - val_loss: 0.2864 - val_acc: 0.8381\n",
      "Epoch 32/200\n",
      "494/494 [==============================] - 0s - loss: 0.2911 - acc: 0.7874 - val_loss: 0.2505 - val_acc: 0.8476\n",
      "Epoch 33/200\n",
      "494/494 [==============================] - 0s - loss: 0.2687 - acc: 0.8279 - val_loss: 0.2500 - val_acc: 0.8476\n",
      "Epoch 34/200\n",
      "494/494 [==============================] - 0s - loss: 0.2620 - acc: 0.8138 - val_loss: 0.2427 - val_acc: 0.8381\n",
      "Epoch 35/200\n",
      "494/494 [==============================] - 0s - loss: 0.2476 - acc: 0.8239 - val_loss: 0.2271 - val_acc: 0.8571\n",
      "Epoch 36/200\n",
      "494/494 [==============================] - 0s - loss: 0.2461 - acc: 0.8401 - val_loss: 0.2540 - val_acc: 0.8476\n",
      "Epoch 37/200\n",
      "494/494 [==============================] - 0s - loss: 0.2344 - acc: 0.8219 - val_loss: 0.2028 - val_acc: 0.8667\n",
      "Epoch 38/200\n",
      "494/494 [==============================] - 0s - loss: 0.2145 - acc: 0.8320 - val_loss: 0.1828 - val_acc: 0.8667\n",
      "Epoch 39/200\n",
      "494/494 [==============================] - 0s - loss: 0.2000 - acc: 0.8441 - val_loss: 0.1768 - val_acc: 0.8762\n",
      "Epoch 40/200\n",
      "494/494 [==============================] - 0s - loss: 0.1926 - acc: 0.8462 - val_loss: 0.1867 - val_acc: 0.8571\n",
      "Epoch 41/200\n",
      "494/494 [==============================] - 0s - loss: 0.2401 - acc: 0.8158 - val_loss: 0.1626 - val_acc: 0.8857\n",
      "Epoch 42/200\n",
      "494/494 [==============================] - 0s - loss: 0.2069 - acc: 0.8482 - val_loss: 0.1469 - val_acc: 0.8762\n",
      "Epoch 43/200\n",
      "494/494 [==============================] - 0s - loss: 0.1807 - acc: 0.8522 - val_loss: 0.1363 - val_acc: 0.8857\n",
      "Epoch 44/200\n",
      "494/494 [==============================] - 0s - loss: 0.1802 - acc: 0.8583 - val_loss: 0.1412 - val_acc: 0.8762\n",
      "Epoch 45/200\n",
      "494/494 [==============================] - 0s - loss: 0.1633 - acc: 0.8684 - val_loss: 0.1358 - val_acc: 0.8857\n",
      "Epoch 46/200\n",
      "494/494 [==============================] - 0s - loss: 0.1757 - acc: 0.8543 - val_loss: 0.1308 - val_acc: 0.8857\n",
      "Epoch 47/200\n",
      "494/494 [==============================] - 0s - loss: 0.1835 - acc: 0.8401 - val_loss: 0.1105 - val_acc: 0.8952\n",
      "Epoch 48/200\n",
      "494/494 [==============================] - 0s - loss: 0.1786 - acc: 0.8421 - val_loss: 0.1139 - val_acc: 0.8952\n",
      "Epoch 49/200\n",
      "494/494 [==============================] - 0s - loss: 0.1435 - acc: 0.8603 - val_loss: 0.1654 - val_acc: 0.8762\n",
      "Epoch 50/200\n",
      "494/494 [==============================] - 0s - loss: 0.1406 - acc: 0.8644 - val_loss: 0.1620 - val_acc: 0.8857\n",
      "Epoch 51/200\n",
      "494/494 [==============================] - 0s - loss: 0.1709 - acc: 0.8482 - val_loss: 0.1045 - val_acc: 0.8857\n",
      "Epoch 52/200\n",
      "494/494 [==============================] - 0s - loss: 0.1695 - acc: 0.8623 - val_loss: 0.1302 - val_acc: 0.8762\n",
      "Epoch 53/200\n",
      "494/494 [==============================] - 0s - loss: 0.1486 - acc: 0.8603 - val_loss: 0.1272 - val_acc: 0.8857\n",
      "Epoch 54/200\n",
      "494/494 [==============================] - 0s - loss: 0.1272 - acc: 0.8684 - val_loss: 0.1295 - val_acc: 0.8762\n",
      "Epoch 55/200\n",
      "494/494 [==============================] - 0s - loss: 0.1417 - acc: 0.8623 - val_loss: 0.0909 - val_acc: 0.9048\n",
      "Epoch 56/200\n",
      "494/494 [==============================] - 0s - loss: 0.1322 - acc: 0.8887 - val_loss: 0.0789 - val_acc: 0.9238\n",
      "Epoch 57/200\n",
      "494/494 [==============================] - 0s - loss: 0.1242 - acc: 0.8785 - val_loss: 0.0843 - val_acc: 0.9048\n",
      "Epoch 58/200\n",
      "494/494 [==============================] - 0s - loss: 0.1114 - acc: 0.8806 - val_loss: 0.0747 - val_acc: 0.9143\n",
      "Epoch 59/200\n",
      "494/494 [==============================] - 0s - loss: 0.1050 - acc: 0.8765 - val_loss: 0.0858 - val_acc: 0.9048\n",
      "Epoch 60/200\n",
      "494/494 [==============================] - 0s - loss: 0.1298 - acc: 0.8704 - val_loss: 0.0751 - val_acc: 0.9143\n",
      "Epoch 61/200\n",
      "494/494 [==============================] - 0s - loss: 0.0909 - acc: 0.8846 - val_loss: 0.1046 - val_acc: 0.8857\n",
      "Epoch 62/200\n",
      "494/494 [==============================] - 0s - loss: 0.0957 - acc: 0.8806 - val_loss: 0.0815 - val_acc: 0.9048\n",
      "Epoch 63/200\n",
      "494/494 [==============================] - 0s - loss: 0.0849 - acc: 0.8826 - val_loss: 0.0687 - val_acc: 0.9143\n",
      "Epoch 64/200\n",
      "494/494 [==============================] - 0s - loss: 0.0933 - acc: 0.8846 - val_loss: 0.0673 - val_acc: 0.9048\n",
      "Epoch 65/200\n",
      "494/494 [==============================] - 0s - loss: 0.0862 - acc: 0.8826 - val_loss: 0.0722 - val_acc: 0.9143\n",
      "Epoch 66/200\n",
      "494/494 [==============================] - 0s - loss: 0.0770 - acc: 0.8947 - val_loss: 0.0609 - val_acc: 0.9048\n",
      "Epoch 67/200\n",
      "494/494 [==============================] - 0s - loss: 0.0861 - acc: 0.8846 - val_loss: 0.0735 - val_acc: 0.9048\n",
      "Epoch 68/200\n",
      "494/494 [==============================] - 0s - loss: 0.0821 - acc: 0.8927 - val_loss: 0.0586 - val_acc: 0.9143\n",
      "Epoch 69/200\n",
      "494/494 [==============================] - 0s - loss: 0.0931 - acc: 0.8826 - val_loss: 0.0798 - val_acc: 0.8952\n",
      "Epoch 70/200\n",
      "494/494 [==============================] - 0s - loss: 0.0985 - acc: 0.8785 - val_loss: 0.0952 - val_acc: 0.8952\n",
      "Epoch 71/200\n",
      "494/494 [==============================] - 0s - loss: 0.0879 - acc: 0.8846 - val_loss: 0.0849 - val_acc: 0.9143\n",
      "Epoch 72/200\n",
      "494/494 [==============================] - 0s - loss: 0.0747 - acc: 0.9049 - val_loss: 0.0490 - val_acc: 0.9333\n",
      "Epoch 73/200\n",
      "494/494 [==============================] - 0s - loss: 0.0787 - acc: 0.8947 - val_loss: 0.0546 - val_acc: 0.9333\n",
      "Epoch 74/200\n",
      "494/494 [==============================] - 0s - loss: 0.0961 - acc: 0.8947 - val_loss: 0.0585 - val_acc: 0.9238\n",
      "Epoch 75/200\n",
      "494/494 [==============================] - 0s - loss: 0.0717 - acc: 0.8947 - val_loss: 0.0557 - val_acc: 0.9238\n",
      "Epoch 76/200\n",
      "494/494 [==============================] - 0s - loss: 0.0728 - acc: 0.9271 - val_loss: 0.0504 - val_acc: 0.9238\n",
      "Epoch 77/200\n",
      "494/494 [==============================] - 0s - loss: 0.0680 - acc: 0.9109 - val_loss: 0.0552 - val_acc: 0.9238\n",
      "Epoch 78/200\n",
      "494/494 [==============================] - 0s - loss: 0.0622 - acc: 0.9069 - val_loss: 0.0569 - val_acc: 0.9143\n",
      "Epoch 79/200\n",
      "494/494 [==============================] - 0s - loss: 0.0548 - acc: 0.9069 - val_loss: 0.0587 - val_acc: 0.9238\n",
      "Epoch 80/200\n",
      "494/494 [==============================] - 0s - loss: 0.0789 - acc: 0.8907 - val_loss: 0.0517 - val_acc: 0.9143\n",
      "Epoch 81/200\n",
      "494/494 [==============================] - 0s - loss: 0.0584 - acc: 0.8927 - val_loss: 0.0484 - val_acc: 0.9238\n",
      "Epoch 82/200\n",
      "494/494 [==============================] - 0s - loss: 0.0624 - acc: 0.8968 - val_loss: 0.0425 - val_acc: 0.9238\n",
      "Epoch 83/200\n",
      "494/494 [==============================] - 0s - loss: 0.0871 - acc: 0.9190 - val_loss: 0.0760 - val_acc: 0.9238\n",
      "Epoch 84/200\n",
      "494/494 [==============================] - 0s - loss: 0.0740 - acc: 0.9251 - val_loss: 0.0656 - val_acc: 0.9143\n",
      "Epoch 85/200\n",
      "494/494 [==============================] - 0s - loss: 0.0697 - acc: 0.9109 - val_loss: 0.0570 - val_acc: 0.9143\n",
      "Epoch 86/200\n",
      "494/494 [==============================] - 0s - loss: 0.0639 - acc: 0.8927 - val_loss: 0.0945 - val_acc: 0.9048\n",
      "Epoch 87/200\n",
      "494/494 [==============================] - 0s - loss: 0.0846 - acc: 0.8826 - val_loss: 0.0789 - val_acc: 0.8857\n",
      "Epoch 88/200\n",
      "494/494 [==============================] - 0s - loss: 0.0691 - acc: 0.8846 - val_loss: 0.0835 - val_acc: 0.9048\n",
      "Epoch 89/200\n",
      "494/494 [==============================] - 0s - loss: 0.0653 - acc: 0.8988 - val_loss: 0.0643 - val_acc: 0.8952\n",
      "Epoch 90/200\n",
      "494/494 [==============================] - 0s - loss: 0.0638 - acc: 0.8765 - val_loss: 0.0535 - val_acc: 0.9143\n",
      "Epoch 91/200\n",
      "494/494 [==============================] - 0s - loss: 0.0655 - acc: 0.9251 - val_loss: 0.0472 - val_acc: 0.9905\n",
      "Epoch 92/200\n",
      "494/494 [==============================] - 0s - loss: 0.0570 - acc: 0.9150 - val_loss: 0.0479 - val_acc: 0.9333\n",
      "Epoch 93/200\n",
      "494/494 [==============================] - 0s - loss: 0.0626 - acc: 0.9150 - val_loss: 0.0682 - val_acc: 0.9238\n",
      "Epoch 94/200\n",
      "494/494 [==============================] - 0s - loss: 0.0694 - acc: 0.9069 - val_loss: 0.0422 - val_acc: 0.9238\n",
      "Epoch 95/200\n",
      "494/494 [==============================] - 0s - loss: 0.0444 - acc: 0.9190 - val_loss: 0.0509 - val_acc: 0.9238\n",
      "Epoch 96/200\n",
      "494/494 [==============================] - 0s - loss: 0.0428 - acc: 0.9069 - val_loss: 0.0544 - val_acc: 0.9238\n",
      "Epoch 97/200\n",
      "494/494 [==============================] - 0s - loss: 0.0413 - acc: 0.9130 - val_loss: 0.0493 - val_acc: 0.9143\n",
      "Epoch 98/200\n",
      "494/494 [==============================] - 0s - loss: 0.0438 - acc: 0.9049 - val_loss: 0.0450 - val_acc: 0.9238\n",
      "Epoch 99/200\n",
      "494/494 [==============================] - 0s - loss: 0.0861 - acc: 0.9028 - val_loss: 0.3745 - val_acc: 0.8667\n",
      "Epoch 100/200\n",
      "494/494 [==============================] - 0s - loss: 0.1085 - acc: 0.8907 - val_loss: 0.0713 - val_acc: 0.9143\n",
      "Epoch 101/200\n",
      "494/494 [==============================] - 0s - loss: 0.0646 - acc: 0.8887 - val_loss: 0.0468 - val_acc: 0.9238\n",
      "Epoch 102/200\n",
      "494/494 [==============================] - 0s - loss: 0.0413 - acc: 0.9028 - val_loss: 0.0672 - val_acc: 0.9238\n",
      "Epoch 103/200\n",
      "494/494 [==============================] - 0s - loss: 0.0565 - acc: 0.9150 - val_loss: 0.0512 - val_acc: 0.9238\n",
      "Epoch 104/200\n",
      "494/494 [==============================] - 0s - loss: 0.0518 - acc: 0.9028 - val_loss: 0.0397 - val_acc: 0.9238\n",
      "Epoch 105/200\n",
      "494/494 [==============================] - 0s - loss: 0.0546 - acc: 0.8947 - val_loss: 0.0758 - val_acc: 0.9238\n",
      "Epoch 106/200\n",
      "494/494 [==============================] - 0s - loss: 0.0419 - acc: 0.9190 - val_loss: 0.0436 - val_acc: 0.9238\n",
      "Epoch 107/200\n",
      "494/494 [==============================] - 0s - loss: 0.0468 - acc: 0.9089 - val_loss: 0.0631 - val_acc: 0.9238\n",
      "Epoch 108/200\n",
      "494/494 [==============================] - 0s - loss: 0.0349 - acc: 0.9089 - val_loss: 0.0611 - val_acc: 0.9048\n",
      "Epoch 109/200\n",
      "494/494 [==============================] - 0s - loss: 0.0375 - acc: 0.9069 - val_loss: 0.0645 - val_acc: 0.9238\n",
      "Epoch 110/200\n",
      "494/494 [==============================] - 0s - loss: 0.0474 - acc: 0.9089 - val_loss: 0.0571 - val_acc: 0.9238\n",
      "Epoch 111/200\n",
      "494/494 [==============================] - 0s - loss: 0.0404 - acc: 0.9008 - val_loss: 0.0405 - val_acc: 0.9333\n",
      "Epoch 112/200\n",
      "494/494 [==============================] - 0s - loss: 0.0303 - acc: 0.9231 - val_loss: 0.0484 - val_acc: 0.9238\n",
      "Epoch 113/200\n",
      "494/494 [==============================] - 0s - loss: 0.0404 - acc: 0.9130 - val_loss: 0.0561 - val_acc: 0.9238\n",
      "Epoch 114/200\n",
      "494/494 [==============================] - 0s - loss: 0.0327 - acc: 0.9170 - val_loss: 0.0440 - val_acc: 0.9238\n",
      "Epoch 115/200\n",
      "494/494 [==============================] - 0s - loss: 0.0507 - acc: 0.8988 - val_loss: 0.0341 - val_acc: 0.9333\n",
      "Epoch 116/200\n",
      "494/494 [==============================] - 0s - loss: 0.0363 - acc: 0.9109 - val_loss: 0.0376 - val_acc: 0.9333\n",
      "Epoch 117/200\n",
      "494/494 [==============================] - 0s - loss: 0.0303 - acc: 0.9130 - val_loss: 0.0378 - val_acc: 0.9238\n",
      "Epoch 118/200\n",
      "494/494 [==============================] - 0s - loss: 0.0300 - acc: 0.9109 - val_loss: 0.0398 - val_acc: 0.9238\n",
      "Epoch 119/200\n",
      "494/494 [==============================] - 0s - loss: 0.0266 - acc: 0.9231 - val_loss: 0.0338 - val_acc: 0.9333\n",
      "Epoch 120/200\n",
      "494/494 [==============================] - 0s - loss: 0.0284 - acc: 0.9089 - val_loss: 0.0437 - val_acc: 0.9238\n",
      "Epoch 121/200\n",
      "494/494 [==============================] - 0s - loss: 0.0372 - acc: 0.8968 - val_loss: 0.0370 - val_acc: 0.9333\n",
      "Epoch 122/200\n",
      "494/494 [==============================] - 0s - loss: 0.0309 - acc: 0.9049 - val_loss: 0.0556 - val_acc: 0.9238\n",
      "Epoch 123/200\n",
      "494/494 [==============================] - 0s - loss: 0.0385 - acc: 0.9008 - val_loss: 0.0383 - val_acc: 0.9333\n",
      "Epoch 124/200\n",
      "494/494 [==============================] - 0s - loss: 0.0332 - acc: 0.9150 - val_loss: 0.0390 - val_acc: 0.9238\n",
      "Epoch 125/200\n",
      "494/494 [==============================] - 0s - loss: 0.0348 - acc: 0.8968 - val_loss: 0.0313 - val_acc: 0.9333\n",
      "Epoch 126/200\n",
      "494/494 [==============================] - 0s - loss: 0.0525 - acc: 0.9251 - val_loss: 0.0314 - val_acc: 0.9333\n",
      "Epoch 127/200\n",
      "494/494 [==============================] - 0s - loss: 0.0319 - acc: 0.9190 - val_loss: 0.0317 - val_acc: 0.9238\n",
      "Epoch 128/200\n",
      "494/494 [==============================] - 0s - loss: 0.0265 - acc: 0.9049 - val_loss: 0.0483 - val_acc: 0.9143\n",
      "Epoch 129/200\n",
      "494/494 [==============================] - 0s - loss: 0.0357 - acc: 0.8988 - val_loss: 0.0495 - val_acc: 0.9238\n",
      "Epoch 130/200\n",
      "494/494 [==============================] - 0s - loss: 0.0356 - acc: 0.9332 - val_loss: 0.0427 - val_acc: 0.9238\n",
      "Epoch 131/200\n",
      "494/494 [==============================] - 0s - loss: 0.0285 - acc: 0.9150 - val_loss: 0.0332 - val_acc: 0.9429\n",
      "Epoch 132/200\n",
      "494/494 [==============================] - 0s - loss: 0.0330 - acc: 0.9028 - val_loss: 0.0351 - val_acc: 0.9333\n",
      "Epoch 133/200\n",
      "494/494 [==============================] - 0s - loss: 0.0296 - acc: 0.9130 - val_loss: 0.0506 - val_acc: 0.9238\n",
      "Epoch 134/200\n",
      "494/494 [==============================] - 0s - loss: 0.0842 - acc: 0.8988 - val_loss: 0.0282 - val_acc: 0.9333\n",
      "Epoch 135/200\n",
      "494/494 [==============================] - 0s - loss: 0.0406 - acc: 0.9028 - val_loss: 0.1744 - val_acc: 0.8857\n",
      "Epoch 136/200\n",
      "494/494 [==============================] - 0s - loss: 0.0518 - acc: 0.8988 - val_loss: 0.0952 - val_acc: 0.9048\n",
      "Epoch 137/200\n",
      "494/494 [==============================] - 0s - loss: 0.0598 - acc: 0.8947 - val_loss: 0.0626 - val_acc: 0.9048\n",
      "Epoch 138/200\n",
      "494/494 [==============================] - 0s - loss: 0.0399 - acc: 0.9028 - val_loss: 0.0396 - val_acc: 0.9238\n",
      "Epoch 139/200\n",
      "494/494 [==============================] - 0s - loss: 0.0352 - acc: 0.9049 - val_loss: 0.0313 - val_acc: 0.9333\n",
      "Epoch 140/200\n",
      "494/494 [==============================] - 0s - loss: 0.0279 - acc: 0.9211 - val_loss: 0.0323 - val_acc: 0.9333\n",
      "Epoch 141/200\n",
      "494/494 [==============================] - 0s - loss: 0.0342 - acc: 0.9170 - val_loss: 0.0330 - val_acc: 0.9333\n",
      "Epoch 142/200\n",
      "494/494 [==============================] - 0s - loss: 0.0417 - acc: 0.9089 - val_loss: 0.0761 - val_acc: 0.9143\n",
      "Epoch 143/200\n",
      "494/494 [==============================] - 0s - loss: 0.0392 - acc: 0.9109 - val_loss: 0.0526 - val_acc: 0.9238\n",
      "Epoch 144/200\n",
      "494/494 [==============================] - 0s - loss: 0.0342 - acc: 0.9049 - val_loss: 0.0547 - val_acc: 0.9048\n",
      "Epoch 145/200\n",
      "494/494 [==============================] - 0s - loss: 0.0487 - acc: 0.8927 - val_loss: 0.0357 - val_acc: 0.9238\n",
      "Epoch 146/200\n",
      "494/494 [==============================] - 0s - loss: 0.0301 - acc: 0.9049 - val_loss: 0.0688 - val_acc: 0.9143\n",
      "Epoch 147/200\n",
      "494/494 [==============================] - 0s - loss: 0.0366 - acc: 0.9109 - val_loss: 0.0336 - val_acc: 0.9238\n",
      "Epoch 148/200\n",
      "494/494 [==============================] - 0s - loss: 0.0409 - acc: 0.9008 - val_loss: 0.0346 - val_acc: 0.9143\n",
      "Epoch 149/200\n",
      "494/494 [==============================] - 0s - loss: 0.0301 - acc: 0.9109 - val_loss: 0.0366 - val_acc: 0.9238\n",
      "Epoch 150/200\n",
      "494/494 [==============================] - 0s - loss: 0.0206 - acc: 0.9170 - val_loss: 0.0561 - val_acc: 0.9143\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fc2a98003d0>"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, nb_epoch=200, shuffle=True, callbacks=[earlyStopping], \n",
    "          validation_data = (x_valid, y_valid), batch_size=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/107 [===========================>..] - ETA: 0s"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.086966711609998595, 0.88785044612171493]"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_test, y_test, batch_size=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "107/107 [==============================] - 1s     \n",
      "[1 2 2 0 2 2 0 3 1 3 3 3 3 2 0 3 1 1 3 1 1 2 2 3 3 0 1 3 3 2 3 2 3 1 0 0 0\n",
      " 3 3 1 3 2 0 3 0 1 3 1 0 3 3 1 3 1 1 1 1 0 3 0 3 0 0 3 0 0 3 1 3 2 1 0 2 3\n",
      " 3 3 3 2 0 1 0 3 1 3 3 3 3 1 3 1 3 0 3 0 3 3 3 3 0 3 3 0 2 2 2 3 2]\n",
      "107/107 [==============================] - 0s     \n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "class 0(0back)       1.00      0.72      0.84        32\n",
      "class 1(1back)       0.95      1.00      0.98        21\n",
      "class 2(2back)       0.88      1.00      0.94        15\n",
      " class3(3back)       0.80      0.92      0.86        39\n",
      "\n",
      "   avg / total       0.90      0.89      0.89       107\n",
      "\n",
      "[[23  0  0  9]\n",
      " [ 0 21  0  0]\n",
      " [ 0  0 15  0]\n",
      " [ 0  1  2 36]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "\n",
    "y_pred = model.predict_classes(x_test)\n",
    "print(y_pred)\n",
    "\n",
    "p=model.predict_proba(x_test)\n",
    "\n",
    "target_names = ['class 0(0back)', 'class 1(1back)', 'class 2(2back)', 'class3(3back)']\n",
    "print(classification_report(np.argmax(y_test,axis=1), y_pred,target_names=target_names))\n",
    "print(confusion_matrix(np.argmax(y_test,axis=1), y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
